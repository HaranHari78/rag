import os
import pandas as pd
import configparser
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain.chains import RetrievalQA

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

# Azure OpenAI credentials
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Load CSV data
csv_path = r"C:\Users\HariharaM12\Downloads\Medical_Data.csv"
df = pd.read_csv(csv_path)

documents = []
for _, row in df.iterrows():
    if pd.isna(row["text"]):
        continue
    documents.append(Document(
        page_content=row["text"],
        metadata={"source": row["title"]}
    ))

# Split documents into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(documents)

# Embedding model
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000  # required to avoid KeyError
)

# Build and save FAISS vector store
vectorstore = FAISS.from_documents(chunks, embedding_model)
vectorstore.save_local("faiss_index")

# Create retriever and QA chain
retriever = vectorstore.as_retriever()

llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# Run query
query = "What diagnosis is mentioned in the document?"
response = qa_chain({"query": query})

print("üìò Answer:", response["result"])
print("üìé Sources:", [doc.metadata["source"] for doc in response["source_documents"]])


MY_CODE::



import os
import pandas as pd
import configparser
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Set TOP_K
TOP_K = 5  # You can change this value as needed

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

# Azure OpenAI credentials
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Load CSV data
csv_path = r"C:\Users\HariharaM12\Downloads\Medical_Data.csv"
df = pd.read_csv(csv_path)

# Create Document objects
documents = []
for _, row in df.iterrows():
    if pd.isna(row["text"]):
        continue
    documents.append(Document(
        page_content=row["text"],
        metadata={"source": row["title"]}
    ))

# Show full text of the first document
print("üìÑ FULL TEXT OF FIRST DOCUMENT:\n")
print(documents[0].page_content)
first_doc_title = documents[0].metadata["source"]

# Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)

# Show chunks for first document
print("\nüß© CHUNKS FOR FIRST DOCUMENT:\n")
for i, chunk in enumerate(chunks):
    if chunk.metadata["source"] == first_doc_title:
        print(f"[Chunk {i+1}]")
        print(chunk.page_content)
        print("-" * 80)

# Embedding model
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build and save FAISS vector store
vectorstore = FAISS.from_documents(chunks, embedding_model)
vectorstore.save_local("faiss_index")

# Define query
query = "Based on the clinical note, what medications has the patient received, and what is the current status of those treatments?"

# Perform similarity search with scores
top_results = vectorstore.similarity_search_with_score(query, k=TOP_K)
docs_only = [doc for doc, _ in top_results]

# LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Build improved clinical prompt
context = "\n\n".join([doc.page_content for doc in docs_only])
full_prompt = f"""
You are a clinical assistant specialized in analyzing oncology patient records.

Your task is to read the provided clinical notes and answer the medical question based only on the given context.

If the answer is not found in the context, respond with "Not mentioned in the context."

---

üìÑ Context:
{context}

---

‚ùì Question:
{query}
"""

# Invoke LLM
response = llm.invoke(full_prompt)

# Output
print(f"\nüìò Answer:\n{response.content}")
print(f"\nüîç Top {TOP_K} Retrieved Chunks with Similarity Scores:\n")

for i, (doc, score) in enumerate(top_results):
    print(f"--- [Chunk {i+1}] from Document: {doc.metadata['source']} ---")
    print(f"üî¢ Similarity Score: {score:.4f}")
    print(doc.page_content)
    print("-" * 100)

