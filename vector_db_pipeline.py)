import os
import pandas as pd
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import AzureOpenAIEmbeddings
from langchain.chat_models import AzureChatOpenAI
from langchain.chains import RetrievalQA
import configparser

# Load config from INI
config = configparser.ConfigParser()
config.read("config.ini")

# Azure OpenAI setup
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]
EMBEDDING_DEPLOYMENT = "text-embedding-3-large-EUS-2-01"
EMBEDDING_MODEL = "text-embedding-3-large"

# CSV input
input_file = "medicaldata.csv"  # Ensure this file is in your working dir
df = pd.read_csv(input_file)
documents = []

# Create LangChain Documents with metadata
for _, row in df.iterrows():
    if pd.isna(row["text"]):
        continue
    documents.append(Document(
        page_content=row["text"],
        metadata={"source": row["title"]}
    ))

# Split documents into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(documents)

# Initialize Azure Embedding model
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT
)

# Build FAISS vector DB
vectorstore = FAISS.from_documents(chunks, embedding_model)
vectorstore.save_local("faiss_index")

# (Optional) Setup RetrievalQA chain with GPT-4o
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_version=AZURE_OPENAI_API_VERSION,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    temperature=0
)

retriever = vectorstore.as_retriever()
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# Example query
query = "Was the patient diagnosed with AML?"
response = qa_chain({"query": query})

print("Answer:", response["result"])
print("Sources:", [doc.metadata["source"] for doc in response["source_documents"]])
