import os
import re
import json
import pandas as pd
import configparser
import faiss
import numpy as np
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config
config = configparser.ConfigParser()
config.read("config.ini")

# Azure credentials
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Load clinical CSV
csv_path = "MEDICAL_DATAS.csv"
df = pd.read_csv(csv_path)

# Create Documents
documents = []
for _, row in df.iterrows():
    if pd.isna(row["text"]):
        continue
    documents.append(Document(page_content=row["text"], metadata={"source": row["title"]}))

# Split with smart overlap
splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
chunks = splitter.split_documents(documents)

# Embed
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

texts = [chunk.page_content for chunk in chunks]
metadatas = [chunk.metadata for chunk in chunks]
embeddings = embedding_model.embed_documents(texts)
embeddings = np.array(embeddings).astype("float32")
faiss.normalize_L2(embeddings)

index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(embeddings)

# Filter relevant chunks
filtered_chunks = []
def normalize_text(text):
    return re.sub(r"\s+", " ", text.lower()).strip()

for chunk in chunks:
    norm_text = normalize_text(chunk.page_content)
    source_title = chunk.metadata.get("source", "Unknown")
    if source_title != "Unknown" and ("kappa" in norm_text or "lambda" in norm_text or "ratio" in norm_text):
        filtered_chunks.append({"title": source_title, "content": chunk.page_content})

# Helper to batchify
def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# Prepare LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

final_results = []

def run_llm_on_batch(i, batch):
    json_context = [
        {"note_id": j + 1, "title": item["title"], "content": item["content"]}
        for j, item in enumerate(batch)
    ]
    titles = [item["title"] for item in batch]
    context_json_str = json.dumps(json_context, indent=2)

    full_prompt = f"""
You are a clinical assistant analyzing oncology lab results.
Extract the following values ONLY if explicitly mentioned in the context. If not present, write null.
Also extract ONE evidence sentence from the context that best supports all the values together.
Respond in strict JSON like:
{{
  \"kappa_light_chain_mg_per_dl\": \"0.09 mg/dL\",
  \"lambda_light_chain_mg_per_dl\": \"0.15 mg/dL\",
  \"kappa_lambda_ratio\": \"0.60\",
  \"evidence_sentences\": ["..."]
}}
Respond with ONLY the JSON object. Do not include any explanation or preamble.

---
Context (as JSON array of clinical notes):
{context_json_str}
---
"""
    batch_results = []
    try:
        response = llm.invoke(full_prompt)
        if not response.content.strip().startswith("{"):
            raise ValueError("Empty or non-JSON response")
        data = json.loads(response.content)
        for title in titles:
            entry = {
                "source_document": title,
                "kappa_light_chain_mg_per_dl": data.get("kappa_light_chain_mg_per_dl", ""),
                "lambda_light_chain_mg_per_dl": data.get("lambda_light_chain_mg_per_dl", ""),
                "kappa_lambda_ratio": data.get("kappa_lambda_ratio", ""),
                "evidence_sentences": data.get("evidence_sentences", []),
                "context": json.dumps({
                    "kappa_flc": data.get("kappa_light_chain_mg_per_dl", ""),
                    "lambda_flc": data.get("lambda_light_chain_mg_per_dl", ""),
                    "kappa_lambda_ratio": data.get("kappa_lambda_ratio", ""),
                    "date_of_lab": data.get("date_of_lab", ""),
                    "evidence_sentences": data.get("evidence_sentences", [])
                })
            }
            batch_results.append(entry)
    except Exception as e:
        print(f"Error in batch {i + 1}: {e}")
        with open("llm_error_logs.txt", "a", encoding="utf-8") as f:
            f.write(f"\n\n---\nBATCH {i + 1} TITLES: {titles}\nPROMPT:\n{full_prompt}\nRESPONSE:\n{getattr(response, 'content', 'No response')}\n")
    return batch_results

# Run LLM with threading
batches = list(batchify(filtered_chunks, 10))
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(run_llm_on_batch, i, batch): batch for i, batch in enumerate(batches)}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            result = future.result()
            final_results.extend(result)
        except Exception as e:
            print(f"Threaded batch failed: {e}")

# Save to Excel
if final_results:
    result_df = pd.DataFrame(final_results)
    cols = ["source_document", "kappa_light_chain_mg_per_dl", "lambda_light_chain_mg_per_dl", "kappa_lambda_ratio", "evidence_sentences", "context"]
    result_df = result_df[cols]
    result_df.drop_duplicates(subset=cols, inplace=True)

    output_dir = r"C:\\Users\\HariharaM12\\PycharmProjects\\Rag1\\Task"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, "extracted_results.xlsx")
    result_df.to_excel(output_path, index=False)
    print(f"✅ Extraction complete. Output saved to {output_path}")
else:
    print("⚠️ No valid extraction results. Please check your data and LLM responses.")
