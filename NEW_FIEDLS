import os
import re
import json
import pandas as pd
import configparser
import faiss
import numpy as np
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config
config = configparser.ConfigParser()
config.read("config.ini")

# Azure credentials
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Load clinical CSV
csv_path = "MEDICAL_DATAS.csv"
df = pd.read_csv(csv_path)

# Create Documents
documents = []
for _, row in df.iterrows():
    if pd.isna(row["text"]):
        continue
    documents.append(Document(page_content=row["text"], metadata={"source": row["title"]}))

# Split with smart overlap
splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
chunks = splitter.split_documents(documents)

# Embed
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

texts = [chunk.page_content for chunk in chunks]
metadatas = [chunk.metadata for chunk in chunks]
embeddings = embedding_model.embed_documents(texts)
embeddings = np.array(embeddings).astype("float32")
faiss.normalize_L2(embeddings)

index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(embeddings)

llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

final_results = []
TOP_K = 4

for doc in documents:
    query_embedding = embedding_model.embed_query("Free light chain levels")
    query_embedding = np.array([query_embedding]).astype("float32")
    faiss.normalize_L2(query_embedding)
    D, I = index.search(query_embedding, TOP_K)

    selected_chunks = [chunks[i] for i in I[0] if chunks[i].metadata["source"] == doc.metadata["source"]]
    if not selected_chunks:
        continue

    json_context = [
        {
            "note_id": j + 1,
            "title": chunk.metadata["source"],
            "content": chunk.page_content
        }
        for j, chunk in enumerate(selected_chunks)
    ]

    context_json_str = json.dumps(json_context, indent=2)

    full_prompt = f"""
You are a clinical assistant analyzing oncology lab results.
Extract the following values ONLY if explicitly mentioned in the context. If not present, write null.
Also extract ONE evidence sentence from the context that best supports all the values together.
Respond in strict JSON like:
{{
  \"kappa_light_chain_mg_per_dl\": \"0.09 mg/dL\",
  \"lambda_light_chain_mg_per_dl\": \"0.15 mg/dL\",
  \"kappa_lambda_ratio\": \"0.60\",
  \"evidence_sentences\": ["..."]
}}
Respond with ONLY the JSON object. Do not include any explanation or preamble.

---
Context (as JSON array of clinical notes):
{context_json_str}
---
"""
    try:
        response = llm.invoke(full_prompt)
        if not response.content.strip().startswith("{"):
            raise ValueError("Empty or non-JSON response")
        data = json.loads(response.content)
        data["source_document"] = doc.metadata["source"]
        data["context"] = json.dumps({
            "kappa_flc": data.get("kappa_light_chain_mg_per_dl", ""),
            "lambda_flc": data.get("lambda_light_chain_mg_per_dl", ""),
            "kappa_lambda_ratio": data.get("kappa_lambda_ratio", ""),
            "date_of_lab": data.get("date_of_lab", ""),
            "evidence_sentences": data.get("evidence_sentences", [])
        })
        final_results.append(data)
    except Exception as e:
        print(f"Error for document {doc.metadata['source']}: {e}")
        with open("llm_error_logs.txt", "a", encoding="utf-8") as f:
            f.write(f"\n\n---\nSOURCE: {doc.metadata['source']}\nPROMPT:\n{full_prompt}\nRESPONSE:\n{getattr(response, 'content', 'No response')}\n")

# Only proceed if valid data exists
if final_results:
    result_df = pd.DataFrame(final_results)
    cols = ["source_document", "kappa_light_chain_mg_per_dl", "lambda_light_chain_mg_per_dl", "kappa_lambda_ratio", "evidence_sentences", "context"]
    result_df = result_df[cols]
    result_df.drop_duplicates(subset=cols, inplace=True)

    output_dir = r"C:\\Users\\HariharaM12\\PycharmProjects\\Rag1\\Task"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, "extracted_results.xlsx")
    result_df.to_excel(output_path, index=False)
    print(f"✅ Extraction complete. Output saved to {output_path}")
else:
    print("⚠️ No valid extraction results. Please check your data and LLM responses.")
