import os
import re
import json
import numpy as np
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
from datetime import datetime
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# === Load config.ini ===
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# === File paths ===
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
output_dir = "output/fields"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, "kappa_lambda_results_updated.xlsx")

# === Helper functions ===
def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def parse_llm_json(raw_text: str) -> str:
    try:
        pattern = r"```(?:json)?\s*(.*?)```"
        match = re.search(pattern, raw_text, flags=re.DOTALL)
        content = match.group(1).strip() if match else raw_text.strip()
        if content.startswith("json"):
            content = content[len("json"):].strip()
        if not content:
            raise ValueError("Empty content after parsing")
        return json.dumps(json.loads(content))
    except Exception:
        try:
            return json.dumps(json.loads(raw_text.strip()))
        except Exception as fallback_error:
            raise ValueError(f"Both regex and fallback JSON parse failed: {fallback_error}")

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\\s+', ' ', text).strip()
    return text

def extract_date(date_str):
    try:
        return datetime.strptime(date_str, "%Y-%m-%d")
    except:
        return None

def evidence_score(evidence, ratio):
    evidence = evidence.lower()
    if ratio and ratio in evidence:
        return 2
    if "kappa" in evidence and "lambda" in evidence:
        return 1
    return 0

# === Load CSV and chunk ===
df = pd.read_csv(csv_path)
documents = []
for _, row in tqdm(df.iterrows(), total=len(df), desc="üìÑ Loading documents"):
    if pd.notna(row["text"]):
        documents.append(Document(page_content=row["text"], metadata={"source": row["title"]}))

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)
chunk_batches = list(batchify(chunks, 20))

# === Embedding model setup ===
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# === Build FAISS index ===
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in chunk_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# === Similarity search ===
query = "Extract the patient's kappa free light chain (mg/L), lambda free light chain (mg/L), and kappa/lambda ratio, along with the lab date and evidence."
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
results = vectorstore.similarity_search(query, k=1000)

# === Filtered Chunks ===
filtered_chunks = []
for doc in results:
    norm = normalize_text(doc.page_content)
    if 'kappa' in norm or 'lambda' in norm or 'ratio' in norm:
        filtered_chunks.append((doc.metadata.get("source", "Unknown"), doc.page_content))

# === LLM Setup ===
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# === Prompting and extraction ===
final_results = []
batches = list(batchify(filtered_chunks, 10))

for i, batch in enumerate(batches):
    json_context = [
        {"source": source, "content": doc}
        for source, doc in batch
    ]
    prompt = f"""
You are a medical information extraction assistant. Your task is to extract lab results from the provided clinical notes.

From the context below, extract the following values **only if they are explicitly mentioned**:
- Kappa free light chains (mg/dL)
- Lambda free light chains (mg/dL)
- Kappa/Lambda ratio
- Lab test date (YYYY-MM-DD format)
- Supporting evidence sentence
- The document title the sentence came from (you will be given that)

IMPORTANT:
- If the date is incomplete (e.g., only month and year), fill the missing parts with "XX".
- DO NOT guess or infer dates.
- Return structured JSON list. One object per lab result.
- Include "evidence_sentences" and "source_document".

--- CONTEXT START ---
{json.dumps(json_context, indent=2)}
--- CONTEXT END ---
"""
    try:
        response = llm.invoke(prompt)
        if not response.content.strip():
            continue
        cleaned = parse_llm_json(response.content)
        extracted_items = json.loads(cleaned)
        for item, (source_title, _) in zip(extracted_items, batch):
            item["source_document"] = source_title
            final_results.append(item)
    except Exception as e:
        print(f"‚ùå Failed batch {i + 1}: {e}")
        continue

# === Save Raw JSON ===
with open(os.path.join(output_dir, "kappa_lambda_results.json"), "w", encoding="utf-8") as f:
    json.dump(final_results, f, indent=2, ensure_ascii=False)

# === Aggregate by source_document with filtering ===
merged = defaultdict(list)
for item in final_results:
    merged[item["source_document"]].append(item)

final_cleaned = []

for doc, entries in merged.items():
    best_entry = None
    best_score = -1
    for entry in entries:
        kappa = entry.get("kappa_flc", "")
        lam = entry.get("lambda_flc", "")
        ratio = entry.get("kappa_lambda_ratio", "")
        evs = entry.get("evidence_sentences", [])
        score = max([evidence_score(ev, ratio) for ev in evs]) if evs else 0

        if score > best_score:
            if ("mg/dL" in kappa or "mg/dL" in lam) and re.search(r"[<>]", ratio or ""):
                best_entry = entry
                best_score = score

    if best_entry:
        dates = [extract_date(e.get("date_of_lab", "")) for e in entries if extract_date(e.get("date_of_lab", ""))]
        best_date = min(dates).strftime("%Y-%m-%d") if dates else "Unknown"

        final_cleaned.append({
            "source_document": doc,
            "kappa_flc": best_entry.get("kappa_flc", ""),
            "lambda_flc": best_entry.get("lambda_flc", ""),
            "kappa_lambda_ratio": best_entry.get("kappa_lambda_ratio", ""),
            "date_of_lab": best_date,
            "evidence_sentences": list(set(best_entry.get("evidence_sentences", []))),
            "context": json.dumps({
                "kappa_flc": best_entry.get("kappa_flc", ""),
                "lambda_flc": best_entry.get("lambda_flc", ""),
                "kappa_lambda_ratio": best_entry.get("kappa_lambda_ratio", ""),
                "date_of_lab": best_date,
                "evidence_sentences": best_entry.get("evidence_sentences", [])
            })
        })

# === Export Final Cleaned Results ===
df = pd.DataFrame(final_cleaned)
if not df.empty:
    df["evidence_sentences"] = df["evidence_sentences"].apply(lambda x: "\n".join(x) if isinstance(x, list) else str(x))
    df.drop_duplicates(subset=["source_document", "context"], inplace=True)
    df = df[["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]]
    df.to_excel(output_path, index=False)
    print(f"‚úÖ Extracted results saved to {output_path}")
else:
    print("‚ö†Ô∏è No valid cleaned data to save.")
