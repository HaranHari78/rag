import os
import faiss
import numpy as np
import pandas as pd
import configparser
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores.faiss import FAISS
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings

# üîß Custom Embedding class with normalization
class NormalizedAzureOpenAIEmbeddings(AzureOpenAIEmbeddings):
    def embed_documents(self, texts):
        vectors = super().embed_documents(texts)
        return [self._normalize(vec) for vec in vectors]

    def embed_query(self, text):
        vector = super().embed_query(text)
        return self._normalize(vector)

    def _normalize(self, vec):
        norm = np.linalg.norm(vec)
        return (np.array(vec) / norm).tolist() if norm > 0 else vec

# Set top-k
TOP_K = 5

# Load config
config = configparser.ConfigParser()
config.read("config.ini")
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Load medical data
csv_path = r"C:\Users\HariharaM12\Downloads\Medical_Data.csv"
df = pd.read_csv(csv_path)

# Build LangChain documents
documents = []
for _, row in df.iterrows():
    if pd.isna(row["text"]):
        continue
    documents.append(Document(
        page_content=row["text"],
        metadata={"source": row["title"]}
    ))

# Show text from first document
print("üìÑ FULL TEXT OF FIRST DOCUMENT:\n")
print(documents[0].page_content)
first_doc_title = documents[0].metadata["source"]

# Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)

# Show chunks from first document
print("\nüß© CHUNKS FOR FIRST DOCUMENT:\n")
for i, chunk in enumerate(chunks):
    if chunk.metadata["source"] == first_doc_title:
        print(f"[Chunk {i+1}]")
        print(chunk.page_content)
        print("-" * 80)

# Init normalized embedding model
embedding_model = NormalizedAzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Manually embed and build cosine-based FAISS index
texts = [doc.page_content for doc in chunks]
metadatas = [doc.metadata for doc in chunks]
embeddings = embedding_model.embed_documents(texts)

index = faiss.IndexFlatIP(len(embeddings[0]))  # cosine = inner product on normalized vectors
index.add(np.array(embeddings).astype("float32"))

# Build FAISS vector store manually
vectorstore = FAISS(embedding_model, index, texts=chunks, documents=chunks)
vectorstore.save_local("faiss_index_cosine")

# Define clinical query
query = "Based on the clinical note, what medications has the patient received, and what is the current status of those treatments?"

# Search with cosine similarity
top_results = vectorstore.similarity_search_with_score(query, k=TOP_K)
docs_only = [doc for doc, _ in top_results]

# Initialize LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Compose clinical prompt
context = "\n\n".join([doc.page_content for doc in docs_only])
full_prompt = f"""
You are a clinical assistant specialized in analyzing oncology patient records.

Your task is to read the provided clinical notes and answer the medical question based only on the given context.

If the answer is not found in the context, respond with "Not mentioned in the context."

---

üìÑ Context:
{context}

---

‚ùì Question:
{query}
"""

# Get LLM response
response = llm.invoke(full_prompt)

# Print answer
print(f"\nüìò Answer:\n{response.content}")
print(f"\nüîç Top {TOP_K} Retrieved Chunks with Cosine Similarity:\n")

for i, (doc, score) in enumerate(top_results):
    print(f"--- [Chunk {i+1}] from Document: {doc.metadata['source']} ---")
    print(f"üîó Cosine Similarity Score: {score:.4f}")
    print(doc.page_content)
    print("-" * 100)
