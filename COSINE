

import os
import faiss
import numpy as np
import pandas as pd
import configparser
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores.faiss import FAISS
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings

# üîß Normalized embedding wrapper for cosine similarity
class NormalizedAzureOpenAIEmbeddings(AzureOpenAIEmbeddings):
    def embed_documents(self, texts):
        vectors = super().embed_documents(texts)
        return [self._normalize(vec) for vec in vectors]

    def embed_query(self, text):
        vector = super().embed_query(text)
        return self._normalize(vector)

    def _normalize(self, vec):
        norm = np.linalg.norm(vec)
        return (np.array(vec) / norm).tolist() if norm > 0 else vec

# Parameters
TOP_K = 5

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

# Azure config
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Load CSV
csv_path = r"d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# Convert to LangChain Documents
documents = []
for _, row in df.iterrows():
    if pd.isna(row["text"]):
        continue
    documents.append(Document(
        page_content=row["text"],
        metadata={"source": row["title"]}
    ))

# Chunk documents
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)

# Embedding model (normalized)
embedding_model = NormalizedAzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Embed and build FAISS index
texts = [doc.page_content for doc in chunks]
embeddings = embedding_model.embed_documents(texts)
embeddings = np.array(embeddings).astype("float32")

index = faiss.IndexFlatIP(embeddings.shape[1])  # cosine similarity
index.add(embeddings)

# Build docstore & index mapping
docstore = InMemoryDocstore({str(i): chunks[i] for i in range(len(chunks))})
index_to_docstore = {i: str(i) for i in range(len(chunks))}

# Vector store with cosine similarity
vectorstore = FAISS(embedding_model, index, chunks, index_to_docstore, docstore)
vectorstore.save_local("faiss_index_cosine")

# Query
query = "Based on the clinical note, what medications has the patient received, and what is the current status of those treatments?"
top_results = vectorstore.similarity_search_with_score(query, k=TOP_K)
docs_only = [doc for doc, _ in top_results]

# LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Prompt
context = "\n\n".join([doc.page_content for doc in docs_only])
full_prompt = f"""
You are a clinical assistant specialized in analyzing oncology patient records.
Your task is to read the provided clinical notes and answer the medical question based only on the given context.
If the answer is not found in the context, respond with \"Not mentioned in the context.\"

---
üìÑ Context:
{context}

---
‚ùì Question:
{query}
"""

# Run LLM
response = llm.invoke(full_prompt)

# Output
print(f"\nüìò Answer:\n{response.content}")
print(f"\nüîç Top {TOP_K} Retrieved Chunks with Cosine Similarity:\n")

for i, (doc, score) in enumerate(top_results):
    print(f"--- [Chunk {i+1}] from Document: {doc.metadata['source']} ---")
    print(f"üîó Cosine Similarity Score: {score:.4f}")
    print(doc.page_content)
    print("-" * 100)


import faiss
import numpy as np
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import AzureOpenAIEmbeddings

# Load embeddings
embedding_model = AzureOpenAIEmbeddings(
    deployment="your_deployment_name",
    model="text-embedding-3-large",
    openai_api_key="your_api_key",
    azure_endpoint="your_endpoint",
    openai_api_version="your_version"
)

# Prepare documents
documents = [Document(page_content="Patient started azacitidine and venetoclax.", metadata={"source": "doc1"})]

# Split docs
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
chunks = splitter.split_documents(documents)

# Embed and normalize
embeddings = embedding_model.embed_documents([doc.page_content for doc in chunks])
embeddings = np.array(embeddings)
faiss.normalize_L2(embeddings)  # ‚úÖ normalize for cosine

# FAISS Index with Cosine (using Inner Product on normalized vectors)
dimension = embeddings.shape[1]
index = faiss.IndexFlatIP(dimension)
index.add(embeddings)

# Build vectorstore manually
vectorstore = FAISS(embedding_function=embedding_model.embed_query, index=index, documents=chunks)

# Query
query = "What treatment is the patient receiving?"
query_vector = embedding_model.embed_query(query)
faiss.normalize_L2(np.array([query_vector]))  # ‚úÖ normalize query too

# Search
results = vectorstore.similarity_search_with_score(query, k=3)
for doc, score in results:
    print("üìÑ", doc.page_content[:60])
    print("üî¢ Cosine Similarity:", score)


3RD:::



import os
import pandas as pd
import configparser
import faiss
import numpy as np
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

# Azure credentials
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Load your CSV dataset
csv_path = r"C:\Users\HariharaM12\Downloads\Medical_Data.csv"
df = pd.read_csv(csv_path)

# Create LangChain Documents
documents = []
for _, row in df.iterrows():
    if pd.isna(row["text"]):
        continue
    documents.append(Document(page_content=row["text"], metadata={"source": row["title"]}))

# Chunk the documents
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)
target_doc_title = "2021-10-15_00:00:00.000_Telephone_Encounter"
print(f"\nüß© Chunks for document: {target_doc_title}\n" + "-"*100)
for i, chunk in enumerate(chunks):
    if chunk.metadata["source"] == target_doc_title:
        print(f"[Chunk {i+1}]\n{chunk.page_content}")
        print("-" * 100)

# Create Azure embedding model
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Embed the chunks
texts = [chunk.page_content for chunk in chunks]
metadatas = [chunk.metadata for chunk in chunks]
embeddings = embedding_model.embed_documents(texts)
embeddings = np.array(embeddings).astype("float32")

# Normalize vectors to enable cosine similarity
faiss.normalize_L2(embeddings)

# Build FAISS index using cosine similarity (IndexFlatIP)
index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(embeddings)

# Define query
query = "Based on the clinical note, what medications has the patient received, and what is the current status of those treatments?"

# Embed and normalize query
query_embedding = embedding_model.embed_query(query)
query_embedding = np.array([query_embedding]).astype("float32")
faiss.normalize_L2(query_embedding)

# Perform similarity search
TOP_K = 5
D, I = index.search(query_embedding, TOP_K)

# Get top-K documents
top_docs = [(chunks[i], D[0][rank]) for rank, i in enumerate(I[0])]

# Prepare context for LLM
context = "\n\n".join([doc.page_content for doc, _ in top_docs])
full_prompt = f"""
You are a clinical assistant specialized in analyzing oncology patient records.

Your task is to read the provided clinical notes and answer the medical question based only on the given context.

If the answer is not found in the context, respond with "Not mentioned in the context."

---

üìÑ Context:
{context}

---

‚ùì Question:
{query}
"""

# Call LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

response = llm.invoke(full_prompt)

# Show result
print(f"\nüìò Answer:\n{response.content}")
print(f"\nüîç Top {TOP_K} Retrieved Chunks with Cosine Similarity Scores:\n")

for i, (doc, score) in enumerate(top_docs):
    print(f"--- [Chunk {i+1}] from Document: {doc.metadata['source']} ---")
    print(f"üî¢ Cosine Similarity Score: {score:.4f}")
    print(doc.page_content)
    print("-" * 100)

