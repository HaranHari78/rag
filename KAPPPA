import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# === Load config.ini ==
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# === File paths ===
csv_path = "MEDICAL_DATAS.csv"
output_dir = "output/kappa_only"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, "kappa_only_results.xlsx")
json_path = os.path.join(output_dir, "kappa_only_results.json")

# === Helper functions ===
def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def parse_llm_json(raw_text: str):
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        return json.loads(raw_text)
    except json.JSONDecodeError:
        return []

def normalize_text(text):
    return re.sub(r"[^a-z0-9]", " ", text.lower())

# === Load CSV and chunk ===
df = pd.read_csv(csv_path)
documents = [Document(page_content=row["text"], metadata={"source": row["title"]})
             for _, row in tqdm(df.iterrows(), total=len(df), desc="üìÑ Loading documents") if pd.notna(row["text"])]

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)
chunk_batches = list(batchify(chunks, 20))

# === Embedding setup ===
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in chunk_batches}
    for future in tqdm(as_completed(futures), total=len(futures), desc="üîç Building FAISS"):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# === Similarity search for Kappa only ===
query = "Extract kappa free light chain mg/dL"
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
results = vectorstore.similarity_search(query, k=1000)
print(f"üîç Retrieved {len(results)} chunks")

filtered_chunks = []
for doc in results:
    norm = normalize_text(doc.page_content)
    if "kappa" in norm:
        filtered_chunks.append((doc.metadata.get("source", "Unknown"), doc.page_content))

llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# === Prompting for kappa only ===
final_results = []
batches = list(batchify(filtered_chunks, 10))
for i, batch in enumerate(batches):
    context = [f"Note {j + 1} from {source}::\n{doc}" for j, (source, doc) in enumerate(batch)]
    full_prompt = f"""
Extract the following from the clinical notes below:
- kappa free light chain (mg/dL only)
- lab test date (YYYY-MM-DD or partial)
- most relevant evidence sentence
- source document title

Return list of JSON objects like:
[
  {{
    "kappa_flc": "<value mg/dL>",
    "date_of_lab": "<date>",
    "evidence_sentences": ["<sentence>"],
    "source_document": "<title>"
  }}
]

--- CONTEXT START ---
{json.dumps(context)}
--- CONTEXT END ---
"""
    try:
        response = llm.invoke(full_prompt)
        print(f"üß† Batch {i+1} response preview: {response.content[:300]}")
        parsed = parse_llm_json(response.content)
        final_results.extend(parsed)
    except Exception as e:
        print(f"‚ùå Failed batch {i + 1}: {e}")

# === Filter valid kappa and export ===
valid_entries = []
for item in final_results:
    kappa = item.get("kappa_flc", "")
    if isinstance(kappa, str) and "mg/dl" in kappa.lower():
        evs = item.get("evidence_sentences", [])
        evs = evs[0] if isinstance(evs, list) and evs else ""
        valid_entries.append({
            "source_document": item.get("source_document", ""),
            "kappa_flc": kappa.strip(),
            "date_of_lab": item.get("date_of_lab", "").strip(),
            "evidence_sentences": evs.strip(),
            "context": json.dumps({
                "kappa_flc": kappa.strip(),
                "date_of_lab": item.get("date_of_lab", "").strip(),
                "evidence_sentences": evs.strip()
            })
        })

kappa_df = pd.DataFrame(valid_entries)
kappa_df.drop_duplicates(subset=["source_document", "kappa_flc", "date_of_lab", "evidence_sentences"], inplace=True)
kappa_df.to_excel(output_path, index=False)
with open(json_path, "w", encoding="utf-8") as jf:
    json.dump(valid_entries, jf, indent=2, ensure_ascii=False)

print(f"‚úÖ Saved Excel: {output_path}")
print(f"‚úÖ Saved JSON: {json_path}")
