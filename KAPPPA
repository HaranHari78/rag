import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# === Load config.ini ===
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# === Paths ===
csv_path = "MEDICAL_DATAS.csv"
output_dir = "output/kappa"
os.makedirs(output_dir, exist_ok=True)
output_excel_path = os.path.join(output_dir, "kappa_extracted_results.xlsx")
output_json_path = os.path.join(output_dir, "kappa_extracted_results.json")

# === Helpers ===
def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def parse_llm_json(raw_text: str) -> str:
    try:
        pattern = r"```(?:json)?\s*(.*?)```"
        match = re.search(pattern, raw_text, flags=re.DOTALL)
        content = match.group(1).strip() if match else raw_text.strip()
        if content.startswith("json"):
            content = content[len("json"):].strip()
        return json.dumps(json.loads(content))
    except Exception:
        return json.dumps(json.loads(raw_text.strip()))

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# === Load CSV and Chunk ===
df = pd.read_csv(csv_path)
documents = []
for _, row in tqdm(df.iterrows(), total=len(df), desc="üìÑ Loading documents"):
    if pd.notna(row["text"]):
        documents.append(Document(page_content=row["text"], metadata={"source": row["title"]}))

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)
chunk_batches = list(batchify(chunks, 20))

# === Embedding ===
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# === Build FAISS Index ===
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in chunk_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index_kappa")

# === Similarity Search ===
query = "Extract kappa free light chain values in mg/dL from the clinical note"
vectorstore = FAISS.load_local("faiss_index_kappa", embeddings=embedding_model, allow_dangerous_deserialization=True)
results = vectorstore.similarity_search(query, k=1000)

# === Filter Chunks ===
filtered_chunks = []
for doc in results:
    norm = normalize_text(doc.page_content)
    if "kappa" in norm and "mg/dl" in norm:
        filtered_chunks.append((doc.metadata.get("source", "Unknown"), doc.page_content))

# === LLM Setup ===
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# === Run LLM on batches ===
final_results = []
batches = list(batchify(filtered_chunks, 10))

for i, batch in enumerate(batches):
    context = [{"source": src, "content": doc} for src, doc in batch]
    prompt = f"""
You are a medical extraction assistant. From the clinical notes below, extract the following only if explicitly stated:

- Kappa free light chain (in mg/dL)
- Associated lab date (YYYY-MM-DD or partial)
- The best sentence that supports the extracted value

Respond in strict JSON like this:

[
  {{
    "kappa_flc": "0.25 mg/dL",
    "date_of_lab": "2023-06-21",
    "evidence_sentences": "...",
    "source_document": "2023-06-21_Lab_123"
  }}
]

DO NOT guess. Only include entries that mention "kappa" and "mg/dL".

--- CONTEXT START ---
{json.dumps(context, indent=2)}
--- CONTEXT END ---
"""
    try:
        response = llm.invoke(prompt)
        cleaned = parse_llm_json(response.content)
        extracted_items = json.loads(cleaned)
        for item in extracted_items:
            final_results.append(item)
    except Exception as e:
        print(f"‚ùå Batch {i+1} failed: {e}")
        continue

# === Deduplicate and filter ===
filtered = []
seen_docs = set()
for item in final_results:
    kappa = item.get("kappa_flc", "")
    if isinstance(kappa, str) and "mg/dL" in kappa and item["source_document"] not in seen_docs:
        seen_docs.add(item["source_document"])
        item["context"] = json.dumps({
            "kappa_flc": item["kappa_flc"],
            "date_of_lab": item.get("date_of_lab", ""),
            "evidence_sentences": item.get("evidence_sentences", "")
        })
        filtered.append(item)

# === Save output ===
df_kappa = pd.DataFrame(filtered)
df_kappa.to_excel(output_excel_path, index=False)
with open(output_json_path, "w", encoding="utf-8") as jf:
    json.dump(filtered, jf, indent=2, ensure_ascii=False)

print(f"‚úÖ Saved extracted kappa values to:\n- Excel: {output_excel_path}\n- JSON:  {output_json_path}")
