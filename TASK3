Traceback (most recent call last):
  File "C:\Users\HariharaM12\PycharmProjects\Task3\main.py", line 196, in <module>
    df["evidence_key"] = df["evidence_sentences"].astype(str).str.lower().str.replace(r"\s+", " ", regex=True).str.strip()
                         ~~^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task3\.venv\Lib\site-packages\pandas\core\frame.py", line 4107, in __getitem__
    indexer = self.columns.get_loc(key)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task3\.venv\Lib\site-packages\pandas\core\indexes\range.py", line 417, in get_loc
    raise KeyError(key)
KeyError: 'evidence_sentences'

Process finished with exit code 1




import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Load dataset
csv_path = "MEDICAL_DATAS.csv"
df = pd.read_csv(csv_path)

# Prepare documents
documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    documents.append(Document(page_content=row["text"], metadata={"source": str(row["title"])}))

# Chunking
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)
batches = list(batchify(chunks, 20))

# Embedding model
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build FAISS index
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# Vector DB search for Talquetamab context
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Talquetamab or TAL treatment details, dose, frequency, toxicity, trial participation"
results = vectorstore.similarity_search(query, k=1000)

# Filter chunks mentioning Talquetamab
filtered_chunks = []
for doc in results:
    norm_text = normalize_text(doc.page_content)
    if "tal" in norm_text or "talquetamab" in norm_text:
        filtered_chunks.append({"title": doc.metadata.get("source", "Unknown"), "content": doc.page_content})

# Setup LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Prompt for Talquetamab field extraction
final_results = []
batches = list(batchify(filtered_chunks, 10))

for i, batch in enumerate(batches):
    context = "\n\n".join(f"Note {j + 1}:\n{item['content']}" for j, item in enumerate(batch))
    titles = [item["title"] for item in batch]

   full_prompt = f"""
You are a medical data extraction assistant. Your task is to identify and extract key details related to Talquetamab (also called "TAL") from clinical notes.

Your response must only include information when it is explicitly stated in the note. Do not infer or guess values. If no value is mentioned, use null or "Unknown".

üìå Specifically extract the following fields:

- "talquetamab_initiation_date": The exact date Talquetamab treatment was started.
- "talquetamab_dose": Dosage amount (e.g., 0.4 mg/kg weekly, or 800 mcg/kg).
- "talquetamab_frequency": Administration schedule (e.g., weekly, biweekly).
- "talquetamab_duration": Mentioned or inferred treatment length if explicitly described.
- "talquetamab_response": Any description of clinical response (e.g., PR, VGPR, CR, progression).
- "talquetamab_toxicity": Any side effects or adverse reactions clearly linked to Talquetamab.
- "talquetamab_trial_participation": If the patient is enrolled in a clinical trial involving Talquetamab (mention trial name or phase if available).
- "evidence_sentences": Provide exact sentences from the note that support the above information.

Respond in strict JSON format like below:

```json
{{
  "talquetamab_initiation_date": "2024-01-24",
  "talquetamab_dose": "0.4 mg/kg",
  "talquetamab_frequency": "weekly",
  "talquetamab_duration": "6 months",
  "talquetamab_response": "Very good partial response (VGPR)",
  "talquetamab_toxicity": "Grade 2 cytokine release syndrome",
  "talquetamab_trial_participation": "MonumenTAL-1 trial (Phase 1)",
  "evidence_sentences": [
    "The patient was started on Talquetamab 0.4 mg/kg weekly on 01/24/2024.",
    "Enrolled in the MonumenTAL-1 trial.",
    "Experienced Grade 2 CRS after the second dose.",
    "Achieved VGPR after 6 cycles."
  ]
}}

---
üìÑ Context:
{context}
"""

try:
    print(f"\nüß† Processing batch {i + 1}/{len(batches)}...")
    response = llm.invoke(full_prompt)
    cleaned = parse_llm_json(response.content)
    batch_result = json.loads(cleaned)
    for item, title in zip(batch_result, titles):
        item["source_document"] = title
    final_results.extend(batch_result)
except Exception as e:
    print(f"‚ùå Failed batch {i + 1}: {e}")

# Post-process and save

# Remove invalid entries (non-dicts)
final_results = [row for row in final_results if isinstance(row, dict)]

# Define required fields
required_keys = [
    "source_document", "talquetamab_initiation_date", "talquetamab_dose",
    "talquetamab_frequency", "talquetamab_duration", "talquetamab_response",
    "talquetamab_toxicity", "talquetamab_trial_participation", "evidence_sentences"
]

# Add missing keys with None
for row in final_results:
    for key in required_keys:
        row.setdefault(key, None)

# Convert to DataFrame
df = pd.DataFrame(final_results)

# Join list of evidence sentences into string if column exists
if "evidence_sentences" in df.columns:
    df["evidence_sentences"] = df["evidence_sentences"].apply(
        lambda x: "\n".join(x) if isinstance(x, list) else x
    )

    # Normalize and deduplicate by evidence content
    df["evidence_key"] = df["evidence_sentences"].astype(str).str.lower().str.replace(r"\s+", " ", regex=True).str.strip()
    df = df.drop_duplicates(subset=["evidence_key"])
    df.drop(columns=["evidence_key"], inplace=True)

# Reorder columns safely if present
existing_cols = [col for col in required_keys if col in df.columns]
df = df[existing_cols]

# Save
output_dir = r"C:\Users\HariharaM12\PycharmProjects\Task3"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, "Talquetamab_Extraction_Output.xlsx")
df.to_excel(output_path, index=False)

print(f"\n‚úÖ Final Talquetamab file saved: {output_path}")
