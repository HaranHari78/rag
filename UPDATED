Traceback (most recent call last):
  File "C:\Users\HariharaM12\PycharmProjects\Task2\main.py", line 160, in <module>
    response = llm.invoke(full_prompt)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 372, in invoke
    self.generate_prompt(
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 957, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 776, in generate
    self._generate_with_cache(
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1022, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\langchain_openai\chat_models\base.py", line 1071, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\openai\_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\openai\_base_client.py", line 972, in request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\httpx\_transports\default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 256, in handle_request
    raise exc from None
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 103, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 136, in handle_request
    raise exc
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 106, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 177, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 217, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\Task2\.venv\Lib\site-packages\httpcore\_backends\sync.py", line 128, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\ssl.py", line 1295, in recv
    return self.read(buflen)
           ^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\ssl.py", line 1168, in read
    return self._sslobj.read(len)
           ^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

Process finished with exit code -1073741510 (0xC000013A: interrupted by Ctrl+C)












import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

# Azure credentials
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Helpers
def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def numeric_only(value: str) -> bool:
    return bool(re.match(r"^[<>]?\d+(\.\d+)?$", value.strip()))

def clean_numeric(val: str) -> str:
    match = re.findall(r"[<>]?\d+\.?\d*", val)
    return match[0] if match else ""

# Load dataset
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# Grouped chunks per document
splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=500)
grouped_batches = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    source = row["title"]
    chunks = splitter.split_text(row["text"])
    doc_chunks = [Document(page_content=chunk, metadata={"source": source}) for chunk in chunks]
    if doc_chunks:
        grouped_batches.append(doc_chunks)

# Embedding model
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build FAISS index
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# Vector DB Search
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract numeric values for kappa, lambda, and ratio only. No units."
results = vectorstore.similarity_search(query, k=1000)

# Filter matching content
filtered_chunks = []
for doc in results:
    norm_text = normalize_text(doc.page_content)
    source_title = doc.metadata.get("source", "Unknown")
    if source_title != "Unknown" and ('kappa' in norm_text or 'lambda' in norm_text or 'ratio' in norm_text):
        filtered_chunks.append({"title": source_title, "content": doc.page_content})

# Regroup filtered chunks per doc
grouped_filtered = {}
for chunk in filtered_chunks:
    grouped_filtered.setdefault(chunk["title"], []).append(chunk["content"])

# Setup LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Run LLM
final_results = []
for i, (doc_title, chunks) in enumerate(grouped_filtered.items()):
    json_context = [{"note_id": j + 1, "title": doc_title, "content": chunk} for j, chunk in enumerate(chunks)]

    full_prompt = f"""
You are a medical information extraction assistant. Your task is to extract lab results from clinical notes.

Your goal is to extract ONLY numeric values for the following fields. Do NOT include units (like mg/dL or mg/L). Do NOT include text terms like "normal" or "elevated". Only numeric forms allowed (e.g., 1.2, <0.3, >4.5).

- ✅ Kappa free light chains
- ✅ Lambda free light chains
- ✅ Kappa/Lambda ratio
- ✅ Lab test date (YYYY-MM-DD or partial as YYYY-XX-XX)
- ✅ Evidence sentences (verbatim sentences that contain the extracted values)
IMPORTANT:
- If the date is incomplete (e.g., only month and year), fill the missing parts with "XX".
- DO NOT guess or infer dates.

Respond in strict JSON format like:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"🧠 Running batch {i+1}/{len(grouped_filtered)}...")
        response = llm.invoke(full_prompt)
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item in batch_result:
            item["source_document"] = doc_title
            item["kappa_flc"] = clean_numeric(item.get("kappa_flc", ""))
            item["lambda_flc"] = clean_numeric(item.get("lambda_flc", ""))
            item["kappa_lambda_ratio"] = clean_numeric(item.get("kappa_lambda_ratio", ""))
            item["context"] = json.dumps(item, indent=2)
        final_results.extend(batch_result)
    except Exception as e:
        print(f"❌ Failed batch {i+1}: {e}")

# Post-filtering for numeric-only
numeric_filtered = []
for row in final_results:
    kappa = row.get("kappa_flc", "")
    lambd = row.get("lambda_flc", "")
    ratio = row.get("kappa_lambda_ratio", "")
    if all(map(numeric_only, [kappa, lambd, ratio])):
        numeric_filtered.append(row)

# Format & Save
for row in numeric_filtered:
    if isinstance(row.get("evidence_sentences"), list):
        row["evidence_sentences"] = "\n".join(row["evidence_sentences"])

df = pd.DataFrame(numeric_filtered)
df = df[["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]]
df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], inplace=True)

output_dir = r"C:\\Users\\HariharaM12\\PycharmProjects\\Task2"
os.makedirs(output_dir, exist_ok=True)

excel_path = os.path.join(output_dir, "Output2.xlsx")
json_path = os.path.join(output_dir, "Output2.json")

df.to_excel(excel_path, index=False)
df.to_json(json_path, orient="records", indent=2)

print(f"\n✅ Excel saved: {excel_path}")
print(f"✅ JSON saved: {json_path}")
