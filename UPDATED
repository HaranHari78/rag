import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
import hashlib

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[\d.]+", val)
    return match.group(0) if match else ""

# Load dataset
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# Dynamic chunking per doc
grouped_batches = []

for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue

    text = row["text"]
    source = row["title"]
    word_count = len(text.split())

    if word_count < 500:
        chunk_size = 1000
        chunk_overlap = 0
    elif word_count < 2000:
        chunk_size = 1500
        chunk_overlap = 250
    else:
        chunk_size = 3000
        chunk_overlap = 500

    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    doc_chunks = [Document(page_content=chunk, metadata={"source": source}) for chunk in chunks]

    if doc_chunks:
        grouped_batches.append(doc_chunks)

# Embeddings
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build FAISS
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# Vector search
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/L), lambda free light chain (mg/L), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

# Filter relevant chunks
filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    norm_text = normalize_text(content)

    if 'kappa' in norm_text or 'lambda' in norm_text or 'ratio' in norm_text:
        filtered_chunks.append({"title": source_title, "content": content})

print(f"\nüî¢ Filtered Chunks Count: {len(filtered_chunks)}")
for i, c in enumerate(filtered_chunks[:3]):
    print(f"\nüîπ Example {i+1} Title: {c['title']}")
    print(c['content'][:400])

# Regroup chunks by title
grouped_filtered = {}
for chunk in filtered_chunks:
    grouped_filtered.setdefault(chunk["title"], []).append(chunk["content"])

print(f"üóÇÔ∏è Grouped Documents Count: {len(grouped_filtered)}")

# LLM setup
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Run LLM
final_results = []
used_hashes = set()

for i, (doc_title, chunks) in enumerate(grouped_filtered.items()):
    json_context = [{"note_id": j + 1, "title": doc_title, "content": chunk} for j, chunk in enumerate(chunks)]

    full_prompt = f"""
You are a clinical data extraction assistant. For the given document, extract the following **only if all values are explicitly present with correct units (mg/dL or mg/L)**:

- `kappa_flc`: Kappa free light chain (numeric + unit like mg/dL or mg/L)
- `lambda_flc`: Lambda free light chain (numeric + unit)
- `kappa_lambda_ratio`: The ratio value (can be raw number or include < or >)
- `date_of_lab`: If exact date found, use that. If partial (e.g. year only), format as:
  - 2024 ‚Üí 2024-XX-XX
  - 2023-05 ‚Üí 2023-05-XX
- `evidence_sentences`: Sentence(s) where **ALL 3 values are clearly mentioned**. Do not extract values unless this sentence confirms all 3.

üî¥ Do NOT guess or infer. If even one value is missing, SKIP.

Respond only in strict JSON format:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\nüß† Running batch {i+1}/{len(grouped_filtered)}...")
        response = llm.invoke(full_prompt)
        print("\nüîé Raw LLM Response:\n", response.content)
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item in batch_result:
            item["kappa_flc"] = clean_numeric(item.get("kappa_flc", ""))
            item["lambda_flc"] = clean_numeric(item.get("lambda_flc", ""))
            item["kappa_lambda_ratio"] = clean_numeric(item.get("kappa_lambda_ratio", ""))
            item["source_document"] = doc_title
            item["context"] = json.dumps(item, indent=2)

            evidence_key = (
                item["kappa_flc"]
                + item["lambda_flc"]
                + item["kappa_lambda_ratio"]
                + ''.join(item.get("evidence_sentences", []))
                + item["source_document"]
            )
            evidence_hash = hashlib.md5(evidence_key.encode()).hexdigest()
            if evidence_hash not in used_hashes:
                final_results.append(item)
                used_hashes.add(evidence_hash)
            else:
                print(f"üö´ Duplicate skipped for doc: {doc_title}")

    except Exception as e:
        print(f"‚ùå Failed batch {i+1}: {e}")

# Clean output
df = pd.DataFrame(final_results)

if "evidence_sentences" in df.columns and not df.empty:
    if isinstance(df["evidence_sentences"].iloc[0], list):
        df["evidence_sentences"] = df["evidence_sentences"].apply(lambda x: "\n".join(x))

cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], inplace=True)

    output_dir = r"C:\\Users\\HariharaM12\\PycharmProjects\\task_2"
    os.makedirs(output_dir, exist_ok=True)

    excel_path = os.path.join(output_dir, "Output1.xlsx")
    json_path = os.path.join(output_dir, "Output1.json")

    df.to_excel(excel_path, index=False)
    df.to_json(json_path, orient="records", indent=2)

    print(f"\n‚úÖ Excel saved: {excel_path}")
    print(f"‚úÖ JSON saved: {json_path}")
else:
    print("‚ö†Ô∏è No results to save. The final DataFrame is empty or missing expected columns.")






TO GET UNIT < >:::: PERFECTLY WORKS 


import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
import hashlib

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[\d.]+", val)
    return match.group(0) if match else ""

def find_enriched_value(value, evidence_text):
    if not value or not evidence_text:
        return value
    value_str = str(value).strip()
    pattern = rf"([<>]?\s*{re.escape(value_str)}\s*(?:mg/dL|mg/L)?)"
    match = re.search(pattern, evidence_text, re.IGNORECASE)
    if match:
        return match.group(1).replace(" ", "")
    return value_str

# Load dataset
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# Dynamic chunking per doc
grouped_batches = []

for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    text = row["text"]
    source = row["title"]
    word_count = len(text.split())

    if word_count < 500:
        chunk_size = 1000
        chunk_overlap = 0
    elif word_count < 2000:
        chunk_size = 1500
        chunk_overlap = 250
    else:
        chunk_size = 3000
        chunk_overlap = 500

    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    doc_chunks = [Document(page_content=chunk, metadata={"source": source}) for chunk in chunks]

    if doc_chunks:
        grouped_batches.append(doc_chunks)

# Embedding setup
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build FAISS
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# Vector search
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/L), lambda free light chain (mg/L), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

# Filter relevant chunks
filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    norm_text = normalize_text(content)

    if 'kappa' in norm_text or 'lambda' in norm_text or 'ratio' in norm_text:
        filtered_chunks.append({"title": source_title, "content": content})

# Regroup chunks by title
grouped_filtered = {}
for chunk in filtered_chunks:
    grouped_filtered.setdefault(chunk["title"], []).append(chunk["content"])

# LLM setup
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Run LLM and parse
final_results = []
used_hashes = set()

for i, (doc_title, chunks) in enumerate(grouped_filtered.items()):
    json_context = [{"note_id": j + 1, "title": doc_title, "content": chunk} for j, chunk in enumerate(chunks)]

    full_prompt = f"""
You are a clinical data extraction assistant. For the given document, extract the following **only if all values are explicitly present with correct units (mg/dL or mg/L)**:

- `kappa_flc`: Kappa free light chain (numeric + unit like mg/dL or mg/L)
- `lambda_flc`: Lambda free light chain (numeric + unit)
- `kappa_lambda_ratio`: The ratio value (can be raw number or include < or >)
- `date_of_lab`: If exact date found, use that. If partial (e.g. year only), format as:
  - 2024 ‚Üí 2024-XX-XX
  - 2023-05 ‚Üí 2023-05-XX
- `evidence_sentences`: Sentence(s) where **ALL 3 values are clearly mentioned**. Do not extract values unless this sentence confirms all 3.

üî¥ Do NOT guess or infer. If even one value is missing, SKIP.

Respond only in strict JSON format:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\nüß† Running batch {i+1}/{len(grouped_filtered)}...")
        response = llm.invoke(full_prompt)
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item in batch_result:
            kappa_clean = clean_numeric(item.get("kappa_flc", ""))
            lambda_clean = clean_numeric(item.get("lambda_flc", ""))
            ratio_clean = clean_numeric(item.get("kappa_lambda_ratio", ""))
            evidence_text = "\n".join(item.get("evidence_sentences", []))

            # Enrich with units/symbols
            item["kappa_flc"] = find_enriched_value(kappa_clean, evidence_text)
            item["lambda_flc"] = find_enriched_value(lambda_clean, evidence_text)
            item["kappa_lambda_ratio"] = find_enriched_value(ratio_clean, evidence_text)

            item["source_document"] = doc_title
            item["context"] = json.dumps(item, indent=2)

            evidence_key = (
                item["kappa_flc"]
                + item["lambda_flc"]
                + item["kappa_lambda_ratio"]
                + ''.join(item.get("evidence_sentences", []))
                + item["source_document"]
            )
            evidence_hash = hashlib.md5(evidence_key.encode()).hexdigest()
            if evidence_hash not in used_hashes:
                final_results.append(item)
                used_hashes.add(evidence_hash)
            else:
                print(f"üö´ Duplicate skipped for doc: {doc_title}")

    except Exception as e:
        print(f"‚ùå Failed batch {i+1}: {e}")

# Final cleanup and output
df = pd.DataFrame(final_results)

if "evidence_sentences" in df.columns and not df.empty:
    if isinstance(df["evidence_sentences"].iloc[0], list):
        df["evidence_sentences"] = df["evidence_sentences"].apply(lambda x: "\n".join(x))

cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], inplace=True)

    output_dir = r"C:\\Users\\HariharaM12\\PycharmProjects\\task_2"
    os.makedirs(output_dir, exist_ok=True)

    excel_path = os.path.join(output_dir, "Output1_with_units.xlsx")
    json_path = os.path.join(output_dir, "Output1_with_units.json")

    df.to_excel(excel_path, index=False)
    df.to_json(json_path, orient="records", indent=2)

    print(f"\n‚úÖ Excel saved: {excel_path}")
    print(f"‚úÖ JSON saved: {json_path}")
else:
    print("‚ö†Ô∏è No results to save. The final DataFrame is empty or missing expected columns.")



WITHOUT EVIDENCE DROPING ::: ALWAYS USE THIS ONE 


import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
import hashlib

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[\d.]+", val)
    return match.group(0) if match else ""

def find_enriched_value(value, evidence_text):
    if not value or not evidence_text:
        return value
    value_str = str(value).strip()
    pattern = rf"([<>]?\s*{re.escape(value_str)}\s*(?:mg/dL|mg/L)?)"
    match = re.search(pattern, evidence_text, re.IGNORECASE)
    if match:
        return match.group(1).replace(" ", "")
    return value_str

# Load dataset
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# Dynamic chunking per doc
grouped_batches = []

for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    text = row["text"]
    source = row["title"]
    word_count = len(text.split())

    if word_count < 500:
        chunk_size = 1000
        chunk_overlap = 0
    elif word_count < 2000:
        chunk_size = 1500
        chunk_overlap = 250
    else:
        chunk_size = 3000
        chunk_overlap = 500

    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    doc_chunks = [Document(page_content=chunk, metadata={"source": source}) for chunk in chunks]

    if doc_chunks:
        grouped_batches.append(doc_chunks)

# Embedding setup
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build FAISS
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# Vector search
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/L), lambda free light chain (mg/L), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

# Filter relevant chunks
filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    norm_text = normalize_text(content)

    if 'kappa' in norm_text or 'lambda' in norm_text or 'ratio' in norm_text:
        filtered_chunks.append({"title": source_title, "content": content})

# Regroup chunks by title
grouped_filtered = {}
for chunk in filtered_chunks:
    grouped_filtered.setdefault(chunk["title"], []).append(chunk["content"])

# LLM setup
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Run LLM and parse
final_results = []
used_hashes = set()

for i, (doc_title, chunks) in enumerate(grouped_filtered.items()):
    json_context = [{"note_id": j + 1, "title": doc_title, "content": chunk} for j, chunk in enumerate(chunks)]

    full_prompt = f"""
You are a clinical data extraction assistant. For the given document, extract the following **only if all values are explicitly present with correct units (mg/dL or mg/L)**:

- `kappa_flc`: Kappa free light chain (numeric + unit like mg/dL or mg/L)
- `lambda_flc`: Lambda free light chain (numeric + unit)
- `kappa_lambda_ratio`: The ratio value (can be raw number or include < or >)
- `date_of_lab`: If exact date found, use that. If partial (e.g. year only), format as:
  - 2024 ‚Üí 2024-XX-XX
  - 2023-05 ‚Üí 2023-05-XX
- `evidence_sentences`: Sentence(s) where **ALL 3 values are clearly mentioned**. Do not extract values unless this sentence confirms all 3.

üî¥ Do NOT guess or infer. If even one value is missing, SKIP.

Respond only in strict JSON format:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\nüß† Running batch {i+1}/{len(grouped_filtered)}...")
        response = llm.invoke(full_prompt)
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item in batch_result:
            kappa_clean = clean_numeric(item.get("kappa_flc", ""))
            lambda_clean = clean_numeric(item.get("lambda_flc", ""))
            ratio_clean = clean_numeric(item.get("kappa_lambda_ratio", ""))
            evidence_text = "\n".join(item.get("evidence_sentences", []))

            # Enrich with units/symbols
            item["kappa_flc"] = find_enriched_value(kappa_clean, evidence_text)
            item["lambda_flc"] = find_enriched_value(lambda_clean, evidence_text)
            item["kappa_lambda_ratio"] = find_enriched_value(ratio_clean, evidence_text)

            item["source_document"] = doc_title
            item["context"] = json.dumps(item, indent=2)

            evidence_key = (
                item["kappa_flc"]
                + item["lambda_flc"]
                + item["kappa_lambda_ratio"]
                + ''.join(item.get("evidence_sentences", []))
                + item["source_document"]
            )
            evidence_hash = hashlib.md5(evidence_key.encode()).hexdigest()
            if evidence_hash not in used_hashes:
                final_results.append(item)
                used_hashes.add(evidence_hash)
            else:
                print(f"üö´ Duplicate skipped for doc: {doc_title}")

    except Exception as e:
        print(f"‚ùå Failed batch {i+1}: {e}")

# Final cleanup and output
df = pd.DataFrame(final_results)

if "evidence_sentences" in df.columns and not df.empty:
    if isinstance(df["evidence_sentences"].iloc[0], list):
        df["evidence_sentences"] = df["evidence_sentences"].apply(lambda x: "\n".join(x))

cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"],keep='first', inplace=True)

    output_dir = r"C:\\Users\\HariharaM12\\PycharmProjects\\task_2"
    os.makedirs(output_dir, exist_ok=True)

    excel_path = os.path.join(output_dir, "Output_with_units.xlsx")
    json_path = os.path.join(output_dir, "Output_with_units.json")

    df.to_excel(excel_path, index=False)
    df.to_json(json_path, orient="records", indent=2)

    print(f"\n‚úÖ Excel saved: {excel_path}")
    print(f"‚úÖ JSON saved: {json_path}")
else:
    print("‚ö†Ô∏è No results to save. The final DataFrame is empty or missing expected columns.")







import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
import hashlib

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[0-9]+\.?[0-9]*", val)
    return match.group(0) if match else ""

def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# Load dataset
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# Create a flat list of all documents
all_documents = []

for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    text = row["text"]
    source = row["title"]
    word_count = len(text.split())

    if word_count < 500:
        chunk_size = 1000
        chunk_overlap = 0
    elif word_count < 2000:
        chunk_size = 1500
        chunk_overlap = 250
    else:
        chunk_size = 3000
        chunk_overlap = 500

    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    all_documents.extend([Document(page_content=chunk, metadata={"source": source}) for chunk in chunks])

# Batch the documents
grouped_batches = list(batchify(all_documents, 10))

# Embedding setup
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build FAISS
sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(FAISS.from_documents, batch, embedding_model): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# Vector search
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/L), lambda free light chain (mg/L), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

# Filter relevant chunks
filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    norm_text = normalize_text(content)

    if 'kappa' in norm_text or 'lambda' in norm_text or 'ratio' in norm_text:
        filtered_chunks.append({"title": source_title, "content": content})

# LLM setup
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Run LLM and parse
final_results = []

for i, batch in enumerate(batchify(filtered_chunks, 10)):
    json_context = [{"note_id": j + 1, "title": doc["title"], "content": doc["content"]} for j, doc in enumerate(batch)]
    titles = [doc["title"] for doc in batch]

    full_prompt = f"""
You are a clinical data extraction assistant. For the given document, extract the following **only if all values are explicitly present with correct units (mg/dL or mg/L)**:

- `kappa_flc`: Kappa free light chain (numeric + unit like mg/dL or mg/L)
- `lambda_flc`: Lambda free light chain (numeric + unit)
- `kappa_lambda_ratio`: The ratio value (can be raw number or include < or >)
- `date_of_lab`: If exact date found, use that. If partial (e.g. year only), format as:
  - 2024 ‚Üí 2024-XX-XX
  - 2023-05 ‚Üí 2023-05-XX
- `evidence_sentences`: Sentence(s) where **ALL 3 values are clearly mentioned**. Do not extract values unless this sentence confirms all 3.

üî¥ Do NOT guess or infer. If even one value is missing, SKIP.

Respond only in strict JSON format:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\nüß† Running batch {i+1}...")
        response = llm.invoke(full_prompt)
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item, title in zip(batch_result, titles):
            item["kappa_flc"] = clean_numeric(item.get("kappa_flc", ""))
            item["lambda_flc"] = clean_numeric(item.get("lambda_flc", ""))
            item["kappa_lambda_ratio"] = clean_numeric(item.get("kappa_lambda_ratio", ""))
            item["source_document"] = title
            item["context"] = json.dumps(item, indent=2)
            final_results.append(item)

    except Exception as e:
        print(f"‚ùå Failed batch {i+1}: {e}")

# Final cleanup and output
df = pd.DataFrame(final_results)

cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], keep='first', inplace=True)

    output_dir = r"C:\\Users\\HariharaM12\\PycharmProjects\\task_2"
    os.makedirs(output_dir, exist_ok=True)

    excel_path = os.path.join(output_dir, "Output_with_unit.xlsx")
    json_path = os.path.join(output_dir, "Output_with_unit.json")

    df.to_excel(excel_path, index=False)
    df.to_json(json_path, orient="records", indent=2)

    print(f"\n‚úÖ Excel saved: {excel_path}")
    print(f"‚úÖ JSON saved: {json_path}")
else:
    print("‚ö†Ô∏è No results to save. The final DataFrame is empty or missing expected columns.")

