import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
import hashlib

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Helpers
def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[\d.]+", val)
    return match.group(0) if match else ""

# Load dataset
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# Dynamic chunking per doc
grouped_batches = []

for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue

    text = row["text"]
    source = row["title"]
    word_count = len(text.split())

    if word_count < 500:
        chunk_size = 1000
        chunk_overlap = 0
    elif word_count < 2000:
        chunk_size = 1500
        chunk_overlap = 250
    else:
        chunk_size = 3000
        chunk_overlap = 500

    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    doc_chunks = [Document(page_content=chunk, metadata={"source": source}) for chunk in chunks]

    if doc_chunks:
        grouped_batches.append(doc_chunks)

# Embeddings
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build FAISS
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# Vector search
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/L), lambda free light chain (mg/L), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

# Filter chunks with strict source match
filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    norm_text = normalize_text(content)

    if ('kappa' in norm_text or 'lambda' in norm_text or 'ratio' in norm_text):
        if source_title in content:
            filtered_chunks.append({"title": source_title, "content": content})

# Regroup
grouped_filtered = {}
for chunk in filtered_chunks:
    grouped_filtered.setdefault(chunk["title"], []).append(chunk["content"])

# LLM setup
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Run LLM
final_results = []
used_hashes = set()

for i, (doc_title, chunks) in enumerate(grouped_filtered.items()):
    json_context = [{"note_id": j + 1, "title": doc_title, "content": chunk} for j, chunk in enumerate(chunks)]

    full_prompt = f"""
You are a medical information extraction assistant. Extract ONLY if the document contains numeric values for:
- Kappa free light chains
- Lambda free light chains
- Kappa/Lambda ratio
Include also:
- Date of lab (format partial as YYYY-MM-XX or YYYY-XX-XX)
- Evidence sentences (must contain the numeric value)
If values are copied or unclear, SKIP.
Respond in JSON:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\nüß† Running batch {i+1}/{len(grouped_filtered)}...")
        response = llm.invoke(full_prompt)
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item in batch_result:
            item["kappa_flc"] = clean_numeric(item.get("kappa_flc", ""))
            item["lambda_flc"] = clean_numeric(item.get("lambda_flc", ""))
            item["kappa_lambda_ratio"] = clean_numeric(item.get("kappa_lambda_ratio", ""))
            item["source_document"] = doc_title
            item["context"] = json.dumps(item, indent=2)

            evidence_key = item["kappa_flc"] + item["lambda_flc"] + item["kappa_lambda_ratio"] + ''.join(item.get("evidence_sentences", []))
            evidence_hash = hashlib.md5(evidence_key.encode()).hexdigest()
            if evidence_hash not in used_hashes:
                final_results.append(item)
                used_hashes.add(evidence_hash)

    except Exception as e:
        print(f"‚ùå Failed batch {i+1}: {e}")

# Clean output
df = pd.DataFrame(final_results)

if "evidence_sentences" in df.columns and not df.empty:
    if isinstance(df["evidence_sentences"].iloc[0], list):
        df["evidence_sentences"] = df["evidence_sentences"].apply(lambda x: "\n".join(x))

cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]

    # Deduplicate strictly
    def normalize_evidence(text):
        if not isinstance(text, str):
            return ""
        return re.sub(r'\s+', ' ', text.strip().lower())

    df["evidence_key"] = df["evidence_sentences"].apply(normalize_evidence)

    df_deduped = df.drop_duplicates(
        subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_key", "source_document"]
    )

    df = df_deduped.drop(columns=["evidence_key"])

    # Save
    output_dir = r"C:\\Users\\HariharaM12\\PycharmProjects\\task_2"
    os.makedirs(output_dir, exist_ok=True)

    excel_path = os.path.join(output_dir, "Output1.xlsx")
    json_path = os.path.join(output_dir, "Output1.json")

    df.to_excel(excel_path, index=False)
    df.to_json(json_path, orient="records", indent=2)

    print(f"\n‚úÖ Excel saved: {excel_path}")
    print(f"‚úÖ JSON saved: {json_path}")
else:
    print("‚ö†Ô∏è No results to save. The final DataFrame is empty or missing expected columns.")


import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config
config = configparser.ConfigParser()
config.read("config.ini")

# Azure credentials
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Paths
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
output_dir = r"C:\Users\HariharaM12\PycharmProjects\Rag1\Task"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, "extracted_results.xlsx")

# Helpers
def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def extract_numeric(value):
    if not isinstance(value, str):
        return value
    match = re.search(r"[\d.]+", value.replace(',', ''))
    return match.group(0) if match else ""

# Load and prepare documents
df = pd.read_csv(csv_path)
documents = [
    Document(page_content=row["text"], metadata={"source": row["title"]})
    for _, row in df.iterrows() if pd.notna(row["text"])
]

# Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)
chunk_batches = list(batchify(chunks, 20))

# Setup embedding
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build FAISS index in parallel
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in chunk_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# Query FAISS
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/L), lambda free light chain (mg/L), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

# Filter relevant chunks
filtered_chunks = []
for doc in results:
    norm = normalize_text(doc.page_content)
    if 'kappa' in norm or 'lambda' in norm or 'ratio' in norm:
        filtered_chunks.append((doc.metadata.get("source", "Unknown"), doc.page_content))

# LLM setup
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Prepare prompt batches
final_results = []
batches = list(batchify(filtered_chunks, 10))
for i, batch in enumerate(batches):
    context = "\n\n".join(f"Note {i + 1} from {source}::\n{doc}" for i, (source, doc) in enumerate(batch))
    prompt = f"""
You are a medical information extraction assistant. Your task is to extract lab results from the provided clinical notes.

From the context below, extract the following values **only if they are explicitly mentioned**:
- Kappa free light chains (mg/L)
- Lambda free light chains (mg/L)
- Kappa/Lambda ratio
- Lab test date (YYYY-MM-DD format)
- Supporting evidence sentence
- The document title the sentence came from (you will be given that)

IMPORTANT:
- If the date is incomplete (e.g., only month and year), fill the missing parts with "XX".
- DO NOT guess or infer dates.
- Return structured JSON list. One object per lab result.
- Include "evidence_sentences" and "source_document".

[
  {{
    "kappa_flc": "<value with unit>",
    "lambda_flc": "<value with unit>",
    "kappa_lambda_ratio": "<numeric ratio>",
    "date_of_lab": "<YYYY-MM-DD or with XX>",
    "evidence_sentences": ["<sentence>"],
    "source_document": "<document title>"
  }}
]

--- CONTEXT START ---
{context}
--- CONTEXT END ---
"""
    try:
        print(f"\nüß† Batch {i + 1}/{len(batches)}")
        response = llm.invoke(prompt)
        cleaned = parse_llm_json(response.content)
        extracted_items = json.loads(cleaned)

        for item, (source_title, _) in zip(extracted_items, batch):
            item["source_document"] = source_title
            item["context"] = json.dumps({
                "kappa_flc": item.get("kappa_flc", ""),
                "lambda_flc": item.get("lambda_flc", ""),
                "kappa_lambda_ratio": item.get("kappa_lambda_ratio", ""),
                "date_of_lab": item.get("date_of_lab", ""),
                "evidence_sentences": item.get("evidence_sentences", [])
            })
            final_results.append(item)

    except Exception as e:
        print(f"‚ùå Failed batch {i + 1}: {e}")

# Save cleaned file
if final_results:
    df = pd.DataFrame(final_results)
    df["evidence_sentences"] = df["evidence_sentences"].apply(lambda x: "\n".join(x) if isinstance(x, list) else str(x))
    for col in ["kappa_flc", "lambda_flc", "kappa_lambda_ratio"]:
        df[col] = df[col].apply(extract_numeric)
    df = df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"])
    column_order = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]
    df = df[column_order]
    df.to_excel(output_path, index=False)
    print(f"\n‚úÖ Saved at: {output_path}")
else:
    print("‚ö†Ô∏è No data extracted.")

    # --- Save just the "context" column to a separate JSONL file ---
    context_output_path = os.path.join(output_dir, "extracted_contexts.jsonl")

    with open(context_output_path, "w", encoding="utf-8") as f:
        for ctx in df["context"]:
            f.write(ctx + "\n")

    print(f"üìù Contexts saved to: {context_output_path}")





import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config
config = configparser.ConfigParser()
config.read("config.ini")

# Azure credentials
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Paths
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
output_dir = r"C:\\Users\\HariharaM12\\PycharmProjects\\Rag1\\Task"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, "extracted_results.xlsx")

# Helpers
def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def get_value_with_unit_loose(field_value, context_text, field_name):
    if not field_value or not context_text:
        return field_value
    try:
        number_pattern = r"[<>]?\s*[\d]+(?:\.\d+)?"
        if field_name in ["kappa_flc", "lambda_flc"]:
            unit_pattern = rf"({number_pattern}\s*(mg/dl|mg/l))"
        else:
            unit_pattern = rf"({number_pattern})"
        matches = re.findall(unit_pattern, context_text, re.IGNORECASE)
        for match in matches:
            full_match = match[0] if isinstance(match, tuple) else match
            cleaned = re.sub(r'[<>\s]', '', full_match)
            if cleaned.startswith(field_value.replace('.', '')) or field_value in cleaned:
                return full_match.replace(" ", "")
    except:
        return field_value
    return field_value

def flatten_context(context_json):
    obj = json.loads(context_json)
    return " ".join(str(v) if not isinstance(v, list) else " ".join(v) for v in obj.values())

def is_valid_numeric(s):
    return bool(re.search(r"[<>]?\s*\d+(\.\d+)?", s))

# Load and prepare documents
df = pd.read_csv(csv_path)
documents = [
    Document(page_content=row["text"], metadata={"source": row["title"]})
    for _, row in df.iterrows() if pd.notna(row["text"])
]

# Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)
chunk_batches = list(batchify(chunks, 20))

# Setup embedding
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build FAISS index in parallel
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in chunk_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# Query FAISS
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/L), lambda free light chain (mg/L), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

# Filter relevant chunks
filtered_chunks = []
for doc in results:
    norm = normalize_text(doc.page_content)
    if 'kappa' in norm or 'lambda' in norm or 'ratio' in norm:
        filtered_chunks.append((doc.metadata.get("source", "Unknown"), doc.page_content))

# LLM setup
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Prepare prompt batches
final_results = []
batches = list(batchify(filtered_chunks, 10))
for i, batch in enumerate(batches):
    context = "\n\n".join(f"Note {i + 1} from {source}::\n{doc}" for i, (source, doc) in enumerate(batch))
    prompt = f"""
You are a medical information extraction assistant. Your task is to extract lab results from the provided clinical notes.

From the context below, extract the following values **only if they are explicitly mentioned**:
- Kappa free light chains (mg/L)
- Lambda free light chains (mg/L)
- Kappa/Lambda ratio
- Lab test date (YYYY-MM-DD format)
- Supporting evidence sentence
- The document title the sentence came from (you will be given that)

IMPORTANT:
- If the date is incomplete (e.g., only month and year), fill the missing parts with "XX".
- DO NOT guess or infer dates.
- Return structured JSON list. One object per lab result.
- Include "evidence_sentences" and "source_document".

[
  {{
    "kappa_flc": "<value with unit>",
    "lambda_flc": "<value with unit>",
    "kappa_lambda_ratio": "<numeric ratio>",
    "date_of_lab": "<YYYY-MM-DD or with XX>",
    "evidence_sentences": ["<sentence>"],
    "source_document": "<document title>"
  }}
]

--- CONTEXT START ---
{context}
--- CONTEXT END ---
"""
    try:
        print(f"\nüß† Batch {i + 1}/{len(batches)}")
        response = llm.invoke(prompt)
        cleaned = parse_llm_json(response.content)
        extracted_items = json.loads(cleaned)

        for item, (source_title, _) in zip(extracted_items, batch):
            item["source_document"] = source_title
            item["context"] = json.dumps({
                "kappa_flc": item.get("kappa_flc", ""),
                "lambda_flc": item.get("lambda_flc", ""),
                "kappa_lambda_ratio": item.get("kappa_lambda_ratio", ""),
                "date_of_lab": item.get("date_of_lab", ""),
                "evidence_sentences": item.get("evidence_sentences", [])
            })
            final_results.append(item)

    except Exception as e:
        print(f"‚ùå Failed batch {i + 1}: {e}")

# Save enriched result
if final_results:
    df = pd.DataFrame(final_results)
    df["evidence_sentences"] = df["evidence_sentences"].apply(lambda x: "\n".join(x) if isinstance(x, list) else str(x))

    for i, row in df.iterrows():
        context_text = flatten_context(row["context"])
        kappa = get_value_with_unit_loose(str(row["kappa_flc"]), context_text, "kappa_flc")
        lambda_ = get_value_with_unit_loose(str(row["lambda_flc"]), context_text, "lambda_flc")
        ratio = get_value_with_unit_loose(str(row["kappa_lambda_ratio"]), context_text, "kappa_lambda_ratio")
        df.at[i, "kappa_flc"] = kappa if is_valid_numeric(kappa) else ""
        df.at[i, "lambda_flc"] = lambda_ if is_valid_numeric(lambda_) else ""
        df.at[i, "kappa_lambda_ratio"] = ratio if is_valid_numeric(ratio) else ""
    # Normalize lab values before deduplication
    for col in ["kappa_flc", "lambda_flc", "kappa_lambda_ratio"]:
        df[col] = df[col].astype(str).str.lower().str.replace(r'\s+', '', regex=True)

    # Drop duplicate lab value sets
    df = df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"])

    column_order = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]
    df = df[column_order]

    df.to_excel(output_path, index=False)
    print(f"\n‚úÖ Saved at: {output_path}")

    # Save context JSONL
    context_output_path = os.path.join(output_dir, "extracted_contexts.jsonl")
    with open(context_output_path, "w", encoding="utf-8") as f:
        for ctx in df["context"]:
            f.write(ctx + "\n")
    print(f"üìù Contexts saved to: {context_output_path}")
else:
    print("‚ö†Ô∏è No data extracted.")


