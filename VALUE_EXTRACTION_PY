from langchain_community.vectorstores import FAISS
from langgraph.graph import StateGraph
from typing import TypedDict, List, Dict, Any
import json
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from config import config
from util import batchify, parse_llm_json
from tqdm import tqdm


class GraphState(TypedDict, total=False):
    queries: List[str]
    prompt_extraction_file_name: str
    prompt_validation_file_name: str
    retrieved_documents: List[Dict[str, Any]]
    extracted_labs: List[Dict[str, Any]]
    validated_data: List[Dict[str, Any]]


embedding_model = AzureOpenAIEmbeddings(
    deployment=config["embedding_models"]["text_embedding_3_large"],
    model="text-embedding-3-large",
    openai_api_key=config["azure_openai"]["api_key"],
    azure_endpoint=config["azure_openai"]["endpoint"],
    openai_api_version=config["azure_openai_4O"]["api_version"],
)

faiss_index = FAISS.load_local("../faiss_index", embedding_model, allow_dangerous_deserialization=True)

llm = AzureChatOpenAI(
    deployment_name=config["azure_openai_4O"]["deployment"],
    api_key=config["azure_openai"]["api_key"],
    api_version=config["azure_openai_4O"]["api_version"],
    azure_endpoint=config["azure_openai"]["endpoint"],
    temperature=0,
    model=config["azure_openai_4O"]["model"]
)


def get_prompt(prompt_file_name: str, context: any) -> str:
    with open("prompts/labs/{0}.txt".format(prompt_file_name), "r", encoding="utf-8") as file:
        template = file.read()
    return template.format(context=json.dumps(context, indent=2))


def retrieve_docs_agent(state: GraphState) -> GraphState:
    print(f"[RetrieveDocs] Incoming state keys: {list(state.keys())}")
    # queries = ["lambda", "klc", "flc", "free light chain"]
    all_results = []

    for query in state["queries"]:
        try:
            docs = faiss_index.similarity_search(query, k=1000)
            filtered_docs = [doc for doc in docs if query.lower() in doc.page_content.lower()]
            all_results.extend(filtered_docs)
        except Exception as e:
            print(f"[RetrieveDocs] Error retrieving for query '{query}': {e}")

    unique_documents = set()
    final_documents = []

    for doc in all_results:
        source = doc.metadata.get("source", "unknown_source")
        key = (source, doc.page_content.strip())
        if key not in unique_documents:
            unique_documents.add(key)
            final_documents.append({
                "title": source,
                "medical_notes": doc.page_content.strip()
            })

    print(f"[RetrieveDocs] Retrieved {len(final_documents)} unique documents.")
    new_state: GraphState = {
        **state,
        "retrieved_documents": final_documents
    }

    return new_state


def extraction_agent(state: GraphState) -> GraphState:
    print("[ExtractLabs] Function entered")
    retrieved_documents = state.get("retrieved_documents", [])
    extracted = []

    # Ensure batchify is available in util.py
    for batch in tqdm(batchify(retrieved_documents, 10)):
        try:
            prompt = get_prompt(state['prompt_extraction_file_name'], batch)
            result = llm.invoke(prompt)
            parsed = json.loads(parse_llm_json(result.content))
            if isinstance(parsed, list):
                extracted.extend(parsed)
            else:
                print(f"[ExtractLabs] Unexpected response format: {parsed}")
        except Exception as e:
            print(f"[ExtractLabs] Extraction failed for batch: {e}")

    updated_state: GraphState = {
        **state,
        "extracted_labs": extracted
    }

    print(f"[ExtractLabs] Returning {len(extracted)} extracted records.")
    return updated_state


def validate_extraction_agent(state: GraphState) -> GraphState:
    extracted_data = state.get("extracted_labs", [])
    batches = list(batchify(extracted_data, 10))
    validated = []

    for batch in batches:
        prompt = get_prompt(state['prompt_extraction_file_name'], batch)
        try:
            response = llm.invoke(prompt)
            parsed = json.loads(parse_llm_json(response.content))
            if isinstance(parsed, list):
                validated.extend(parsed)
        except Exception as e:
            print(f"[Validate] Validation parsing error: {e}")

    return {**state, "validated_data": validated}


# Graph setup
builder = StateGraph(GraphState)
builder.add_node("RetrieveDocs", retrieve_docs_agent)
builder.add_node("Extract", extraction_agent)
builder.add_node("Validate", validate_extraction_agent)

builder.set_entry_point("RetrieveDocs")
builder.add_edge("RetrieveDocs", "Extract")
builder.add_edge("Extract", "Validate")
builder.set_finish_point("Validate")

graph = builder.compile()
print("[LangGraph] Graph compiled")

app = graph


def invoke_value_extraction(queries, prompt_extraction_file_path, prompt_validation_file_path):
    result = app.invoke(
        {
            "queries": queries,
            "prompt_extraction_file_name": prompt_extraction_file_path,
            "prompt_validation_file_name": prompt_validation_file_path
        }
    )

    return result
# if __name__ == "__main__":
#     result = app.invoke(
#         {
#             "queries": ["lambda", "klc", "flc", "free light chain"],
#             "prompt_extraction_file_name": "prompts/labs/flca_extraction.txt",
#             "prompt_validation_file_name": "prompts/labs/flca_validation.txt"
#           }
#     )
#     print(json.dumps(result['validated_data'], indent=2))
#     df = pd.DataFrame(result['validated_data'])
#
#     # Display the DataFrame
#     df.to_excel('flca.xlsx')
