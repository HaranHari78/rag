import os
import pandas as pd
import configparser
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
from langchain.chains import RetrievalQA
import json
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

# Azure OpenAI credentials
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

print('creating embedding model')
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)


# Batch chunks
def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


def parse_llm_json(raw_text: str) -> str:
    """
    Extract the JSON part from triple-backtick code fences (if present)
    and return a proper JSON string.
    """

    # This pattern captures the text inside ```...``` or ```json...```.
    # (?s) in the pattern or re.DOTALL in findall allows '.' to match newlines.
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)

    if match:
        # If a match was found, override raw_text with the inside of the code fence
        raw_text = match.group(1).strip()
    else:
        # No triple-backticks found, so just strip whitespace
        raw_text = raw_text.strip()

    # Remove a leading "json" word if the LLM included it
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()

    # Try to parse as JSON. If it fails due to single quotes, do a fallback.
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        # Attempt a simple fix by swapping single quotes for double quotes
        # Escape control characters
        print(raw_text)
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)

    # Return a valid JSON string (no triple backticks included)
    return json.dumps(parsed)


#############################Create Chunks and Vector DB - Start ##############################################
# # Load CSV data
# csv_path = r"d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
# df = pd.read_csv(csv_path)
#
# # Create documents
# print('creating documents')
# documents = []
# for _, row in tqdm(df.iterrows(), total=len(df)):
#     if pd.isna(row["text"]):
#         continue
#     documents.append(Document(page_content=row["text"], metadata={"source": row["title"]}))
#
# # Split documents into chunks
# splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
# chunks = splitter.split_documents(documents)
#
# print(f"Total chunks: {len(chunks)}")
#
# batches = list(batchify(chunks, 20))  # You can tune the batch size
#
#
# # Function to create FAISS index from a batch
# def build_faiss(batch):
#     return FAISS.from_documents(batch, embedding_model)
#
#
# # Parallel FAISS creation
# print("Building FAISS vectorstore in parallel...")
# sub_indexes = []
# with ThreadPoolExecutor(max_workers=4) as executor:
#     futures = {executor.submit(build_faiss, batch): batch for batch in batches}
#     for future in tqdm(as_completed(futures), total=len(futures)):
#         try:
#             sub_indexes.append(future.result())
#         except Exception as e:
#             print(f"Batch failed: {e}")

# Merge all sub-indexes into one
# print("Merging FAISS indexes...")
# main_index = sub_indexes[0]
# for sub_index in sub_indexes[1:]:
#     main_index.merge_from(sub_index)
#
# # Save final FAISS index
# print("Saving FAISS index...")
# main_index.save_local("faiss_index")
#############################Create Chunks and Vector DB - End ##############################################


#############################Load and fetch values from Vector DB - Start ##############################################
# Load vector DB
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)

# Query vectorstore directly
query = "What is the patient's M spike level?"  # or "monoclonal protein concentration"
top_k = 1000

# Perform vector similarity search
results = vectorstore.similarity_search(query, k=1000)


def normalize_text(text):
    # Lowercase
    text = text.lower()
    # Replace all non-alphanumeric characters with space
    text = re.sub(r'[^a-z0-9]', ' ', text)
    # Replace multiple spaces with a single space
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# Filtered results list
filtered_chunks = []

for doc in results:
    norm_text = normalize_text(doc.page_content)
    if 'm spike' in norm_text or 'monoclonal' in norm_text:
        filtered_chunks.append((doc.metadata.get("source", "Unknown"), doc.page_content))

# Show filtered results
# print(f"Found {len(filtered_chunks)} relevant chunks out of {len(results)}")
#
# for i, (source, content) in enumerate(filtered_chunks):  # show first 10
#     print(f"\n--- Chunk #{i+1} ---")
#     print("Source:", source)
#     print(content)  # limit to 500 chars


#############################Load and fetch values from Vector DB - End ##############################################


structured_prompt = """
You are a medical information extraction assistant. Your task is to extract Structured SPEP lab results **very carefully** from the clinical context below.

‚ö†Ô∏è Strict rules you MUST follow:

1. Only extract **M-spike values** (e.g., "2.3 g/dL") that are clearly stated as lab results.
2. The **lab date** must be specifically associated with the SPEP or M-spike lab result. **Do not pick unrelated or nearby dates**. Double-check that the date refers to the lab result.
3. The **evidence_sentences** must include all relevant sentences that directly justify the extracted value and lab date.
4. Each lab value should be extracted **per note only** (e.g., "Note 1", "Note 2", etc.). Do not combine values across notes.

Respond strictly in this JSON format:

[
  {{
    "m_spike": "<m spike value>",
    "date_of_lab": "<lab test date associated with this m spike>",
    "evidence_sentences": ["<sentence 1>", "<sentence 2>", ...]
  }}
]

If no valid M-spike lab result is found in a note, skip that note and do not include any entry.

Here is the context:
{context}
"""

prompt_template = PromptTemplate(
    input_variables=["context"],
    template=structured_prompt
)

# Step 3: Setup LLM and chain
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

llm_chain = LLMChain(llm=llm, prompt=prompt_template)

# Step 4: Run LLM on each batch and collect results
final_results = []
batches = list(batchify(filtered_chunks, 10))
for i, batch in enumerate(batches):
    batch_context = "\n\n".join(
        f"Note {i + 1}:\n{doc}" for i, (_, doc) in enumerate(batch) # Year: {} \n
    )
    try:
        print(f"\nüß† Processing batch {i + 1}/{len(batches)}...")

        output = llm_chain.run({"context": batch_context})
        output = parse_llm_json(output)
        json_result = json.loads(output)
        final_results.extend(json_result)
    except json.JSONDecodeError:
        print(f"‚ùå Failed to parse JSON in batch {i + 1}. Output:\n{output}")
    except Exception as e:
        print(f"‚ùå Error in batch {i + 1}: {e}")

# Step 5: Show collated result
# print(f"\n‚úÖ Extracted {len(final_results)} M-spike entries in total.")
# for entry in final_results[:5]:  # Show first 5
#     print(json.dumps(entry, indent=2))
df_results = pd.DataFrame(final_results)

# Optional: Reorder columns
df_results = df_results[["m_spike", "date_of_lab", "evidence_sentences"]]

# Save to Excel
output_path = "m_spike_extracted_results.xlsx"
df_results.to_excel(output_path, index=False)
