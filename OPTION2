import os
import json
import re
import pandas as pd
from kappa_prompt import build_kappa_lambda_prompt
from utils import load_config, call_openai_api

# --- PATHS ---
input_dir = r"C:\Users\HariharaM12\PycharmProjects\Med_Data\output\sentences"
output_json = r"C:\Users\HariharaM12\PycharmProjects\Med_Data\output\fields\kappa_lambda_ratio_grouped.json"
output_excel = r"C:\Users\HariharaM12\PycharmProjects\Med_Data\output\fields\kappa_lambda_ratio_result.xlsx"
os.makedirs(os.path.dirname(output_json), exist_ok=True)

# --- MODEL CONFIG ---
config = load_config()
model = config['gpt_models']['model_gpt4o']

# --- CLEAN JSON WRAPPER ---
def clean_response(resp: str) -> str:
    return re.sub(r'```(?:json)?\n?|\n?```', '', resp).strip()

# --- BATCH RANGE ---
start_index = 0
end_index = 100  # Process only files in this range

# --- FINAL RESULTS ---
results = []

all_files = sorted([f for f in os.listdir(input_dir) if f.endswith(".json")])
batch_files = all_files[start_index:end_index]

for filename in batch_files:
    path = os.path.join(input_dir, filename)
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    doc_title = data.get("document_title", filename)
    lab_sentences = data.get("lab_result_sentences", [])
    if not lab_sentences:
        continue

    prompt = build_kappa_lambda_prompt(" ".join(lab_sentences), doc_title)
    response = call_openai_api(prompt, model)

    if not response:
        print(f"‚ùå No response: {doc_title}")
        continue

    try:
        raw = clean_response(response)
        parsed = json.loads(raw)
        results.append(parsed)
        print(f"‚úÖ Extracted: {doc_title}")
    except Exception as e:
        print(f"‚ö†Ô∏è JSON error: {doc_title} ‚Üí {e}")

# --- SAVE JSON OUTPUT ---
with open(output_json, 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=4)
print(f"\nüìÅ JSON saved ‚Üí {output_json}")

# --- FLATTEN + SAVE EXCEL ---
flattened = []
for entry in results:
    flat = {
        "document_title": entry.get("document_title", ""),
        "kappa_value": entry["kappa_flc"]["value"],
        "kappa_date": entry["kappa_flc"]["date"],
        "kappa_evidence": entry["kappa_flc"]["evidence"],
        "lambda_value": entry["lambda_flc"]["value"],
        "lambda_date": entry["lambda_flc"]["date"],
        "lambda_evidence": entry["lambda_flc"]["evidence"],
        "ratio_value": entry["kappa_lambda_ratio"]["value"],
        "ratio_date": entry["kappa_lambda_ratio"]["date"],
        "ratio_evidence": entry["kappa_lambda_ratio"]["evidence"]
    }
    flattened.append(flat)

df_final = pd.DataFrame(flattened)

# Drop duplicates based on value and evidence fields
df_final.drop_duplicates(
    subset=[
        "kappa_value", "lambda_value", "ratio_value",
        "kappa_evidence", "lambda_evidence", "ratio_evidence"
    ],
    inplace=True
)

# Save to Excel
df_final.to_excel(output_excel, index=False)
print(f"üìä Excel saved ‚Üí {output_excel}")
