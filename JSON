import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

# Azure credentials
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Util functions
def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Load dataset
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# Grouped chunks per document
splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=0)
grouped_batches = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    source = row["title"]
    chunks = splitter.split_text(row["text"])
    doc_chunks = [Document(page_content=chunk, metadata={"source": source}) for chunk in chunks]
    if doc_chunks:
        grouped_batches.append(doc_chunks)

# Embedding model
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build FAISS index
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# Vector DB Search
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/L), lambda free light chain (mg/L), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

# Filter matching content
filtered_chunks = []
for doc in results:
    norm_text = normalize_text(doc.page_content)
    source_title = doc.metadata.get("source", "Unknown")
    if source_title != "Unknown" and ('kappa' in norm_text or 'lambda' in norm_text or 'ratio' in norm_text):
        filtered_chunks.append({"title": source_title, "content": doc.page_content})

# Regroup filtered chunks per doc
grouped_filtered = {}
for chunk in filtered_chunks:
    grouped_filtered.setdefault(chunk["title"], []).append(chunk["content"])

# Setup LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Run LLM
final_results = []
for i, (doc_title, chunks) in enumerate(grouped_filtered.items()):
    json_context = [{"note_id": j + 1, "title": doc_title, "content": chunk} for j, chunk in enumerate(chunks)]

    full_prompt = f"""
You are a medical information extraction assistant. Your task is to extract lab results from clinical notes.

Your goal is to extract the following values, **only from the given document context**, and **strictly ignore or skip the document** if:
- You are unsure where the values come from.
- The values appear to be duplicated from a different unrelated document.
- The document contains no lab results.

Extract these fields only if they are clearly present in the current context:
- ‚úÖ **Kappa free light chains** (must have numeric value and unit, e.g., 1.35 mg/dL, <0.15 mg/dL)
- ‚úÖ **Lambda free light chains** (must have numeric value and unit)
- ‚úÖ **Kappa/Lambda ratio** (numeric ratio, may include < or > symbols)
- ‚úÖ **Lab test date** (clearly stated; if partial date like year/month, format as YYYY-MM-XX or YYYY-XX-XX)
- ‚úÖ **Supporting evidence sentences** (must include the above values in the same sentence or logically linked)

‚ö†Ô∏è **Important Constraints:**
-  Do NOT hallucinate values or infer them from vague summaries.
-  Do NOT use values that are not clearly tied to this specific document context.
-  If the extracted values appear copied from another unrelated document, discard the entire output for this document.
-  Do NOT include diagnosis phrases like "monoclonal kappa detected" unless a real numeric value is stated.
- Do NOT reuse values from one document in another.
- If the values (kappa, lambda, or ratio) are not explicitly present in the context of THIS document, skip them.


‚úÖ Proceed only if the lab values are explicitly mentioned with units.

Respond in strict JSON format like this:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"üß† Running batch {i+1}/{len(grouped_filtered)}...")
        response = llm.invoke(full_prompt)
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item in batch_result:
            item["source_document"] = doc_title
            item["context"] = json.dumps(item, indent=2)
        final_results.extend(batch_result)
    except Exception as e:
        print(f"‚ùå Failed batch {i+1}: {e}")

# Format & Save
for row in final_results:
    if isinstance(row.get("evidence_sentences"), list):
        row["evidence_sentences"] = "\n".join(row["evidence_sentences"])

df = pd.DataFrame(final_results)
df = df[["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]]
df.drop_duplicates(
    subset=["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences"],
    inplace=True
)

# Save Excel & JSON
output_dir = r"C:\Users\HariharaM12\PycharmProjects\Task2"
os.makedirs(output_dir, exist_ok=True)

excel_path = os.path.join(output_dir, "Output.xlsx")
json_path = os.path.join(output_dir, "Output.json")

df.to_excel(excel_path, index=False)
df.to_json(json_path, orient="records", indent=2)

print(f"\n‚úÖ Excel saved: {excel_path}")
print(f"‚úÖ JSON saved: {json_path}")

























import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Splitter (no overlap)
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

# Load dataset
df = pd.read_csv("MEDICAL_DATAS.csv")

# Group document-wise chunks
grouped_batches = []
titles = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    source = row["title"]
    chunks = splitter.split_text(row["text"])
    doc_chunks = [Document(page_content=chunk, metadata={"source": source}) for chunk in chunks]
    if doc_chunks:
        grouped_batches.append(doc_chunks)
        titles.append(source)

# Embedding model
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000,
)

# LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0,
)

# Helper: Parse JSON from model output
def parse_llm_json(raw_text: str) -> list:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    try:
        return json.loads(raw_text)
    except:
        fixed = raw_text.replace("'", '"')
        return json.loads(fixed)

# Helper: Ensure value is in evidence
def value_in_evidence(value: str, evidence: str) -> bool:
    if not value or not evidence:
        return False
    return value in evidence

# Prompt template
def get_prompt(context: list) -> str:
    return f"""
You are a medical information extraction assistant. Your task is to extract lab results from clinical notes.

Extract the following values only if explicitly stated:
- Kappa free light chains (mg/L)
- Lambda free light chains (mg/L)
- Kappa/Lambda ratio (with optional < or >)
- Lab test date associated with these values. If date is incomplete, fill with 'XX'. Do not guess.
- Supporting evidence sentences

‚ùó Do NOT reuse values from one document in another.
‚ùó If the values (kappa, lambda, or ratio) are not explicitly present in the context of THIS document, skip them.
‚ùó Values like "1.24", "1.72", "10.15", "115.57" are common hallucinations. Do NOT include them unless clearly shown in this document.

Respond in strict JSON format like:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

---
üìÑ Context:
{json.dumps(context, indent=2)}
""".strip()

# Final results
final_results = []

# Threaded document processing
def process_doc(doc_chunks, title):
    try:
        faiss_index = FAISS.from_documents(doc_chunks, embedding_model)
        query = "Extract the patient's kappa free light chain (mg/L), lambda free light chain (mg/L), and kappa/lambda ratio, along with the lab date and evidence."
        results = faiss_index.similarity_search(query, k=10)
        context = [doc.page_content for doc in results]

        full_prompt = get_prompt(context)
        response = llm.invoke(full_prompt)
        parsed = parse_llm_json(response.content)

        filtered = []
        for item in parsed:
            item["source_document"] = title
            item["context"] = json.dumps(context)
            if isinstance(item.get("evidence_sentences"), list):
                item["evidence_sentences"] = "\n".join(item["evidence_sentences"])
            ev = item["evidence_sentences"]
            keep = True
            for key in ["kappa_flc", "lambda_flc", "kappa_lambda_ratio"]:
                val = str(item.get(key, "")).strip()
                if val and not value_in_evidence(val, ev):
                    keep = False
                    break
            if keep:
                filtered.append(item)
        return filtered
    except Exception as e:
        print(f"‚ùå Error processing {title}: {e}")
        return []

# Run in parallel
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(process_doc, doc_chunks, title): title for doc_chunks, title in zip(grouped_batches, titles)}
    for future in tqdm(as_completed(futures), total=len(futures)):
        final_results.extend(future.result())

# Final dataframe
df = pd.DataFrame(final_results)
df = df[["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]]
df.drop_duplicates(
    subset=["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences"],
    inplace=True,
)

# Save output
output_dir = r"C:\Users\HariharaM12\PycharmProjects\Task2"
os.makedirs(output_dir, exist_ok=True)
excel_path = os.path.join(output_dir, "Chain_Cleaned_Isolated.xlsx")
json_path = os.path.join(output_dir, "Chain_Cleaned_Isolated.json")

df.to_excel(excel_path, index=False)
df.to_json(json_path, orient="records", indent=2)

print(f"‚úÖ Saved Excel: {excel_path}")
print(f"‚úÖ Saved JSON : {json_path}")

