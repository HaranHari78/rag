import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# --- Load config.ini ---
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# --- Helpers ---
def parse_llm_json(raw_text: str) -> str:
    if not raw_text.strip():
        print("‚ö†Ô∏è Warning: Empty LLM response")
        return "[]"

    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()

    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()

    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        print("‚ö†Ô∏è JSON decode error, trying to fix quotes")
        fixed = raw_text.replace("'", '"')
        try:
            parsed = json.loads(fixed)
        except Exception as e:
            print("‚ùå Still failed:", e)
            return "[]"

    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[0-9]+\.?[0-9]*", val)
    return match.group(0) if match else ""

def enrich_value_with_units(value: str, evidence: str) -> str:
    if not value or not evidence:
        return value
    try:
        float_value = float(value)
    except:
        return value
    pattern = re.compile(rf"([<>]?\s*{re.escape(value)}\s*(?:mg/dl|mg/l)?)", re.IGNORECASE)
    match = pattern.search(evidence)
    return match.group(1).strip() if match else value

def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# --- Load dataset ---
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# --- Create full documents ---
documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    documents.append(Document(page_content=row["text"], metadata={"source": row["title"]}))

# --- Apply fixed chunking ---
splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)
chunks = splitter.split_documents(documents)

# --- Create batches for indexing ---
grouped_batches = list(batchify(chunks, 20))

# --- Embedding & FAISS index ---
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(FAISS.from_documents, batch, embedding_model): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# --- Search relevant chunks ---
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/dL), lambda free light chain (mg/dL), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    norm = content.lower()
    if "kappa" in norm and "lambda" in norm and 'ratio' in norm or ("kappa/lambda" in norm and "kappa_lambda" in norm):
        filtered_chunks.append(Document(page_content=content, metadata={"source": source_title}))

# --- LLM setup ---
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# --- Extraction ---
final_results = []
for i, batch in enumerate(batchify(filtered_chunks, 20)):
    json_context = [{"note_id": j + 1,
                     "title": doc.metadata.get("source", ""),
                     "content": doc.page_content
                     }
                    for j, doc in enumerate(batch)]
    titles = [doc.metadata.get("source", "") for doc in batch]

    full_prompt = f"""
You are a clinical data extraction assistant. For the given document, extract if at least **kappa and lambda** are found with correct units (mg/dL or mg/L).  
If `kappa_lambda_ratio` is missing in the sentence, set it to null.

Recognize alternate names:
- `KLC`, `Kappa light`, `Kappa FLC` = `kappa_flc`
- `LLC`, `Lambda light`, `Lambda FLC` = `lambda_flc`
- `K/L`, `kappa/lambda`, `kappa_lambda` = `kappa_lambda_ratio`

Extraction rules:
- Do NOT extract values that appear in diagnosis summaries, baseline history, or older context.
- If the same evidence or sentence appears across multiple notes, extract it only once (skip duplicates).
- If the date is incomplete (e.g., only month/year), format missing parts as "XX". E.g., "2021-06-XX".
- DO NOT guess or infer dates

Example:
{{
  "kappa_flc": "1.91 mg/dL",
  "lambda_flc": "<0.15 mg/dL",
  "kappa_lambda_ratio": null,
  "date_of_lab": "...",
  "evidence_sentences": ["..."]
}}

Respond only in strict JSON format:
[
  {{
    "title":"<EXACTLY MATCHT THE TITLE FIELD FROM THE INPUT>"
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\n Running batch {i + 1}...")
        response = llm.invoke(full_prompt)
        print("\n Raw LLM response:\n", response.content[:1000])
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item in batch_result:
            title = item.get("source_document") or item.get("title")
            matched_doc = next((doc for doc in batch if doc.metadata.get("source") == item.get("title")), None)
            if not matched_doc:
                print(f"No matching doc for title: {title}")
                continue

            evidence_text = " ".join(item.get("evidence_sentences", []))
            kappa = clean_numeric(item.get("kappa_flc", ""))
            lambda_ = clean_numeric(item.get("lambda_flc", ""))
            ratio = clean_numeric(item.get("kappa_lambda_ratio", ""))

            item["kappa_flc"] = enrich_value_with_units(kappa, evidence_text)
            item["lambda_flc"] = enrich_value_with_units(lambda_, evidence_text)
            item["kappa_lambda_ratio"] = enrich_value_with_units(ratio, evidence_text) if ratio else None
            item["source_document"] = title

            item["context"] = json.dumps({
                "title": matched_doc.metadata.get("source", ""),
                "content": matched_doc.page_content
            }, indent=2)

            final_results.append(item)

    except Exception as e:
        print(f"Failed batch {i + 1}: {e}")

df = pd.DataFrame(final_results)

cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]
if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]

    def norm(val):
        return str(val).lower().strip() if pd.notna(val) else None

    df["kappa_norm"] = df["kappa_flc"].apply(norm)
    df["lambda_norm"] = df["lambda_flc"].apply(norm)
    df["ratio_norm"] = df["kappa_lambda_ratio"].apply(norm)

    df["score"] = df.apply(lambda row: sum([
        bool(row["kappa_norm"]),
        bool(row["lambda_norm"]),
        bool(row["ratio_norm"])
    ]), axis=1)

    def dedup_key(row):
        k = row["kappa_norm"]
        l = row["lambda_norm"]
        r = row["ratio_norm"]

        if k and l:
            return f"{k}|{l}"
        elif k and r:
            return f"{k}|{r}"
        elif l and r:
            return f"{l}|{r}"
        elif k:
            return k
        elif l:
            return l
        elif r:
            return r
        else:
            return "unknown"

    df["dedup_key"] = df.apply(dedup_key, axis=1)
    df["parsed_date"] = pd.to_datetime(df["date_of_lab"], errors="coerce")

    df.sort_values(by=["score", "parsed_date"], ascending=[False, False], inplace=True)
    df = df.drop_duplicates(subset=["dedup_key"], keep="first")
    df.sort_values(by=["source_document", "parsed_date", "score"], ascending=[True, False, False], inplace=True)
    df = df.drop_duplicates(subset=["source_document"], keep="first")
    df.drop(columns=["kappa_norm", "lambda_norm", "ratio_norm", "score", "parsed_date", "dedup_key"], inplace=True)

    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output4.xlsx", index=False)
    df.to_json("output/Output4.json", orient="records", indent=2)
    print("‚úÖ Output saved to 'output/' folder.")
else:
    print("‚ö†Ô∏è No results to save. The final DataFrame is empty or missing expected columns.")


THIS IS ALSO CORRECT :::

import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Helpers
def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        return json.dumps(json.loads(raw_text))
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        return json.dumps(json.loads(fixed))

# Load CSV
df = pd.read_csv("d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv")
documents = [Document(page_content=row["text"], metadata={"source": row["title"]})
             for _, row in df.iterrows() if pd.notna(row["text"])]

# Chunking
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)

# Embedding
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# FAISS index path
faiss_path = "faiss_index"

# Load or build FAISS
if os.path.exists(faiss_path):
    vectorstore = FAISS.load_local(faiss_path, embeddings=embedding_model, allow_dangerous_deserialization=True)
else:
    vectorstore = FAISS.from_documents(chunks, embedding_model)
    vectorstore.save_local(faiss_path)

# LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

final_results = []
query = (
    "Extract values related to the free light-chain assay including: "
    "kappa free light chains (mg/L or mg/dL), lambda free light chains (mg/L or mg/dL), and kappa/lambda ratio. "
    "Return numeric values with units when available. Include dates if clearly stated nearby. Partial extraction is allowed."
)

# Run per document
for doc in tqdm(documents):
    title = doc.metadata["source"]
    similar_chunks = vectorstore.similarity_search(query, k=50)
    same_doc_chunks = [c.page_content for c in similar_chunks if c.metadata.get("source") == title]
    if not same_doc_chunks:
        continue

    context = "\n\n".join(same_doc_chunks)

    full_prompt = f"""
full_prompt = f"""
You are a medical information extraction assistant. Your task is to extract precise and structured lab values from the provided clinical notes.

Extract the following only when the evidence includes **explicit numeric values with units** (e.g., `mg/dL`, `mg/L`, or ratio values like `>914.29`):

- Kappa free light chains (mg/L)
- Lambda free light chains (mg/L)
- Kappa/Lambda ratio
- Lab test date associated with these values
- Supporting evidence sentences

üìå Strict Rules:
- Ignore vague phrases like "elevated kappa" or "abnormal lambda" without a value and unit.
- If the value is not numeric or lacks a unit (e.g., just says "kappa abnormal"), skip it.
- Extract only if the sentence includes clear numeric values (e.g., "kappa 129.5 mg/dL", "ratio >914.29").
- If the date is incomplete (e.g., only month/year), format missing parts as "XX". E.g., "2021-06-XX".
- Do not guess or infer values.
- If the same evidence or sentence appears across multiple notes, extract it only once (skip duplicates).

Respond in strict JSON format like this:

[
  {{
    "kappa_flc": "<value with unit>",
    "lambda_flc": "<value with unit>",
    "kappa_lambda_ratio": "<numeric ratio>",
    "date_of_lab": "<YYYY-MM-DD>",
    "evidence_sentences": ["<sentence 1>", "<sentence 2>", ...]
  }}
]

---
üìÑ Context:
{context}
"""


--- CONTEXT START ---
{context}
--- CONTEXT END ---
"""
    try:
        response = llm.invoke(full_prompt)
        parsed = json.loads(parse_llm_json(response.content))
        for item in parsed:
            item["source_document"] = title
            final_results.append(item)
    except Exception as e:
        print(f"‚ùå Failed for {title}: {e}")

# Save results
output_dir = "output/fields"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, "kappa_lambda_51_results_updated.xlsx")

if final_results:
    df = pd.DataFrame(final_results)
    df["evidence_sentences"] = df["evidence_sentences"].apply(lambda x: "\n".join(x) if isinstance(x, list) else str(x))
    df = df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "source_document"])
    column_order = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences"]
    df = df[column_order]
    df.to_excel(output_path, index=False)
    print(f"\n‚úÖ Saved final output to: {output_path}")
else:
    print("\n‚ö†Ô∏è No results extracted.")



THE PERFECT ONE



import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

# Azure credentials
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Utilities
def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Load dataset
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# Prepare documents
documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    documents.append(Document(page_content=row["text"], metadata={"source": row["title"]}))

# Chunking
splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap= 0)
chunks = splitter.split_documents(documents)
batches = list(batchify(chunks, 20))

# Embedding model
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build FAISS index
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# Search
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/L), lambda free light chain (mg/L), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

# Filter matched content
filtered_chunks = []
for doc in results:
    norm_text = normalize_text(doc.page_content)
    source_title = doc.metadata.get("source", "Unknown")
    if source_title != "Unknown" and ('kappa' in norm_text or 'lambda' in norm_text or 'ratio' in norm_text):
        filtered_chunks.append({"title": source_title, "content": doc.page_content})

# Setup LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Run LLM on JSON-formatted context
final_results = []
batches = list(batchify(filtered_chunks, 10))
for i, batch in enumerate(batches):
    json_context = [
        {"note_id": j + 1, "title": item["title"], "content": item["content"]}
        for j, item in enumerate(batch)
    ]
    titles = [item["title"] for item in batch]

    full_prompt = f"""
You are a medical information extraction assistant. Your task is to extract lab results related to free light chains from the provided clinical notes.

From each document, extract **all available** values for the following lab test components:

- Kappa free light chains (mg/dL only)
- Lambda free light chains (mg/dL only)
- Kappa/Lambda ratio (with optional comparison signs like `>` or `<`)
- Associated lab test date (if available)
- Evidence sentences (the line(s) containing the values)

üìå **Extraction Rules**:
- Do not skip values even if they appear in different lines of the document.
- If multiple values are present in a document (e.g., kappa in one line and lambda in another), combine them into one set for that document.
- Prefer:
  - Values with `mg/dL` units for kappa and lambda
  - Ratios that contain `>` or `<`
  - Dates clearly mentioned near the values

üìÜ **Date Formatting Rules**:
- Extract exact dates if available (format: `YYYY-MM-DD`)
- If only partial date is available (e.g., "June 2021"), fill missing parts as `XX`:
  - "June 2021" ‚Üí `2021-06-XX`
  - "2021" only ‚Üí `2021-XX-XX`
- Do not invent or guess dates.

üìë **Evidence Sentence Rule**:
- From the document, include the sentence(s) that clearly show all the extracted values.
- Prefer the sentence that contains all three: kappa, lambda, and ratio.
- If such a sentence is not found, return the best matching line(s) containing the available data.

üì§ **Output Format**:
Respond in strict JSON format, one object per document, like below:

[
  {{
    "kappa_flc": "<value with unit>",
    "lambda_flc": "<value with unit>",
    "kappa_lambda_ratio": "<numeric ratio>",
    "date_of_lab": "<YYYY-MM-DD>",
    "evidence_sentences": ["<sentence 1>", "<sentence 2>", ...]
  }}
]

---
üìÑ Context (as JSON input):
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\nüß† Processing batch {i + 1}/{len(batches)}...")
        response = llm.invoke(full_prompt)
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)
        for item, title in zip(batch_result, titles):
            item["source_document"] = title
            item["context"] = json.dumps({
                "kappa_flc": item.get("kappa_flc", ""),
                "lambda_flc": item.get("lambda_flc", ""),
                "kappa_lambda_ratio": item.get("kappa_lambda_ratio", ""),
                "date_of_lab": item.get("date_of_lab", ""),
                "evidence_sentences": item.get("evidence_sentences", [])
            })
        final_results.extend(batch_result)
    except Exception as e:
        print(f"‚ùå Failed batch {i + 1}: {e}")

# Post-process
for row in final_results:
    if isinstance(row.get("evidence_sentences"), list):
        row["evidence_sentences"] = "\n".join(row["evidence_sentences"])

# Create DataFrame
df = pd.DataFrame(final_results)

# Optional: Select only required columns
df = df[["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]]

# ‚úÖ Step 1: Drop completely empty rows (no values extracted at all)
df = df[
    (df["kappa_flc"].notna()) |
    (df["lambda_flc"].notna()) |
    (df["kappa_lambda_ratio"].notna()) |
    (df["date_of_lab"].notna()) |
    (df["evidence_sentences"].notna())
]

# ‚úÖ Step 2: Merge multiple rows with same source_document
merged_df = df.groupby("source_document").agg({
    "kappa_flc": lambda x: "; ".join(sorted(set(filter(None, x.astype(str))))),
    "lambda_flc": lambda x: "; ".join(sorted(set(filter(None, x.astype(str))))),
    "kappa_lambda_ratio": lambda x: "; ".join(sorted(set(filter(None, x.astype(str))))),
    "date_of_lab": lambda x: "; ".join(sorted(set(filter(None, x.astype(str))))),
    "evidence_sentences": lambda x: "\n".join(sorted(set(filter(None, x.astype(str))))),
    "context": lambda x: "\n".join(sorted(set(filter(None, x.astype(str)))))
}).reset_index()

output_dir = r"C:\Users\HariharaM12\PycharmProjects\Task2"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, "Final_Merged_Output.xlsx")
merged_df.to_excel(output_path, index=False)
print(f"\n‚úÖ Merged final output saved to: {output_path}")

