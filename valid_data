import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config
config = configparser.ConfigParser()
config.read("config.ini")

# Azure credentials
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Paths
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
output_dir = r"C:\Users\HariharaM12\PycharmProjects\Rag1\Task"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, "extracted_results.xlsx")

# Helper functions
def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Load data and convert to Documents
df = pd.read_csv(csv_path)
documents = [
    Document(page_content=row["text"], metadata={"source": row["title"]})
    for _, row in df.iterrows() if pd.notna(row["text"])
]

# Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)
chunk_batches = list(batchify(chunks, 20))

# Setup embedding model
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build FAISS index
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in chunk_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# Query relevant chunks
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/L), lambda free light chain (mg/L), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

# Filter
filtered_chunks = []
for doc in results:
    norm = normalize_text(doc.page_content)
    if 'kappa' in norm or 'lambda' in norm or 'ratio' in norm:
        filtered_chunks.append((doc.metadata.get("source", "Unknown"), doc.page_content))

# Setup LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Prompt each batch
final_results = []
batches = list(batchify(filtered_chunks, 10))
for i, batch in enumerate(batches):
    context = "\n\n".join(f"Note {i + 1} from {source}::\n{doc}" for i, (source, doc) in enumerate(batch))
    prompt = f"""
You are a medical information extraction assistant. Your task is to extract lab results from the provided clinical notes.

From the context below, extract the following values **only if they are explicitly mentioned**:
- Kappa free light chains (mg/L)
- Lambda free light chains (mg/L)
- Kappa/Lambda ratio
- Lab test date (YYYY-MM-DD format)
- Supporting evidence sentence
- The document title the sentence came from (you will be given that)

IMPORTANT:
- If the date is incomplete (e.g., only month and year), fill the missing parts with "XX".
- DO NOT guess or infer dates.
- Return structured JSON list. One object per lab result.
- Include "evidence_sentences" and "source_document".

[
  {{
    "kappa_flc": "<value with unit>",
    "lambda_flc": "<value with unit>",
    "kappa_lambda_ratio": "<numeric ratio>",
    "date_of_lab": "<YYYY-MM-DD or with XX>",
    "evidence_sentences": ["<sentence>"],
    "source_document": "<document title>"
  }}
]

--- CONTEXT START ---
{context}
--- CONTEXT END ---
"""
    try:
        print(f"\n🧠 Batch {i + 1}/{len(batches)}")
        response = llm.invoke(prompt)
        cleaned = parse_llm_json(response.content)
        extracted_items = json.loads(cleaned)
        for item, (source_title, _) in zip(extracted_items, batch):
            item["source_document"] = source_title
            final_results.append(item)
    except Exception as e:
        print(f"❌ Failed batch {i + 1}: {e}")

# Save Excel output
df = pd.DataFrame(final_results)
if not df.empty:
   
    df["evidence_sentences"] = df["evidence_sentences"].apply( lambda x: "\n".join(x) if isinstance(x, list) else str(x) )
    df = df.drop_duplicates()
    column_order = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences"]
    df = df[column_order]
    df.to_excel(output_path, index=False)
    print(f"\n✅ Saved at: {output_path}")
else:
    print("⚠️ No data extracted.")



THIS IS ALSO CORRECT :::

import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Helpers
def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        return json.dumps(json.loads(raw_text))
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        return json.dumps(json.loads(fixed))

# Load CSV
df = pd.read_csv("d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv")
documents = [Document(page_content=row["text"], metadata={"source": row["title"]})
             for _, row in df.iterrows() if pd.notna(row["text"])]

# Chunking
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)

# Embedding
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# FAISS index path
faiss_path = "faiss_index"

# Load or build FAISS
if os.path.exists(faiss_path):
    vectorstore = FAISS.load_local(faiss_path, embeddings=embedding_model, allow_dangerous_deserialization=True)
else:
    vectorstore = FAISS.from_documents(chunks, embedding_model)
    vectorstore.save_local(faiss_path)

# LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

final_results = []
query = (
    "Extract values related to the free light-chain assay including: "
    "kappa free light chains (mg/L or mg/dL), lambda free light chains (mg/L or mg/dL), and kappa/lambda ratio. "
    "Return numeric values with units when available. Include dates if clearly stated nearby. Partial extraction is allowed."
)

# Run per document
for doc in tqdm(documents):
    title = doc.metadata["source"]
    similar_chunks = vectorstore.similarity_search(query, k=50)
    same_doc_chunks = [c.page_content for c in similar_chunks if c.metadata.get("source") == title]
    if not same_doc_chunks:
        continue

    context = "\n\n".join(same_doc_chunks)

    full_prompt = f"""
full_prompt = f"""
You are a medical information extraction assistant. Your task is to extract precise and structured lab values from the provided clinical notes.

Extract the following only when the evidence includes **explicit numeric values with units** (e.g., `mg/dL`, `mg/L`, or ratio values like `>914.29`):

- Kappa free light chains (mg/L)
- Lambda free light chains (mg/L)
- Kappa/Lambda ratio
- Lab test date associated with these values
- Supporting evidence sentences

📌 Strict Rules:
- Ignore vague phrases like "elevated kappa" or "abnormal lambda" without a value and unit.
- If the value is not numeric or lacks a unit (e.g., just says "kappa abnormal"), skip it.
- Extract only if the sentence includes clear numeric values (e.g., "kappa 129.5 mg/dL", "ratio >914.29").
- If the date is incomplete (e.g., only month/year), format missing parts as "XX". E.g., "2021-06-XX".
- Do not guess or infer values.
- If the same evidence or sentence appears across multiple notes, extract it only once (skip duplicates).

Respond in strict JSON format like this:

[
  {{
    "kappa_flc": "<value with unit>",
    "lambda_flc": "<value with unit>",
    "kappa_lambda_ratio": "<numeric ratio>",
    "date_of_lab": "<YYYY-MM-DD>",
    "evidence_sentences": ["<sentence 1>", "<sentence 2>", ...]
  }}
]

---
📄 Context:
{context}
"""


--- CONTEXT START ---
{context}
--- CONTEXT END ---
"""
    try:
        response = llm.invoke(full_prompt)
        parsed = json.loads(parse_llm_json(response.content))
        for item in parsed:
            item["source_document"] = title
            final_results.append(item)
    except Exception as e:
        print(f"❌ Failed for {title}: {e}")

# Save results
output_dir = "output/fields"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, "kappa_lambda_51_results_updated.xlsx")

if final_results:
    df = pd.DataFrame(final_results)
    df["evidence_sentences"] = df["evidence_sentences"].apply(lambda x: "\n".join(x) if isinstance(x, list) else str(x))
    df = df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "source_document"])
    column_order = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences"]
    df = df[column_order]
    df.to_excel(output_path, index=False)
    print(f"\n✅ Saved final output to: {output_path}")
else:
    print("\n⚠️ No results extracted.")



THE PERFECT ONE


import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config.ini
config = configparser.ConfigParser()
config.read("config.ini")

# Azure credentials
AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Utilities
def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Load dataset
csv_path = "MEDICAL_DATAS.csv"
df = pd.read_csv(csv_path)

# Prepare documents with correct title as metadata
documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    documents.append(Document(page_content=row["text"], metadata={"source": row["title"]}))

# Chunking
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)
chunks = splitter.split_documents(documents)
batches = list(batchify(chunks, 20))

# Embedding model
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

# Build FAISS index
def build_faiss(batch):
    return FAISS.from_documents(batch, embedding_model)

sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(build_faiss, batch): batch for batch in batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# Vector DB search
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/L), lambda free light chain (mg/L), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

# Filter chunks with matching content and keep original titles
filtered_chunks = []
for doc in results:
    norm_text = normalize_text(doc.page_content)
    source_title = doc.metadata.get("source", "Unknown")
    if source_title != "Unknown" and ('kappa' in norm_text or 'lambda' in norm_text or 'ratio' in norm_text):
        filtered_chunks.append({"title": source_title, "content": doc.page_content})

# Setup LLM
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Run LLM on batches
final_results = []
batches = list(batchify(filtered_chunks, 10))
for i, batch in enumerate(batches):
    json_context = [
        {"note_id": j + 1, "title": item["title"], "content": item["content"]}
        for j, item in enumerate(batch)
    ]
    titles = [item["title"] for item in batch]

    full_prompt = f"""
You are a medical information extraction assistant. Your task is to extract lab results from clinical notes.

Extract the following values only if explicitly stated:
- Kappa free light chains (mg/L)
- Lambda free light chains (mg/L)
- Kappa/Lambda ratio
- Lab test date associated with these values
- Supporting evidence sentences

\u26a0\ufe0f Do not guess or infer values. Only include notes where at least one of the values and the date are clearly stated.

Respond in strict JSON format like below:

[
  {{
    "kappa_flc": "<value with unit>",
    "lambda_flc": "<value with unit>",
    "kappa_lambda_ratio": "<numeric ratio>",
    "date_of_lab": "<YYYY-MM-DD>",
    "evidence_sentences": ["<sentence 1>", "<sentence 2>", ...]
  }}
]

---
\ud83d\udcc4 Context (as JSON input):
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\n\U0001f9e0 Processing batch {i + 1}/{len(batches)}...")
        response = llm.invoke(full_prompt)
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)
        for item, title in zip(batch_result, titles):
            item["source_document"] = title
        final_results.extend(batch_result)
    except Exception as e:
        print(f"\u274c Failed batch {i + 1}: {e}")

# Post-process and save
for row in final_results:
    if isinstance(row.get("evidence_sentences"), list):
        row["evidence_sentences"] = "\n".join(row["evidence_sentences"])

df = pd.DataFrame(final_results)
df = df[["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences"]]
df.drop_duplicates(inplace=True)

# Export to Excel
output_dir = r"C:\\Users\\HariharaM12\\PycharmProjects\\Task2"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, "Chain_Final_Output.xlsx")
df.to_excel(output_path, index=False)

print(f"\n\u2705 Final Excel file saved: {output_path}")

