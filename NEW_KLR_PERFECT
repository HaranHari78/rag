import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# --- Load config.ini ---
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# --- Helpers ---
def parse_llm_json(raw_text: str) -> str:
    if not raw_text.strip():
        print("⚠️ Warning: Empty LLM response")
        return "[]"

    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()

    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()

    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        print("⚠️ JSON decode error, trying to fix quotes")
        fixed = raw_text.replace("'", '"')
        try:
            parsed = json.loads(fixed)
        except Exception as e:
            print("❌ Still failed:", e)
            return "[]"

    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[0-9]+\.?[0-9]*", val)
    return match.group(0) if match else ""

def enrich_value_with_units(value: str, evidence: str) -> str:
    if not value or not evidence:
        return value
    try:
        float_value = float(value)
    except:
        return value
    pattern = re.compile(rf"([<>]?\s*{re.escape(value)}\s*(?:mg/dl|mg/l)?)", re.IGNORECASE)
    match = pattern.search(evidence)
    return match.group(1).strip() if match else value

def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# --- Load dataset ---
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# --- Create documents ---
all_documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    text = row["text"]
    source = row["title"]
    word_count = len(text.split())
    if word_count < 500:
        chunk_size = 1000
        chunk_overlap = 0
    elif word_count < 2000:
        chunk_size = 1500
        chunk_overlap = 250
    else:
        chunk_size = 3000
        chunk_overlap = 500
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    all_documents.extend([Document(page_content=chunk, metadata={"source": source}) for chunk in chunks])

# --- Embedding & FAISS index ---
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

grouped_batches = list(batchify(all_documents, 10))
sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(FAISS.from_documents, batch, embedding_model): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# --- Search relevant chunks ---
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/dL), lambda free light chain (mg/dL), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    norm = content.lower()
    if "kappa" in norm and "lambda" in norm and 'ratio' in norm or ("kappa/lambda" in norm and "kappa_lambda" in norm):
        filtered_chunks.append({"title": source_title, "content": content})

# --- LLM setup ---
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# --- Extraction ---
final_results = []
for i, batch in enumerate(batchify(filtered_chunks, 10)):
    json_context = [{"note_id": j + 1, "title": doc["title"], "content": doc["content"]} for j, doc in enumerate(batch)]
    titles = [doc["title"] for doc in batch]

    full_prompt = f"""
You are a clinical data extraction assistant. For the given document, extract if at least **kappa and lambda** are found with correct units (mg/dL or mg/L).  
If `kappa_lambda_ratio` is missing in the sentence, set it to null.

Example:
{{
  "kappa_flc": "1.91 mg/dL",
  "lambda_flc": "<0.15 mg/dL",
  "kappa_lambda_ratio": null,
  "date_of_lab": "...",
  "evidence_sentences": ["..."]
}}

Respond only in strict JSON format:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\n🧠 Running batch {i+1}...")
        response = llm.invoke(full_prompt)
        print("\n🔎 Raw LLM response:\n", response.content[:1000])
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item, title in zip(batch_result, titles):
            evidence_text = " ".join(item.get("evidence_sentences", []))
            kappa = clean_numeric(item.get("kappa_flc", ""))
            lambda_ = clean_numeric(item.get("lambda_flc", ""))
            ratio = clean_numeric(item.get("kappa_lambda_ratio", ""))
            item["kappa_flc"] = enrich_value_with_units(kappa, evidence_text)
            item["lambda_flc"] = enrich_value_with_units(lambda_, evidence_text)
            item["kappa_lambda_ratio"] = enrich_value_with_units(ratio, evidence_text) if ratio else None
            item["source_document"] = title
            item["context"] = json.dumps(item, indent=2)
            final_results.append(item)

    except Exception as e:
        print(f"❌ Failed batch {i+1}: {e}")

# --- Save results --- got 203,16,0.08
df = pd.DataFrame(final_results)

# 🔍 Filter by most recent lab year dynamically
df["date_of_lab"] = df["date_of_lab"].astype(str)
lab_years = df["date_of_lab"].str.extract(r"(20\d{2})")[0].dropna()
lab_years = lab_years[lab_years.astype(int) <= 2100]
latest_year = lab_years.astype(int).max()
print(f"🕒 Filtering for latest lab year: {latest_year}")
df = df[df["date_of_lab"].str.contains(str(latest_year), na=False)]

cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], keep='first', inplace=True)
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output.xlsx", index=False)
    df.to_json("output/Output.json", orient="records", indent=2)
    print("\n✅ Output saved to 'output/' folder")
else:
    print("⚠️ No results to save. The final DataFrame is empty or missing expected columns.")




THE BEST ::

# --- Save results ---
df = pd.DataFrame(final_results)
cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]

    # ✅ Filter by latest year from date_of_lab
    df["lab_year"] = df["date_of_lab"].astype(str).str.extract(r"(20\d{2})")
    df["lab_year"] = pd.to_numeric(df["lab_year"], errors="coerce")
    latest_lab_year = df["lab_year"].dropna().max()

    if not pd.isna(latest_lab_year):
        df = df[df["lab_year"] == latest_lab_year]
        print(f"\n📅 Keeping only results from latest lab year: {int(latest_lab_year)}")
    else:
        print("⚠️ No valid lab year found. Skipping year filter.")

    df.drop(columns=["lab_year"], inplace=True)
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], keep='first', inplace=True)
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output_with_units_enriched.xlsx", index=False)
    df.to_json("output/Output_with_units_enriched.json", orient="records", indent=2)
    print("\n✅ Output saved to 'output/' folder")
else:
    print("⚠️ No results to save. The final DataFrame is empty or missing expected columns.")



THIS IS VERYU PERFECT FROM ROWS BECOME  6 TO 5 UNIQU :
# --- Save results ---
df = pd.DataFrame(final_results)

# 🔍 Extract latest year from available lab dates
df["date_of_lab"] = df["date_of_lab"].astype(str)
lab_years = df["date_of_lab"].str.extract(r"(20\d{2})")[0].dropna()
lab_years = lab_years[lab_years.astype(int) <= 2100]
latest_year = lab_years.astype(int).max()
print(f"🕒 Filtering for latest lab year: {latest_year}")
df = df[df["date_of_lab"].str.contains(str(latest_year), na=False)]

# ✅ Convert to datetime, safely handle NaT
df["date_of_lab"] = pd.to_datetime(df["date_of_lab"], errors='coerce')
df = df.dropna(subset=["date_of_lab"])

# ✅ Sort and keep only earliest lab per doc
df = df.sort_values(by=["source_document", "date_of_lab"])
df = df.drop_duplicates(subset=["source_document"], keep="first")

# ✅ Convert date back to string to avoid Excel ####
df["date_of_lab"] = df["date_of_lab"].dt.strftime("%Y-%m-%d")

# ✅ Save final cleaned output
cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]
if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output_with_units_enriched1_cleaned_final.xlsx", index=False)
    df.to_json("output/Output_with_units_enriched1_cleaned_final.json", orient="records", indent=2)
    print("\n✅ Final cleaned output saved to 'output/' folder")
else:
    print("⚠️ No results to save. The final DataFrame is empty or missing expected columns.")





# Convert to DataFrame
df = pd.DataFrame(final_results)
df["date_of_lab"] = df["date_of_lab"].astype(str)
df = df[df["date_of_lab"].str.contains(r"20\d{2}", na=False)]

# Keep only latest year
lab_years = df["date_of_lab"].str.extract(r"(20\d{2})")[0].dropna().astype(int)
latest_year = lab_years.max()
df = df[df["date_of_lab"].str.contains(str(latest_year), na=False)]

# For each document, keep only the earliest date_of_lab
df["date_sort"] = pd.to_datetime(df["date_of_lab"], errors='coerce')
df.sort_values("date_sort", inplace=True)
df = df.drop_duplicates(subset=["source_document"], keep="first")
df.drop(columns=["date_sort"], inplace=True)

# Export final
cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]
df = df[cols]
os.makedirs("output", exist_ok=True)
df.to_excel("output/Output_with_units_enriched_final.xlsx", index=False)
df.to_json("output/Output_with_units_enriched_final.json", orient="records", indent=2)
print("✅ Final output saved.")








JUST TRY IT 
import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# --- Load config.ini ---
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# --- Helpers ---
def parse_llm_json(raw_text: str) -> str:
    if not raw_text.strip():
        print("⚠️ Warning: Empty LLM response")
        return "[]"

    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()

    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()

    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        print("⚠️ JSON decode error, trying to fix quotes")
        fixed = raw_text.replace("'", '"')
        try:
            parsed = json.loads(fixed)
        except Exception as e:
            print("❌ Still failed:", e)
            return "[]"

    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# --- Load dataset ---
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# --- Create documents ---
all_documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    text = row["text"]
    source = row["title"]
    word_count = len(text.split())
    if word_count < 500:
        chunk_size = 1000
        chunk_overlap = 0
    elif word_count < 2000:
        chunk_size = 1500
        chunk_overlap = 250
    else:
        chunk_size = 3000
        chunk_overlap = 500
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    all_documents.extend([Document(page_content=chunk, metadata={"source": source}) for chunk in chunks])

# --- Embedding & FAISS index ---
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

grouped_batches = list(batchify(all_documents, 10))
sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(FAISS.from_documents, batch, embedding_model): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# --- Search relevant chunks ---
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/dL), lambda free light chain (mg/dL), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    if any(keyword in content.lower() for keyword in ["kappa", "lambda", "ratio"]):
        filtered_chunks.append({"title": source_title, "content": content})

# --- Prompt Template ---
prompt_template = """
You are a clinical data extraction assistant. For each given clinical note, extract **Free Light Chain Assay** results 
and categorize them into one of the following phases:

1. Baseline (most recent prior to or on TAL initiation)
2. 1st lab after TAL initiation
3. 2nd lab after TAL initiation
4. 3rd lab after TAL initiation

Instructions:
- The TAL (Talquetamab) initiation date should be inferred from statements like "received talquetamab from 1/24/2024".
- Only extract results with clearly stated **kappa**, **lambda**, and **kappa/lambda ratio**, preferably with units like mg/dL or mg/L.
- Ignore any estimated, referenced, or repeated values from previous documents.
- Only extract new lab test results per phase (i.e., each one should have unique kappa/lambda readings).

Return strict JSON in this format:
[
  {
    "phase": "baseline" | "1st lab" | "2nd lab" | "3rd lab",
    "date_of_lab": "YYYY-MM-DD",
    "kappa_flc": "...",               
    "lambda_flc": "...",              
    "kappa_lambda_ratio": "...",      
    "record_id": "...",
    "evidence": "..."
  }
]
"""

# --- LLM setup ---
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# --- Extraction ---
final_results = []
for i, batch in enumerate(batchify(filtered_chunks, 10)):
    json_context = [{"note_id": j + 1, "title": doc["title"], "content": doc["content"]} for j, doc in enumerate(batch)]
    full_prompt = prompt_template + "\n--- Context:\n" + json.dumps(json_context, indent=2)

    try:
        print(f"\n🧠 Running batch {i+1}...")
        response = llm.invoke(full_prompt)
        print("\n🔎 Raw LLM response:\n", response.content[:1000])
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item in batch_result:
            final_results.append(item)

    except Exception as e:
        print(f"❌ Failed batch {i+1}: {e}")

# --- Save results ---
df = pd.DataFrame(final_results)
if not df.empty:
    df = df[["phase", "date_of_lab", "record_id", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "evidence"]]
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Baseline_and_3_Post_TAL_Labs.xlsx", index=False)
    print("\n✅ Saved structured output to 'output/Baseline_and_3_Post_TAL_Labs.xlsx'")
else:
    print("⚠️ No structured lab results extracted.")
