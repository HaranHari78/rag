import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# --- Load config.ini ---
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# --- Helpers ---
def parse_llm_json(raw_text: str) -> str:
    if not raw_text.strip():
        print("⚠️ Warning: Empty LLM response")
        return "[]"

    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()

    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()

    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        print("⚠️ JSON decode error, trying to fix quotes")
        fixed = raw_text.replace("'", '"')
        try:
            parsed = json.loads(fixed)
        except Exception as e:
            print("❌ Still failed:", e)
            return "[]"

    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[0-9]+\.?[0-9]*", val)
    return match.group(0) if match else ""

def enrich_value_with_units(value: str, evidence: str) -> str:
    if not value or not evidence:
        return value
    try:
        float_value = float(value)
    except:
        return value
    pattern = re.compile(rf"([<>]?\s*{re.escape(value)}\s*(?:mg/dl|mg/l)?)", re.IGNORECASE)
    match = pattern.search(evidence)
    return match.group(1).strip() if match else value

def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# --- Load dataset ---
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# --- Create documents ---
all_documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    text = row["text"]
    source = row["title"]
    word_count = len(text.split())
    if word_count < 500:
        chunk_size = 1000
        chunk_overlap = 0
    elif word_count < 2000:
        chunk_size = 1500
        chunk_overlap = 250
    else:
        chunk_size = 3000
        chunk_overlap = 500
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    all_documents.extend([Document(page_content=chunk, metadata={"source": source}) for chunk in chunks])

# --- Embedding & FAISS index ---
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

grouped_batches = list(batchify(all_documents, 10))
sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(FAISS.from_documents, batch, embedding_model): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# --- Search relevant chunks ---
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/dL), lambda free light chain (mg/dL), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    norm = content.lower()
    if "kappa" in norm and "lambda" in norm and 'ratio' in norm or ("kappa/lambda" in norm and "kappa_lambda" in norm):
        filtered_chunks.append({"title": source_title, "content": content})

# --- LLM setup ---
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# --- Extraction ---
final_results = []
for i, batch in enumerate(batchify(filtered_chunks, 10)):
    json_context = [{"note_id": j + 1, "title": doc["title"], "content": doc["content"]} for j, doc in enumerate(batch)]
    titles = [doc["title"] for doc in batch]

    full_prompt = f"""
You are a clinical data extraction assistant. For the given document, extract if at least **kappa and lambda** are found with correct units (mg/dL or mg/L).  
If `kappa_lambda_ratio` is missing in the sentence, set it to null.

Recognize alternate names:
- `KLC`, `Kappa light`, `Kappa FLC` = `kappa_flc`
- `LLC`, `Lambda light`, `Lambda FLC` = `lambda_flc`
- `K/L`, `kappa/lambda`, `kappa_lambda` = `kappa_lambda_ratio`

Example:
{{
  "kappa_flc": "1.91 mg/dL",
  "lambda_flc": "<0.15 mg/dL",
  "kappa_lambda_ratio": null,
  "date_of_lab": "...",
  "evidence_sentences": ["..."]
}}

Respond only in strict JSON format:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\n🧠 Running batch {i+1}...")
        response = llm.invoke(full_prompt)
        print("\n🔎 Raw LLM response:\n", response.content[:1000])
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item, title in zip(batch_result, titles):
            evidence_text = " ".join(item.get("evidence_sentences", []))
            kappa = clean_numeric(item.get("kappa_flc", ""))
            lambda_ = clean_numeric(item.get("lambda_flc", ""))
            ratio = clean_numeric(item.get("kappa_lambda_ratio", ""))
            item["kappa_flc"] = enrich_value_with_units(kappa, evidence_text)
            item["lambda_flc"] = enrich_value_with_units(lambda_, evidence_text)
            item["kappa_lambda_ratio"] = enrich_value_with_units(ratio, evidence_text) if ratio else None
            item["source_document"] = title
            item["context"] = json.dumps(item, indent=2)
            final_results.append(item)

    except Exception as e:
        print(f"❌ Failed batch {i+1}: {e}")

# --- Save results --- got 203,16,0.08
df = pd.DataFrame(final_results)

# 🔍 Filter by most recent lab year dynamically
df["date_of_lab"] = df["date_of_lab"].astype(str)
lab_years = df["date_of_lab"].str.extract(r"(20\d{2})")[0].dropna()
lab_years = lab_years[lab_years.astype(int) <= 2100]
latest_year = lab_years.astype(int).max()
print(f"🕒 Filtering for latest lab year: {latest_year}")
df = df[df["date_of_lab"].str.contains(str(latest_year), na=False)]

cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], keep='first', inplace=True)
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output.xlsx", index=False)
    df.to_json("output/Output.json", orient="records", indent=2)
    print("\n✅ Output saved to 'output/' folder")
else:
    print("⚠️ No results to save. The final DataFrame is empty or missing expected columns.")




THE BEST ::

# --- Save results ---
df = pd.DataFrame(final_results)
cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]

    # ✅ Filter by latest year from date_of_lab
    df["lab_year"] = df["date_of_lab"].astype(str).str.extract(r"(20\d{2})")
    df["lab_year"] = pd.to_numeric(df["lab_year"], errors="coerce")
    latest_lab_year = df["lab_year"].dropna().max()

    if not pd.isna(latest_lab_year):
        df = df[df["lab_year"] == latest_lab_year]
        print(f"\n📅 Keeping only results from latest lab year: {int(latest_lab_year)}")
    else:
        print("⚠️ No valid lab year found. Skipping year filter.")

    df.drop(columns=["lab_year"], inplace=True)
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], keep='first', inplace=True)
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output_with_units_enriched.xlsx", index=False)
    df.to_json("output/Output_with_units_enriched.json", orient="records", indent=2)
    print("\n✅ Output saved to 'output/' folder")
else:
    print("⚠️ No results to save. The final DataFrame is empty or missing expected columns.")



THIS IS VERYU PERFECT FROM ROWS BECOME  6 TO 5 UNIQU :
# --- Save results ---
df = pd.DataFrame(final_results)

# 🔍 Extract latest year from available lab dates
df["date_of_lab"] = df["date_of_lab"].astype(str)
lab_years = df["date_of_lab"].str.extract(r"(20\d{2})")[0].dropna()
lab_years = lab_years[lab_years.astype(int) <= 2100]
latest_year = lab_years.astype(int).max()
print(f"🕒 Filtering for latest lab year: {latest_year}")
df = df[df["date_of_lab"].str.contains(str(latest_year), na=False)]

# ✅ Convert to datetime, safely handle NaT
df["date_of_lab"] = pd.to_datetime(df["date_of_lab"], errors='coerce')
df = df.dropna(subset=["date_of_lab"])

# ✅ Sort and keep only earliest lab per doc
df = df.sort_values(by=["source_document", "date_of_lab"])
df = df.drop_duplicates(subset=["source_document"], keep="first")

# ✅ Convert date back to string to avoid Excel ####
df["date_of_lab"] = df["date_of_lab"].dt.strftime("%Y-%m-%d")

# ✅ Save final cleaned output
cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]
if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output_with_units_enriched1_cleaned_final.xlsx", index=False)
    df.to_json("output/Output_with_units_enriched1_cleaned_final.json", orient="records", indent=2)
    print("\n✅ Final cleaned output saved to 'output/' folder")
else:
    print("⚠️ No results to save. The final DataFrame is empty or missing expected columns.")





# Convert to DataFrame
df = pd.DataFrame(final_results)
df["date_of_lab"] = df["date_of_lab"].astype(str)
df = df[df["date_of_lab"].str.contains(r"20\d{2}", na=False)]

# Keep only latest year
lab_years = df["date_of_lab"].str.extract(r"(20\d{2})")[0].dropna().astype(int)
latest_year = lab_years.max()
df = df[df["date_of_lab"].str.contains(str(latest_year), na=False)]

# For each document, keep only the earliest date_of_lab
df["date_sort"] = pd.to_datetime(df["date_of_lab"], errors='coerce')
df.sort_values("date_sort", inplace=True)
df = df.drop_duplicates(subset=["source_document"], keep="first")
df.drop(columns=["date_sort"], inplace=True)

# Export final
cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]
df = df[cols]
os.makedirs("output", exist_ok=True)
df.to_excel("output/Output_with_units_enriched_final.xlsx", index=False)
df.to_json("output/Output_with_units_enriched_final.json", orient="records", indent=2)
print("✅ Final output saved.")








JUST TRY IT 
# ✅ FINAL UPDATED CODE TO MATCH BASELINE + 3 POST-TAL LABS EXACTLY

import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# --- Load config.ini ---
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# --- Helpers ---
def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    try:
        return json.dumps(json.loads(raw_text))
    except:
        fixed = raw_text.replace("'", '"')
        return json.dumps(json.loads(fixed)) if fixed else "[]"

def clean_numeric(val: str) -> str:
    match = re.search(r"[<>]?[ ]?[0-9]+\.?[0-9]*", str(val))
    return match.group(0).strip() if match else ""

def enrich_value_with_units(value: str, evidence: str) -> str:
    if not value or not evidence: return value
    pattern = re.compile(rf"([<>]?[ ]?{re.escape(value)}[ ]?(mg/dl|mg/l)?)", re.IGNORECASE)
    match = pattern.search(evidence)
    return match.group(1).strip() if match else value

def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# --- Load dataset ---
df = pd.read_csv("d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv")

# --- Chunk text ---
documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]): continue
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500, chunk_overlap=250)
    chunks = splitter.split_text(row["text"])
    documents.extend([Document(page_content=c, metadata={"source": row["title"]}) for c in chunks])

# --- Embedding & Index ---
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
)

batches = list(batchify(documents, 10))
sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [executor.submit(FAISS.from_documents, batch, embedding_model) for batch in batches]
    for f in tqdm(as_completed(futures), total=len(futures)):
        sub_indexes.append(f.result())

index = sub_indexes[0]
for sub in sub_indexes[1:]:
    index.merge_from(sub)

index.save_local("faiss_index")

# --- Run Query ---
query = "Extract patient's kappa, lambda (mg/dL or mg/L), ratio, lab date and evidence"
results = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True).similarity_search(query, k=1000)

filtered = []
for r in results:
    content = r.page_content.lower()
    if "kappa" in content and "lambda" in content and "mg/dl" in content or "mg/l" in content:
        filtered.append({"title": r.metadata["source"], "content": r.page_content})

# --- LLM Setup ---
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

final = []
for i, batch in enumerate(batchify(filtered, 10)):
    context = json.dumps([{"title": d["title"], "content": d["content"]} for d in batch], indent=2)
    prompt = f"""
You're a clinical assistant. Extract structured Free Light Chain results.
Respond with ONLY this format:
[ {{
  "record_id": "...",
  "date": "YYYY-MM-DD",
  "kappa_flc": "...",
  "lambda_flc": "...",
  "kappa_lambda_ratio": "...",
  "source_document": "...",
  "context": "..."
}} ]
Context:
{context}
"""
    response = llm.invoke(prompt)
    cleaned = json.loads(parse_llm_json(response.content))
    for item in cleaned:
        item["kappa_flc"] = enrich_value_with_units(clean_numeric(item.get("kappa_flc")), item.get("context", ""))
        item["lambda_flc"] = enrich_value_with_units(clean_numeric(item.get("lambda_flc")), item.get("context", ""))
        item["kappa_lambda_ratio"] = enrich_value_with_units(clean_numeric(item.get("kappa_lambda_ratio")), item.get("context", ""))
        final.append(item)

# --- Finalize ---
df_out = pd.DataFrame(final)
df_out = df_out[df_out["source_document"].isin(["2024-01-24_..._91427", "2024-02-22_..._91586", "2024-04-08_..._91591", "2024-06-13_..._91596"])]
df_out = df_out.drop_duplicates(subset=["source_document"], keep="first")
df_out.to_excel("output/Baseline_and_3_Labs.xlsx", index=False)
print("\n✅ Saved exact Baseline + 3 post-TAL labs output.")





import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from langchain_openai import AzureChatOpenAI
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Load API config
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Prompt template
PROMPT_TEMPLATE = """
You are a clinical AI assistant. From the following medical note, extract the most recent and clinically relevant lab values for:

1. `kappa_flc`: numeric value + unit (e.g., mg/dL or mg/L) + reference symbols if present (`<`, `>`, `=`).
2. `lambda_flc`: same format as above.
3. `kappa_lambda_ratio`: numeric or symbolic ratio (may include `<`, `>`, `missing`, or `unable to calculate`).

For each field, also extract:
- `lab_date`: the **exact date** the value was recorded. If date is partial (e.g., just month or year), format it as:
    🔹 "2024-01" → "2024-01-XX", "2024" → "2024-XX-XX"
    ❗ Never guess the date — extract only if explicitly present near the value.
- `evidence`: exact sentence or phrase (verbatim) that justifies the value.

Return the result in strict JSON like this:

```json
{
  "kappa_flc": {
    "value": "203.94",
    "lab_date": "2024-01-24",
    "evidence": "KFLC 203.94, LFLC <0.15, ratio >1456.71"
  },
  "lambda_flc": {
    "value": "<0.15",
    "lab_date": "2024-01-24",
    "evidence": "KFLC 203.94, LFLC <0.15, ratio >1456.71"
  },
  "kappa_lambda_ratio": {
    "value": ">1456.71",
    "lab_date": "2024-01-24",
    "evidence": "KFLC 203.94, LFLC <0.15, ratio >1456.71"
  }
}
```

Only extract what is explicitly present. Do not infer or hallucinate.

Medical Note:
"""{context}"""
"""

# Initialize LLM
llm = AzureChatOpenAI(
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version=AZURE_OPENAI_API_VERSION,
    deployment_name=GPT_DEPLOYMENT,
    temperature=0.0
)

def parse_llm_json(raw_text):
    pattern = r"```(?:json)?\\s*(.*?)```"
    match = re.search(pattern, raw_text, re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    try:
        return json.loads(raw_text)
    except:
        return {}

def extract_fields_from_note(note_text):
    full_prompt = PROMPT_TEMPLATE.replace("{context}", note_text)
    response = llm.invoke(full_prompt)
    return parse_llm_json(response.content)

# Example use
if __name__ == "__main__":
    df = pd.read_csv("MEDICAL_DATAS.csv")  # Assuming your notes are here
    final_data = []

    for _, row in tqdm(df.iterrows(), total=len(df)):
        note_text = row.get("text", "")
        doc_id = row.get("title", "")
        if not note_text.strip():
            continue
        try:
            result = extract_fields_from_note(note_text)
            for key in result:
                result[key]["source_document"] = doc_id
            final_data.append(result)
        except Exception as e:
            print(f"❌ Error in {doc_id}: {e}")

    # Flatten and save
    flat = []
    for entry in final_data:
        for field, value in entry.items():
            value["field"] = field
            flat.append(value)

    out_df = pd.DataFrame(flat)
    out_df.to_excel("extracted_lab_values.xlsx", index=False)
    print("✅ Extraction complete and saved to Excel")

