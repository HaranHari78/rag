import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# --- Load config.ini ---
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# --- Helpers ---
def parse_llm_json(raw_text: str) -> str:
    if not raw_text.strip():
        print("âš ï¸ Warning: Empty LLM response")
        return "[]"

    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()

    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()

    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        print("âš ï¸ JSON decode error, trying to fix quotes")
        fixed = raw_text.replace("'", '"')
        try:
            parsed = json.loads(fixed)
        except Exception as e:
            print("âŒ Still failed:", e)
            return "[]"

    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[0-9]+\.?[0-9]*", val)
    return match.group(0) if match else ""

def enrich_value_with_units(value: str, evidence: str) -> str:
    if not value or not evidence:
        return value
    try:
        float_value = float(value)
    except:
        return value
    pattern = re.compile(rf"([<>]?\s*{re.escape(value)}\s*(?:mg/dl|mg/l)?)", re.IGNORECASE)
    match = pattern.search(evidence)
    return match.group(1).strip() if match else value

def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# --- Load dataset ---
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# --- Create documents ---
all_documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    text = row["text"]
    source = row["title"]
    word_count = len(text.split())
    if word_count < 500:
        chunk_size = 1000
        chunk_overlap = 0
    elif word_count < 2000:
        chunk_size = 1500
        chunk_overlap = 250
    else:
        chunk_size = 3000
        chunk_overlap = 500
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    all_documents.extend([Document(page_content=chunk, metadata={"source": source}) for chunk in chunks])

# --- Embedding & FAISS index ---
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

grouped_batches = list(batchify(all_documents, 10))
sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(FAISS.from_documents, batch, embedding_model): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# --- Search relevant chunks ---
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/dL), lambda free light chain (mg/dL), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    norm = content.lower()
    if "kappa" in norm and "lambda" in norm and 'ratio' in norm or ("kappa/lambda" in norm and "kappa_lambda" in norm):
        filtered_chunks.append({"title": source_title, "content": content})

# --- LLM setup ---
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# --- Extraction ---
final_results = []
for i, batch in enumerate(batchify(filtered_chunks, 10)):
    json_context = [{"note_id": j + 1, "title": doc["title"], "content": doc["content"]} for j, doc in enumerate(batch)]
    titles = [doc["title"] for doc in batch]

    full_prompt = f"""
You are a clinical data extraction assistant. For the given document, extract if at least **kappa and lambda** are found with correct units (mg/dL or mg/L).  
If `kappa_lambda_ratio` is missing in the sentence, set it to null.

Example:
{{
  "kappa_flc": "1.91 mg/dL",
  "lambda_flc": "<0.15 mg/dL",
  "kappa_lambda_ratio": null,
  "date_of_lab": "...",
  "evidence_sentences": ["..."]
}}

Respond only in strict JSON format:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\nðŸ§  Running batch {i+1}...")
        response = llm.invoke(full_prompt)
        print("\nðŸ”Ž Raw LLM response:\n", response.content[:1000])
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item, title in zip(batch_result, titles):
            evidence_text = " ".join(item.get("evidence_sentences", []))
            kappa = clean_numeric(item.get("kappa_flc", ""))
            lambda_ = clean_numeric(item.get("lambda_flc", ""))
            ratio = clean_numeric(item.get("kappa_lambda_ratio", ""))
            item["kappa_flc"] = enrich_value_with_units(kappa, evidence_text)
            item["lambda_flc"] = enrich_value_with_units(lambda_, evidence_text)
            item["kappa_lambda_ratio"] = enrich_value_with_units(ratio, evidence_text) if ratio else None
            item["source_document"] = title
            item["context"] = json.dumps(item, indent=2)
            final_results.append(item)

    except Exception as e:
        print(f"âŒ Failed batch {i+1}: {e}")

# --- Save results --- got 203,16,0.08
df = pd.DataFrame(final_results)

# ðŸ” Filter by most recent lab year dynamically
df["date_of_lab"] = df["date_of_lab"].astype(str)
lab_years = df["date_of_lab"].str.extract(r"(20\d{2})")[0].dropna()
lab_years = lab_years[lab_years.astype(int) <= 2100]
latest_year = lab_years.astype(int).max()
print(f"ðŸ•’ Filtering for latest lab year: {latest_year}")
df = df[df["date_of_lab"].str.contains(str(latest_year), na=False)]

cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], keep='first', inplace=True)
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output.xlsx", index=False)
    df.to_json("output/Output.json", orient="records", indent=2)
    print("\nâœ… Output saved to 'output/' folder")
else:
    print("âš ï¸ No results to save. The final DataFrame is empty or missing expected columns.")




THE BEST ::

# --- Save results ---
df = pd.DataFrame(final_results)
cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]

    # âœ… Filter by latest year from date_of_lab
    df["lab_year"] = df["date_of_lab"].astype(str).str.extract(r"(20\d{2})")
    df["lab_year"] = pd.to_numeric(df["lab_year"], errors="coerce")
    latest_lab_year = df["lab_year"].dropna().max()

    if not pd.isna(latest_lab_year):
        df = df[df["lab_year"] == latest_lab_year]
        print(f"\nðŸ“… Keeping only results from latest lab year: {int(latest_lab_year)}")
    else:
        print("âš ï¸ No valid lab year found. Skipping year filter.")

    df.drop(columns=["lab_year"], inplace=True)
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], keep='first', inplace=True)
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output_with_units_enriched.xlsx", index=False)
    df.to_json("output/Output_with_units_enriched.json", orient="records", indent=2)
    print("\nâœ… Output saved to 'output/' folder")
else:
    print("âš ï¸ No results to save. The final DataFrame is empty or missing expected columns.")



THIS IS VERYU PERFECT FROM ROWS BECOME  6 TO 5 UNIQU :
# --- Save results ---
df = pd.DataFrame(final_results)

# ðŸ” Extract latest year from available lab dates
df["date_of_lab"] = df["date_of_lab"].astype(str)
lab_years = df["date_of_lab"].str.extract(r"(20\d{2})")[0].dropna()
lab_years = lab_years[lab_years.astype(int) <= 2100]
latest_year = lab_years.astype(int).max()
print(f"ðŸ•’ Filtering for latest lab year: {latest_year}")
df = df[df["date_of_lab"].str.contains(str(latest_year), na=False)]

# âœ… Convert to datetime, safely handle NaT
df["date_of_lab"] = pd.to_datetime(df["date_of_lab"], errors='coerce')
df = df.dropna(subset=["date_of_lab"])

# âœ… Sort and keep only earliest lab per doc
df = df.sort_values(by=["source_document", "date_of_lab"])
df = df.drop_duplicates(subset=["source_document"], keep="first")

# âœ… Convert date back to string to avoid Excel ####
df["date_of_lab"] = df["date_of_lab"].dt.strftime("%Y-%m-%d")

# âœ… Save final cleaned output
cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]
if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output_with_units_enriched1_cleaned_final.xlsx", index=False)
    df.to_json("output/Output_with_units_enriched1_cleaned_final.json", orient="records", indent=2)
    print("\nâœ… Final cleaned output saved to 'output/' folder")
else:
    print("âš ï¸ No results to save. The final DataFrame is empty or missing expected columns.")





# Convert to DataFrame
df = pd.DataFrame(final_results)
df["date_of_lab"] = df["date_of_lab"].astype(str)
df = df[df["date_of_lab"].str.contains(r"20\d{2}", na=False)]

# Keep only latest year
lab_years = df["date_of_lab"].str.extract(r"(20\d{2})")[0].dropna().astype(int)
latest_year = lab_years.max()
df = df[df["date_of_lab"].str.contains(str(latest_year), na=False)]

# For each document, keep only the earliest date_of_lab
df["date_sort"] = pd.to_datetime(df["date_of_lab"], errors='coerce')
df.sort_values("date_sort", inplace=True)
df = df.drop_duplicates(subset=["source_document"], keep="first")
df.drop(columns=["date_sort"], inplace=True)

# Export final
cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]
df = df[cols]
os.makedirs("output", exist_ok=True)
df.to_excel("output/Output_with_units_enriched_final.xlsx", index=False)
df.to_json("output/Output_with_units_enriched_final.json", orient="records", indent=2)
print("âœ… Final output saved.")








JUST TRY IT 

import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# --- Load config.ini ---
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]


def parse_llm_json(raw_text: str) -> str:
    if not raw_text.strip():
        return "[]"
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)


def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[<>]?[0-9]+\.?[0-9]*", val)
    return match.group(0) if match else ""


def enrich_value_with_units(value: str, evidence: str) -> str:
    if not value or not evidence:
        return value
    try:
        float(re.sub(r"[<>]", "", value))
    except:
        return value
    pattern = re.compile(rf"([<>]?[ ]*{re.escape(value)}[ ]*(mg/dl|mg/l)?)", re.IGNORECASE)
    match = pattern.search(evidence)
    return match.group(1).strip() if match else value


def extract_lab_date(evidence: str) -> str:
    patterns = [
        r'(\d{1,2})[/-](\d{1,2})[/-](20\d{2})',
        r'(20\d{2})[/-](\d{1,2})[/-](\d{1,2})'
    ]
    for pattern in patterns:
        match = re.search(pattern, evidence)
        if match:
            groups = list(match.groups())
            if len(groups[0]) == 4:
                yyyy, mm, dd = groups
            else:
                mm, dd, yyyy = groups
            return f"{yyyy}-{int(mm):02d}-{int(dd):02d}"
    return "Unknown"


def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


# Load CSV
df = pd.read_csv("d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv")

all_documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    text = row["text"]
    source = row["title"]
    splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=500)
    chunks = splitter.split_text(text)
    all_documents.extend([Document(page_content=chunk, metadata={"source": source}) for chunk in chunks])

embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION
)

# Build FAISS index
sub_indexes = []
grouped_batches = list(batchify(all_documents, 10))
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(FAISS.from_documents, batch, embedding_model): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        sub_indexes.append(future.result())
main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract lab values for kappa, lambda, and kappa/lambda ratio, including date and evidence."
results = vectorstore.similarity_search(query, k=1000)

# Filter for lab-relevant content
filtered_chunks = []
for doc in results:
    content = doc.page_content.lower()
    if all(keyword in content for keyword in ["kappa", "lambda"]):
        filtered_chunks.append({"title": doc.metadata["source"], "content": doc.page_content})

# LLM setup
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# Extract values
final_results = []
for i, batch in enumerate(batchify(filtered_chunks, 10)):
    json_context = [{"title": doc["title"], "content": doc["content"]} for doc in batch]
    full_prompt = f"""
Extract exact values for kappa, lambda, and ratio from the clinical notes. If ratio is not found, leave it null.
Also extract lab test date if mentioned in the content (like 'labs from MM/DD/YYYY')
Respond in strict JSON:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["..."]
  }}
]

Context:
{json.dumps(json_context, indent=2)}
"""
    try:
        response = llm.invoke(full_prompt)
        parsed = json.loads(parse_llm_json(response.content))
        for item, ref in zip(parsed, json_context):
            evidence = " ".join(item.get("evidence_sentences", []))
            item["source_document"] = ref["title"]
            item["kappa_flc"] = enrich_value_with_units(clean_numeric(item.get("kappa_flc", "")), evidence)
            item["lambda_flc"] = enrich_value_with_units(clean_numeric(item.get("lambda_flc", "")), evidence)
            ratio = clean_numeric(item.get("kappa_lambda_ratio", ""))
            item["kappa_lambda_ratio"] = enrich_value_with_units(ratio, evidence) if ratio else None
            item["date_of_lab"] = extract_lab_date(evidence)
            final_results.append(item)
    except Exception as e:
        print(f"Batch {i+1} failed: {e}")

# Create DataFrame and sort chronologically
df_final = pd.DataFrame(final_results)
df_final = df_final[df_final["date_of_lab"] != "Unknown"]
df_final["date_of_lab"] = pd.to_datetime(df_final["date_of_lab"], errors="coerce")
df_final.dropna(subset=["date_of_lab"], inplace=True)
df_final.sort_values("date_of_lab", inplace=True)

# Save final result
os.makedirs("output", exist_ok=True)
df_final.to_excel("output/Final_Lab_Results_Chronological.xlsx", index=False)
df_final.to_json("output/Final_Lab_Results_Chronological.json", orient="records", indent=2)
print("\nâœ… Final lab output saved with all 4 lab cycles.")

