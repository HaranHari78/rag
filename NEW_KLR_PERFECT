C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Scripts\python.exe C:\Users\HariharaM12\PycharmProjects\T_2\main.py 
100%|██████████| 517/517 [00:00<00:00, 623.96it/s]
100%|██████████| 164/164 [00:18<00:00,  8.69it/s]

 Running batch 1...

 Raw LLM response:
 ```json
[
  {
    "kappa_flc": "56.21 mg/dL",
    "lambda_flc": "<3.08 mg/dL",
    "kappa_lambda_ratio": ">115.57",
    "date_of_lab": "2024-02-22",
    "evidence_sentences": [
      "Kappa Free Light Chain 0.76 - 6.83 mg/dL 56.21 (H)",
      "Lambda Free Light Chain 0.68 - 4.58 mg/dL <3.08 (L)",
      "Kappa/Lambda FLC Ratio 0.66 - 2.22  >115.57 (H)"
    ]
  },
  {
    "kappa_flc": "56.21 mg/dL",
    "lambda_flc": "<3.08 mg/dL",
    "kappa_lambda_ratio": ">115.57",
    "date_of_lab": "2024-02-22",
    "evidence_sentences": [
      "Kappa Free Light Chain 0.76 - 6.83 mg/dL 56.21 (H)",
      "Lambda Free Light Chain 0.68 - 4.58 mg/dL <3.08 (L)",
      "Kappa/Lambda FLC Ratio 0.66 - 2.22  >115.57 (H)"
    ]
  },
  {
    "kappa_flc": "203.94 mg/dL",
    "lambda_flc": "<0.15 mg/dL",
    "kappa_lambda_ratio": ">1456.71",
    "date_of_lab": "2024-02-08",
    "evidence_sentences": [
      "Kappa Free Light Chain 203.94 (H) 0.76 - 6.83 mg/dL",
      "Lambda Free Light Chain <0.15 (L) 0.68 - 4.
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None

 Running batch 2...

 Raw LLM response:
 ```json
[
  {
    "kappa_flc": "64.9 mg/dL",
    "lambda_flc": "0.34 mg/dL",
    "kappa_lambda_ratio": "190",
    "date_of_lab": "2019-03-XX",
    "evidence_sentences": [
      "Diagnoses in 3/2019.M-spike of 6,calcium of 9.7, an IgG of 5610,kappa FLC 64.9, kappa/lambda ratio of 190, total protein 10.7, UPEP was positiveby immunofixation,and bone marrow showed myeloma."
    ]
  },
  {
    "kappa_flc": "20.78 mg/dL",
    "lambda_flc": "0.24 mg/dL",
    "kappa_lambda_ratio": "86.58",
    "date_of_lab": "2021-06-XX",
    "evidence_sentences": [
      "In 6/2021, he had kappa FLC of 20.78, lambda FLC of 0.24, kappa/lambda ratio of 86.58."
    ]
  },
  {
    "kappa_flc": "1.24 mg/dL",
    "lambda_flc": "0.72 mg/dL",
    "kappa_lambda_ratio": "1.72",
    "date_of_lab": "2021-12-15",
    "evidence_sentences": [
      "12/15: SPEP IgG kappa M spike of 0.1, Kappa light chain 1.24, lambda light chain 0.72. Kappa/ lambda ratio 1.72, beta 2 microglobulin 1.3"
    ]
  },
  {
    "kappa_flc": "1.45 
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None

 Running batch 3...

 Raw LLM response:
 ```json
[
  {
    "kappa_flc": "65 mg/dL",
    "lambda_flc": "0.34 mg/dL",
    "kappa_lambda_ratio": "190",
    "date_of_lab": "2021-XX-XX",
    "evidence_sentences": [
      "baseline labs showing an IgG kappa monoclonal protein of 6 g/dL, with kappa light chain of 65, lambda of 0.34 and ratio of 190."
    ]
  },
  {
    "kappa_flc": "<0.06 mg/dL",
    "lambda_flc": "<1.61 mg/dL",
    "kappa_lambda_ratio": null,
    "date_of_lab": "2024-04-08",
    "evidence_sentences": [
      "Labs from 4/8/2024: Kappa <0.06 mg/dL, Lambda <1.61 mg/dL, SPEP with M-spike 0.3 g/dL, IgG kappa"
    ]
  },
  {
    "kappa_flc": "64.9 mg/dL",
    "lambda_flc": null,
    "kappa_lambda_ratio": "190",
    "date_of_lab": "2019-03-XX",
    "evidence_sentences": [
      "At diagnosis M-spike was 6, Hgb 8.4, calcium of 9.7, albumin 2.3, adjusted calcium level 11.1, creatinine 0.99. IgG of 5610, kappa FLC 64.9, lappa/lambda ratio of 190, total protein 10.7."
    ]
  },
  {
    "kappa_flc": "129.54 mg/dL",
    "lamb
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
 No results to save. The final DataFrame is empty or missing expected columns.

Process finished with exit code 0



import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# --- Load config.ini ---
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]


# --- Helpers ---
def parse_llm_json(raw_text: str) -> str:
    if not raw_text.strip():
        print("⚠️ Warning: Empty LLM response")
        return "[]"

    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()

    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()

    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        print("⚠️ JSON decode error, trying to fix quotes")
        fixed = raw_text.replace("'", '"')
        try:
            parsed = json.loads(fixed)
        except Exception as e:
            print("❌ Still failed:", e)
            return "[]"

    return json.dumps(parsed)


def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[0-9]+\.?[0-9]*", val)
    return match.group(0) if match else ""


def enrich_value_with_units(value: str, evidence: str) -> str:
    if not value or not evidence:
        return value
    try:
        float_value = float(value)
    except:
        return value
    pattern = re.compile(rf"([<>]?\s*{re.escape(value)}\s*(?:mg/dl|mg/l)?)", re.IGNORECASE)
    match = pattern.search(evidence)
    return match.group(1).strip() if match else value


def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


# --- Load dataset ---
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# --- Create documents ---
all_documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    text = row["text"]
    source = row["title"]
    word_count = len(text.split())
    if word_count < 500:
        chunk_size = 1000
        chunk_overlap = 0
    elif word_count < 2000:
        chunk_size = 1500
        chunk_overlap = 250
    else:
        chunk_size = 3000
        chunk_overlap = 500
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    all_documents.extend([Document(page_content=chunk, metadata={"source": source}) for chunk in chunks])

# --- Embedding & FAISS index ---
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

grouped_batches = list(batchify(all_documents, 10))
sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(FAISS.from_documents, batch, embedding_model): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# --- Search relevant chunks ---
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/dL), lambda free light chain (mg/dL), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    norm = content.lower()
    if ("kappa" in norm and "lambda" in norm and 'ratio' in norm or
            ("kappa/lambda" in norm and "kappa_lambda" in norm)):
        filtered_chunks.append(Document(page_content=content,metadata={"source":source_title}))
        # filtered_chunks.append({"title": source_title, "content": content})


# --- LLM setup ---
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# --- Extraction ---
final_results = []
for i, batch in enumerate(batchify(filtered_chunks, 10)):
    json_context = [{"note_id": j + 1,
                     "title": doc.metadata.get("source", ""),
                     "content": doc.page_content
                     }
                    for j, doc in enumerate(batch)]
    titles = [doc.metadata.get("source", "") for doc in batch]

    full_prompt = f"""
You are a clinical data extraction assistant. For the given document, extract if at least **kappa and lambda** are found with correct units (mg/dL or mg/L).  
If `kappa_lambda_ratio` is missing in the sentence, set it to null.
 
Recognize alternate names:
- `KLC`, `Kappa light`, `Kappa FLC` = `kappa_flc`
- `LLC`, `Lambda light`, `Lambda FLC` = `lambda_flc`
- `K/L`, `kappa/lambda`, `kappa_lambda` = `kappa_lambda_ratio`
- If the date is incomplete (e.g., only month and year), fill the missing parts with "XX".
- DO NOT guess or infer dates
- If multiple partial rows are found (e.g., one has kappa+ratio, another has kappa+lambda), combine them **only if they belong to the same lab date** and clearly reference the same lab context.
- Prefer values where all three fields appear in close proximity (same paragraph or sentence).
- If multiple rows with similar values (e.g., 64.9 vs 65) exist, keep the one with **more fields present** and **clear unit** (mg/dL or mg/L).
 
Example:
{{
  "kappa_flc": "1.91 mg/dL",
  "lambda_flc": "<0.15 mg/dL",
  "kappa_lambda_ratio": null,
  "date_of_lab": "...",
  "evidence_sentences": ["..."]
}}
 
Respond only in strict JSON format:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]
 
--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\n Running batch {i + 1}...")
        response = llm.invoke(full_prompt)
        print("\n Raw LLM response:\n", response.content[:1000])
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item in batch_result:
            title = item.get("source_document") or item.get("title")

            matched_doc = next((doc for doc in batch if doc.metadata.get("source") == item.get("title")), None)
            if not matched_doc:
                print(f"No matching doc for title: {title}")
                continue

            evidence_text = " ".join(item.get("evidence_sentences", []))
            kappa = clean_numeric(item.get("kappa_flc", ""))
            lambda_ = clean_numeric(item.get("lambda_flc", ""))
            ratio = clean_numeric(item.get("kappa_lambda_ratio", ""))

            item["kappa_flc"] = enrich_value_with_units(kappa, evidence_text)
            item["lambda_flc"] = enrich_value_with_units(lambda_, evidence_text)
            item["kappa_lambda_ratio"] = enrich_value_with_units(ratio, evidence_text) if ratio else None
            item["source_document"] = title

            item["context"] = json.dumps({
                "title": matched_doc.metadata.get("source", ""),
                "content": matched_doc.page_content
            }, indent=2)

            final_results.append(item)

    except Exception as e:
        print(f"Failed batch {i + 1}: {e}")

df = pd.DataFrame(final_results)

df["completeness_score"] = df[["kappa_flc", "lambda_flc", "kappa_lambda_ratio"]].notna().sum(axis=1)

# ✅ Sort so that the most complete row comes first per source_document + date
df = df.sort_values(by=["source_document", "date_of_lab", "completeness_score"], ascending=[True, True, False])

# 🧹 Drop duplicates keeping the most complete per document and date
df = df.drop_duplicates(subset=["source_document", "date_of_lab"], keep="first")

cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences",
        "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], keep='first', inplace=True)
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output2.xlsx", index=False)
    df.to_json("output/Output2.json", orient="records", indent=2)
    print("\n Output saved to 'output/' folder")
else:
    print(" No results to save. The final DataFrame is empty or missing expected columns.")






import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# --- Load config.ini ---
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]


# --- Helpers ---
def parse_llm_json(raw_text: str) -> str:
    if not raw_text.strip():
        print("⚠️ Warning: Empty LLM response")
        return "[]"

    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()

    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()

    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        print("⚠️ JSON decode error, trying to fix quotes")
        fixed = raw_text.replace("'", '"')
        try:
            parsed = json.loads(fixed)
        except Exception as e:
            print("❌ Still failed:", e)
            return "[]"

    return json.dumps(parsed)


def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[0-9]+\.?[0-9]*", val)
    return match.group(0) if match else ""


def enrich_value_with_units(value: str, evidence: str) -> str:
    if not value or not evidence:
        return value
    try:
        float_value = float(value)
    except:
        return value
    pattern = re.compile(rf"([<>]?\s*{re.escape(value)}\s*(?:mg/dl|mg/l)?)", re.IGNORECASE)
    match = pattern.search(evidence)
    return match.group(1).strip() if match else value


def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


# --- Load dataset ---
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# --- Create documents ---
all_documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    text = row["text"]
    source = row["title"]
    word_count = len(text.split())
    if word_count < 500:
        chunk_size = 1000
        chunk_overlap = 0
    elif word_count < 2000:
        chunk_size = 1500
        chunk_overlap = 250
    else:
        chunk_size = 3000
        chunk_overlap = 500
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    all_documents.extend([Document(page_content=chunk, metadata={"source": source}) for chunk in chunks])

# --- Embedding & FAISS index ---
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

grouped_batches = list(batchify(all_documents, 10))
sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(FAISS.from_documents, batch, embedding_model): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# --- Search relevant chunks ---
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/dL), lambda free light chain (mg/dL), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    norm = content.lower()
    title_lower = source_title.lower()

    if (
        ("progress" in title_lower or "imtx conference" in title_lower) and
        (("kappa" in norm and "lambda" in norm and "ratio" in norm) or
         ("kappa/lambda" in norm and "kappa_lambda" in norm))
    ):
        filtered_chunks.append(Document(page_content=content, metadata={"source": source_title}))



# --- LLM setup ---
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# --- Extraction ---
final_results = []
for i, batch in enumerate(batchify(filtered_chunks, 10)):
    json_context = [{"note_id": j + 1, "title": doc.metadata.get("source", ""),
                     "content": doc.page_content
                     }
                    for j, doc in enumerate(batch)]
    titles = [doc.metadata.get("source", "") for doc in batch]

    full_prompt = f"""
You are a clinical data extraction assistant. For the given document, extract if at least **kappa and lambda** are found with correct units (mg/dL or mg/L).  
If `kappa_lambda_ratio` is missing in the sentence, set it to null.

- Only extract kappa or lambda values that are clearly stated with proper units (e.g., mg/dL or mg/L).
- If the ratio is not available or not explicitly stated, return it as `null`.
- Do not hallucinate or guess the ratio.
- If kappa and lambda appear in multiple places, extract only the most clinically recent or relevant one.
- Avoid re-extracting same values from different parts of the same document unless there's a meaningful change.

Recognize alternate names:
- `KLC`, `Kappa light`, `Kappa FLC` = `kappa_flc`
- `LLC`, `Lambda light`, `Lambda FLC` = `lambda_flc`
- `K/L`, `kappa/lambda`, `kappa_lambda` = `kappa_lambda_ratio`
- If the date is incomplete (e.g., only month and year), fill the missing parts with "XX".
- DO NOT guess or infer dates

TRY:::

You are a clinical data extraction assistant. From the given medical notes, extract the following lab values:

- `kappa_flc`: Kappa Free Light Chain (with unit like mg/dL or mg/L)
- `lambda_flc`: Lambda Free Light Chain (same unit)
- `kappa_lambda_ratio`: the ratio if present (may include `<`, `>`, or be null if not given)
- `date_of_lab`: exact or partial date near the lab values
- `evidence_sentences`: the exact sentence(s) where these values are mentioned

full_prompt = f"""
You are a clinical data extraction assistant. From the given medical notes, extract the following lab values:

- `kappa_flc`: Kappa Free Light Chain (with unit like mg/dL or mg/L)
- `lambda_flc`: Lambda Free Light Chain (same unit)
- `kappa_lambda_ratio`: the ratio if present (may include `<`, `>`, or be null if not given)
- `date_of_lab`: exact or partial date near the lab values
- `evidence_sentences`: the exact sentence(s) where these values are mentioned

🧠 Extraction Rules:
- Only extract `kappa` or `lambda` values that are **clearly stated with proper units** (e.g., mg/dL or mg/L).
- If the ratio is **not explicitly stated**, return `"kappa_lambda_ratio": null`. Never infer or guess the value.
- If multiple sets of values appear, extract only the **most clinically recent or relevant** one.
- Avoid repeating the same values across different sections of the same document unless there is a **clear change**.

📅 Date Handling Rules:
- Extract `date_of_lab` only if it is **explicitly present near the values**.
- If the date is partial (e.g., only month or year), format as:
    🔹 "2024-01" → "2024-01-XX", "2024" → "2024-XX-XX"
- If the date is vague (e.g., "baseline", "prior", "at diagnosis"), return `"date_of_lab": null`.
- ❗ Never infer or guess the date based on clinical stage descriptions.

🗂️ Output Matching:
- Each input note contains a `title` field. You must include `"title"` in each extracted result exactly as received. This is required for correct matching.

🧾 Format the output in strict JSON:
[
  {{
    "title": "<exact title from the input>",
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""


TRT::::2
🧠 Extraction Rules:
- Extract `kappa_flc` and `lambda_flc` if they are **clearly stated with units** (e.g., mg/dL or mg/L), even if the ratio is not present.
- If the ratio is **not explicitly stated**, return `"kappa_lambda_ratio": null`. ❗ Never guess or calculate it.
- Always keep `<`, `>` or `=` symbols if mentioned.
- If multiple values are mentioned, extract only the **most clinically recent or relevant** one.
- Do not repeat the same values from multiple parts unless they represent a different time or state.


    try:
        print(f"\n Running batch {i + 1}...")
        response = llm.invoke(full_prompt)
        print("\n Raw LLM response:\n", response.content[:1000])
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item in batch_result:
            title = item.get("source_document") or item.get("title")

            matched_doc = next((doc for doc in batch if doc.metadata.get("source") == title), None)
            if not matched_doc:
                print(f"No matching doc for title: {title}")
                continue

            evidence_text = " ".join(item.get("evidence_sentences", []))
            kappa = clean_numeric(item.get("kappa_flc", ""))
            lambda_ = clean_numeric(item.get("lambda_flc", ""))
            ratio = clean_numeric(item.get("kappa_lambda_ratio", ""))

            item["kappa_flc"] = enrich_value_with_units(kappa, evidence_text)
            item["lambda_flc"] = enrich_value_with_units(lambda_, evidence_text)
            item["kappa_lambda_ratio"] = enrich_value_with_units(ratio, evidence_text) if ratio else None
            item["source_document"] = title

            item["context"] = json.dumps({
                "title": matched_doc.metadata.get("source", ""),
                "content": matched_doc.page_content
            }, indent=2)

            final_results.append(item)

    except Exception as e:
        print(f"Failed batch {i + 1}: {e}")

df = pd.DataFrame(final_results)

cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences",
        "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], keep='first', inplace=True)
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output1.xlsx", index=False)
    df.to_json("output/Output1.json", orient="records", indent=2)
    print("\n Output saved to 'output/' folder")
else:
    print(" No results to save. The final DataFrame is empty or missing expected columns.")

C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Scripts\python.exe C:\Users\HariharaM12\PycharmProjects\T_2\main.py 
100%|██████████| 517/517 [00:00<00:00, 695.45it/s]
100%|██████████| 164/164 [00:16<00:00,  9.76it/s]

 Running batch 1...

 Raw LLM response:
 ```json
[
  {
    "kappa_flc": "56.21 mg/dL",
    "lambda_flc": "<3.08 mg/dL",
    "kappa_lambda_ratio": ">115.57",
    "date_of_lab": "2024-02-22",
    "evidence_sentences": [
      "Latest Reference Range & Units 02/22/24 09:20   Kappa Free Light Chain 0.76 - 6.83 mg/dL 56.21 (H)   Lambda Free Light Chain 0.68 - 4.58 mg/dL <3.08 (L)   Kappa/Lambda FLC Ratio 0.66 - 2.22  >115.57 (H)"
    ]
  },
  {
    "kappa_flc": "56.21 mg/dL",
    "lambda_flc": "<3.08 mg/dL",
    "kappa_lambda_ratio": ">115.57",
    "date_of_lab": "2024-02-22",
    "evidence_sentences": [
      "Free Light Chain 0.76 - 6.83 mg/dL 56.21 (H)   Lambda Free Light Chain 0.68 - 4.58 mg/dL <3.08 (L)   Kappa/Lambda FLC Ratio 0.66 - 2.22  >115.57 (H)"
    ]
  },
  {
    "kappa_flc": "203.94 mg/dL",
    "lambda_flc": "<0.15 mg/dL",
    "kappa_lambda_ratio": ">1456.71",
    "date_of_lab": "2024-02-08",
    "evidence_sentences": [
      "Free Light Chains   Result Value Ref Range    Kappa Free Light Chain 203.94 (H) 0.76 - 6.8
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None

 Running batch 2...

 Raw LLM response:
 ```json
[
  {
    "kappa_flc": "64.9 mg/dL",
    "lambda_flc": null,
    "kappa_lambda_ratio": "190",
    "date_of_lab": "03/XX/2019",
    "evidence_sentences": [
      "Diagnoses in 3/2019.M-spike of 6,calcium of 9.7, an IgG of 5610,kappa FLC 64.9, kappa/lambda ratio of 190, total protein 10.7, UPEP was positiveby immunofixation,and bone marrow showed myeloma."
    ]
  },
  {
    "kappa_flc": "20.78 mg/dL",
    "lambda_flc": "0.24 mg/dL",
    "kappa_lambda_ratio": "86.58",
    "date_of_lab": "06/XX/2021",
    "evidence_sentences": [
      "In 6/2021, he had kappa FLC of 20.78, lambda FLC of 0.24, kappa/lambda ratio of 86.58."
    ]
  },
  {
    "kappa_flc": "1.24 mg/dL",
    "lambda_flc": null,
    "kappa_lambda_ratio": "1.72",
    "date_of_lab": "12/15/2021",
    "evidence_sentences": [
      "Labs 12/15 show M-spike 0.1, KFLC 1.24, K/L ratio 1.72."
    ]
  },
  {
    "kappa_flc": "1.24 mg/dL",
    "lambda_flc": "0.72 mg/dL",
    "kappa_lambda_ratio": "1.72",
    "date_of_lab": "12/1
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None

 Running batch 3...

 Raw LLM response:
 ```json
[
  {
    "kappa_flc": "65 mg/dL",
    "lambda_flc": "0.34 mg/dL",
    "kappa_lambda_ratio": "190",
    "date_of_lab": "2021-XX-XX",
    "evidence_sentences": [
      "baseline labs showing an IgG kappa monoclonal protein of 6 g/dL, with kappa light chain of 65, lambda of 0.34 and ratio of 190."
    ]
  },
  {
    "kappa_flc": "<0.06 mg/dL",
    "lambda_flc": "<1.61 mg/dL",
    "kappa_lambda_ratio": null,
    "date_of_lab": "2024-04-08",
    "evidence_sentences": [
      "Labs from 4/8/2024: Kappa <0.06 mg/dL, Lambda <1.61 mg/dL, SPEP with M-spike 0.3 g/dL, IgG kappa"
    ]
  },
  {
    "kappa_flc": "64.9 mg/dL",
    "lambda_flc": null,
    "kappa_lambda_ratio": "190",
    "date_of_lab": "2019-03-XX",
    "evidence_sentences": [
      "At diagnosis M-spike was 6, Hgb 8.4, calcium of 9.7, albumin 2.3, adjusted calcium level 11.1, creatinine 0.99. IgG of 5610, kappa FLC 64.9, lappa/lambda ratio of 190, total protein 10.7."
    ]
  },
  {
    "kappa_flc": "16.18 mg/dL",
    "lambd
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
No matching doc for title: None
 No results to save. The final DataFrame is empty or missing expected columns.

Process finished with exit code 0




import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# --- Load config.ini ---
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]


# --- Helpers ---
def parse_llm_json(raw_text: str) -> str:
    if not raw_text.strip():
        print("⚠️ Warning: Empty LLM response")
        return "[]"

    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()

    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()

    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        print("⚠️ JSON decode error, trying to fix quotes")
        fixed = raw_text.replace("'", '"')
        try:
            parsed = json.loads(fixed)
        except Exception as e:
            print("❌ Still failed:", e)
            return "[]"

    return json.dumps(parsed)


def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[0-9]+\.?[0-9]*", val)
    return match.group(0) if match else ""


def enrich_value_with_units(value: str, evidence: str) -> str:
    if not value or not evidence:
        return value
    try:
        float_value = float(value)
    except:
        return value
    pattern = re.compile(rf"([<>]?\s*{re.escape(value)}\s*(?:mg/dl|mg/l)?)", re.IGNORECASE)
    match = pattern.search(evidence)
    return match.group(1).strip() if match else value


def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


# --- Load dataset ---
csv_path = "d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv"
df = pd.read_csv(csv_path)

# --- Create documents ---
all_documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    text = row["text"]
    source = row["title"]
    word_count = len(text.split())
    if word_count < 500:
        chunk_size = 1000
        chunk_overlap = 0
    elif word_count < 2000:
        chunk_size = 1500
        chunk_overlap = 250
    else:
        chunk_size = 3000
        chunk_overlap = 500
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    all_documents.extend([Document(page_content=chunk, metadata={"source": source}) for chunk in chunks])

# --- Embedding & FAISS index ---
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

grouped_batches = list(batchify(all_documents, 10))
sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(FAISS.from_documents, batch, embedding_model): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# --- Search relevant chunks ---
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/dL), lambda free light chain (mg/dL), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    norm = content.lower()
    if "kappa" in norm and "lambda" in norm and 'ratio' in norm or ("kappa/lambda" in norm and "kappa_lambda" in norm):
        filtered_chunks.append({"title": source_title, "content": content})

# --- LLM setup ---
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# --- Extraction ---
final_results = []
for i, batch in enumerate(batchify(filtered_chunks, 10)):
    json_context = [{"note_id": j + 1, "title": doc["title"], "content": doc["content"]} for j, doc in enumerate(batch)]
    titles = [doc["title"] for doc in batch]

    full_prompt = f"""
You are a clinical data extraction assistant. For the given document, extract if at least **kappa and lambda** are found with correct units (mg/dL or mg/L).  
If `kappa_lambda_ratio` is missing in the sentence, set it to null.

Recognize alternate names:
- `KLC`, `Kappa light`, `Kappa FLC` = `kappa_flc`
- `LLC`, `Lambda light`, `Lambda FLC` = `lambda_flc`
- `K/L`, `kappa/lambda`, `kappa_lambda` = `kappa_lambda_ratio`
- If the date is incomplete (e.g., only month and year), fill the missing parts with "XX".
- DO NOT guess or infer dates

Example:
{{
  "kappa_flc": "1.91 mg/dL",
  "lambda_flc": "<0.15 mg/dL",
  "kappa_lambda_ratio": null,
  "date_of_lab": "...",
  "evidence_sentences": ["..."]
}}

Respond only in strict JSON format:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\n🧠 Running batch {i + 1}...")
        response = llm.invoke(full_prompt)
        print("\n🔎 Raw LLM response:\n", response.content[:1000])
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item, title in zip(batch_result, titles):
            evidence_text = " ".join(item.get("evidence_sentences", []))
            kappa = clean_numeric(item.get("kappa_flc", ""))
            lambda_ = clean_numeric(item.get("lambda_flc", ""))
            ratio = clean_numeric(item.get("kappa_lambda_ratio", ""))
            item["kappa_flc"] = enrich_value_with_units(kappa, evidence_text)
            item["lambda_flc"] = enrich_value_with_units(lambda_, evidence_text)
            item["kappa_lambda_ratio"] = enrich_value_with_units(ratio, evidence_text) if ratio else None
            item["source_document"] = title
            item["context"] = json.dumps(item, indent=2)
            final_results.append(item)


TRY ::::

for item in batch_result:
    title = item.get("source_document") or item.get("title")

    matched_doc = next((doc for doc in batch if doc.metadata.get("source") == title), None)
    if not matched_doc:
        print(f"⚠️ No matching doc for title: {title}")
        continue

    evidence_text = " ".join(item.get("evidence_sentences", []))
    kappa = clean_numeric(item.get("kappa_flc", ""))
    lambda_ = clean_numeric(item.get("lambda_flc", ""))
    ratio = clean_numeric(item.get("kappa_lambda_ratio", ""))

    item["kappa_flc"] = enrich_value_with_units(kappa, evidence_text)
    item["lambda_flc"] = enrich_value_with_units(lambda_, evidence_text)
    item["kappa_lambda_ratio"] = enrich_value_with_units(ratio, evidence_text) if ratio else None
    item["source_document"] = title

    item["context"] = json.dumps({
        "title": matched_doc.metadata.get("source", ""),
        "content": matched_doc.page_content
    }, indent=2)

    final_results.append(item)


    except Exception as e:
        print(f"❌ Failed batch {i + 1}: {e}")



df = pd.DataFrame(final_results)

cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences",
        "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], keep='first', inplace=True)
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output2.xlsx", index=False)
    df.to_json("output/Output2.json", orient="records", indent=2)
    print("\n✅ Output saved to 'output/' folder")
else:
    print("⚠️ No results to save. The final DataFrame is empty or missing expected columns.")




THE BEST ::

# --- Save results ---
df = pd.DataFrame(final_results)
cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]

    # ✅ Filter by latest year from date_of_lab
    df["lab_year"] = df["date_of_lab"].astype(str).str.extract(r"(20\d{2})")
    df["lab_year"] = pd.to_numeric(df["lab_year"], errors="coerce")
    latest_lab_year = df["lab_year"].dropna().max()

    if not pd.isna(latest_lab_year):
        df = df[df["lab_year"] == latest_lab_year]
        print(f"\n📅 Keeping only results from latest lab year: {int(latest_lab_year)}")
    else:
        print("⚠️ No valid lab year found. Skipping year filter.")

    df.drop(columns=["lab_year"], inplace=True)
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], keep='first', inplace=True)
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output_with_units_enriched.xlsx", index=False)
    df.to_json("output/Output_with_units_enriched.json", orient="records", indent=2)
    print("\n✅ Output saved to 'output/' folder")
else:
    print("⚠️ No results to save. The final DataFrame is empty or missing expected columns.")



THIS IS VERYU PERFECT FROM ROWS BECOME  6 TO 5 UNIQU :
# --- Save results ---
df = pd.DataFrame(final_results)

# 🔍 Extract latest year from available lab dates
df["date_of_lab"] = df["date_of_lab"].astype(str)
lab_years = df["date_of_lab"].str.extract(r"(20\d{2})")[0].dropna()
lab_years = lab_years[lab_years.astype(int) <= 2100]
latest_year = lab_years.astype(int).max()
print(f"🕒 Filtering for latest lab year: {latest_year}")
df = df[df["date_of_lab"].str.contains(str(latest_year), na=False)]

# ✅ Convert to datetime, safely handle NaT
df["date_of_lab"] = pd.to_datetime(df["date_of_lab"], errors='coerce')
df = df.dropna(subset=["date_of_lab"])

# ✅ Sort and keep only earliest lab per doc
df = df.sort_values(by=["source_document", "date_of_lab"])
df = df.drop_duplicates(subset=["source_document"], keep="first")

# ✅ Convert date back to string to avoid Excel ####
df["date_of_lab"] = df["date_of_lab"].dt.strftime("%Y-%m-%d")

# ✅ Save final cleaned output
cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]
if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output_with_units_enriched1_cleaned_final.xlsx", index=False)
    df.to_json("output/Output_with_units_enriched1_cleaned_final.json", orient="records", indent=2)
    print("\n✅ Final cleaned output saved to 'output/' folder")
else:
    print("⚠️ No results to save. The final DataFrame is empty or missing expected columns.")





# Convert to DataFrame
df = pd.DataFrame(final_results)
df["date_of_lab"] = df["date_of_lab"].astype(str)
df = df[df["date_of_lab"].str.contains(r"20\d{2}", na=False)]

# Keep only latest year
lab_years = df["date_of_lab"].str.extract(r"(20\d{2})")[0].dropna().astype(int)
latest_year = lab_years.max()
df = df[df["date_of_lab"].str.contains(str(latest_year), na=False)]

# For each document, keep only the earliest date_of_lab
df["date_sort"] = pd.to_datetime(df["date_of_lab"], errors='coerce')
df.sort_values("date_sort", inplace=True)
df = df.drop_duplicates(subset=["source_document"], keep="first")
df.drop(columns=["date_sort"], inplace=True)

# Export final
cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]
df = df[cols]
os.makedirs("output", exist_ok=True)
df.to_excel("output/Output_with_units_enriched_final.xlsx", index=False)
df.to_json("output/Output_with_units_enriched_final.json", orient="records", indent=2)
print("✅ Final output saved.")








JUST TRY IT 
# ✅ FINAL UPDATED CODE TO MATCH BASELINE + 3 POST-TAL LABS EXACTLY

import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# --- Load config.ini ---
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# --- Helpers ---
def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    try:
        return json.dumps(json.loads(raw_text))
    except:
        fixed = raw_text.replace("'", '"')
        return json.dumps(json.loads(fixed)) if fixed else "[]"

def clean_numeric(val: str) -> str:
    match = re.search(r"[<>]?[ ]?[0-9]+\.?[0-9]*", str(val))
    return match.group(0).strip() if match else ""

def enrich_value_with_units(value: str, evidence: str) -> str:
    if not value or not evidence: return value
    pattern = re.compile(rf"([<>]?[ ]?{re.escape(value)}[ ]?(mg/dl|mg/l)?)", re.IGNORECASE)
    match = pattern.search(evidence)
    return match.group(1).strip() if match else value

def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# --- Load dataset ---
df = pd.read_csv("d2c1f46e2b3267d315fb03f76724aa7036ea01b3f1803e94126e26dc26881629.csv")

# --- Chunk text ---
documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]): continue
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500, chunk_overlap=250)
    chunks = splitter.split_text(row["text"])
    documents.extend([Document(page_content=c, metadata={"source": row["title"]}) for c in chunks])

# --- Embedding & Index ---
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
)

batches = list(batchify(documents, 10))
sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [executor.submit(FAISS.from_documents, batch, embedding_model) for batch in batches]
    for f in tqdm(as_completed(futures), total=len(futures)):
        sub_indexes.append(f.result())

index = sub_indexes[0]
for sub in sub_indexes[1:]:
    index.merge_from(sub)

index.save_local("faiss_index")

# --- Run Query ---
query = "Extract patient's kappa, lambda (mg/dL or mg/L), ratio, lab date and evidence"
results = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True).similarity_search(query, k=1000)

filtered = []
for r in results:
    content = r.page_content.lower()
    if "kappa" in content and "lambda" in content and "mg/dl" in content or "mg/l" in content:
        filtered.append({"title": r.metadata["source"], "content": r.page_content})

# --- LLM Setup ---
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

final = []
for i, batch in enumerate(batchify(filtered, 10)):
    context = json.dumps([{"title": d["title"], "content": d["content"]} for d in batch], indent=2)
    prompt = f"""
You're a clinical assistant. Extract structured Free Light Chain results.
Respond with ONLY this format:
[ {{
  "record_id": "...",
  "date": "YYYY-MM-DD",
  "kappa_flc": "...",
  "lambda_flc": "...",
  "kappa_lambda_ratio": "...",
  "source_document": "...",
  "context": "..."
}} ]
Context:
{context}
"""
    response = llm.invoke(prompt)
    cleaned = json.loads(parse_llm_json(response.content))
    for item in cleaned:
        item["kappa_flc"] = enrich_value_with_units(clean_numeric(item.get("kappa_flc")), item.get("context", ""))
        item["lambda_flc"] = enrich_value_with_units(clean_numeric(item.get("lambda_flc")), item.get("context", ""))
        item["kappa_lambda_ratio"] = enrich_value_with_units(clean_numeric(item.get("kappa_lambda_ratio")), item.get("context", ""))
        final.append(item)

# --- Finalize ---
df_out = pd.DataFrame(final)
df_out = df_out[df_out["source_document"].isin(["2024-01-24_..._91427", "2024-02-22_..._91586", "2024-04-08_..._91591", "2024-06-13_..._91596"])]
df_out = df_out.drop_duplicates(subset=["source_document"], keep="first")
df_out.to_excel("output/Baseline_and_3_Labs.xlsx", index=False)
print("\n✅ Saved exact Baseline + 3 post-TAL labs output.")





# Final updated Python script with:
# - Updated prompt
# - Historical filtering logic
# - Grouping enriched lab data per doc

import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# Load config
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# Utilities
def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        parsed = json.loads(raw_text.replace("'", '"'))
    return json.dumps(parsed)

def clean_numeric(val: str) -> str:
    match = re.search(r"[0-9]+\.?[0-9]*", str(val))
    return match.group(0) if match else ""

def enrich_value_with_units(value: str, evidence: str) -> str:
    pattern = re.compile(rf"([<>]?\s*{re.escape(value)}\s*(?:mg/dl|mg/l)?)", re.IGNORECASE)
    match = pattern.search(evidence)
    return match.group(1).strip() if match else value

def is_historical_context(text):
    hist_terms = ["diagnosed with", "baseline", "at diagnosis", "presented with", "previous labs", "in march 2019"]
    return sum(kw in text.lower() for kw in hist_terms) >= 2

def clean_and_enrich(item, title):
    evidence_text = " ".join(item.get("evidence_sentences", []))
    if is_historical_context(evidence_text):
        return None
    kappa = clean_numeric(item.get("kappa_flc", ""))
    lambda_ = clean_numeric(item.get("lambda_flc", ""))
    ratio = clean_numeric(item.get("kappa_lambda_ratio", ""))
    item["kappa_flc"] = enrich_value_with_units(kappa, evidence_text)
    item["lambda_flc"] = enrich_value_with_units(lambda_, evidence_text)
    item["kappa_lambda_ratio"] = enrich_value_with_units(ratio, evidence_text) if ratio else None
    item["source_document"] = title
    item["context"] = json.dumps(item, indent=2)
    return item

# Load and preprocess dataset
csv_path = "MEDICAL_DATAS.csv"
df = pd.read_csv(csv_path)

all_documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    text = row["text"]
    source = row["title"]
    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=250)
    chunks = splitter.split_text(text)
    all_documents.extend([Document(page_content=chunk, metadata={"source": source}) for chunk in chunks])

embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

grouped_batches = list([all_documents[i:i+10] for i in range(0, len(all_documents), 10)])
sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(FAISS.from_documents, batch, embedding_model): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa, lambda, and ratio lab values with units and dates."
results = vectorstore.similarity_search(query, k=1000)

filtered_chunks = []
search_terms = ["kappa", "lambda", "ratio", "kappa/lambda", "kappa_lambda", "klc", "llc", "k/l", "kappa light", "lambda light"]
for doc in results:
    content = doc.page_content
    if any(term in content.lower() for term in search_terms):
        filtered_chunks.append({"title": doc.metadata.get("source", "Unknown"), "content": content})

llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

final_results = []
for i, batch in enumerate(batchify(filtered_chunks, 10)):
    json_context = [{"note_id": j + 1, "title": doc["title"], "content": doc["content"]} for j, doc in enumerate(batch)]
    titles = [doc["title"] for doc in batch]

    full_prompt = f"""
You are a clinical data extraction assistant.

From the following medical note, extract only the most recent and valid free light chain laboratory values:
- Do NOT extract values that appear in diagnosis summaries, baseline history, or older context.
- DO extract values that occur with lab units (e.g., mg/dL, mg/L) and numbers.
- Focus on sections like \"Laboratory Results\", \"Lab Values\", \"CBC\", or \"Recent Labs\".

Extract:
1. `kappa_flc`: value + unit (e.g., \"1.24 mg/dL\")
2. `lambda_flc`: value + unit
3. `kappa_lambda_ratio`: value (e.g., \"1.72\", \">1450\", or \"null\" if missing)

Also return:
- `date_of_lab`: exact date found near the values
- `evidence_sentences`: the exact sentence(s) where the value was mentioned

Respond ONLY in this JSON format:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        response = llm.invoke(full_prompt)
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)
        for item, title in zip(batch_result, titles):
            enriched = clean_and_enrich(item, title)
            if enriched:
                final_results.append(enriched)
    except Exception as e:
        print(f"❌ Failed batch {i+1}: {e}")

# Final deduplication and output
output_df = pd.DataFrame(final_results)
output_df["date_parsed"] = pd.to_datetime(output_df["date_of_lab"].str.replace("XX", "01"), errors="coerce")
output_df = output_df.sort_values("date_parsed", ascending=False)
output_df = output_df.groupby(["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], as_index=False).first()

cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]
output_df = output_df[cols]

os.makedirs("output", exist_ok=True)
output_df.to_excel("output/Output.xlsx", index=False)
output_df.to_json("output/Output.json", orient="records", indent=2)
print("\n✅ Output saved to 'output/' folder")

