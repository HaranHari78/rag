C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Scripts\python.exe C:\Users\HariharaM12\PycharmProjects\T_2\main.py 
100%|██████████| 517/517 [00:00<00:00, 604.52it/s]
100%|██████████| 164/164 [00:19<00:00,  8.33it/s]

🧠 Running batch 1...
❌ Failed batch 1: Expecting value: line 1 column 1 (char 0)

🧠 Running batch 2...
❌ Failed batch 2: Expecting value: line 1 column 1 (char 0)

🧠 Running batch 3...
Traceback (most recent call last):
  File "C:\Users\HariharaM12\PycharmProjects\T_2\main.py", line 173, in <module>
    response = llm.invoke(full_prompt)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 372, in invoke
    self.generate_prompt(
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 957, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 776, in generate
    self._generate_with_cache(
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1022, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\langchain_openai\chat_models\base.py", line 1071, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\openai\_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\openai\_base_client.py", line 972, in request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\httpx\_transports\default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 256, in handle_request
    raise exc from None
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 103, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 136, in handle_request
    raise exc
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 106, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 177, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 217, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HariharaM12\PycharmProjects\T_2\.venv\Lib\site-packages\httpcore\_backends\sync.py", line 128, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\ssl.py", line 1295, in recv
    return self.read(buflen)
           ^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python311\Lib\ssl.py", line 1168, in read
    return self._sslobj.read(len)
           ^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

Process finished with exit code -1073741510 (0xC000013A: interrupted by Ctrl+C)



import os
import re
import json
import pandas as pd
import configparser
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# --- Load config.ini ---
config = configparser.ConfigParser()
config.read("config.ini")

AZURE_OPENAI_API_KEY = config["azure_openai"]["api_key"]
AZURE_OPENAI_ENDPOINT = config["azure_openai"]["endpoint"]
AZURE_OPENAI_API_VERSION = config["azure_openai"]["api_version"]
EMBEDDING_DEPLOYMENT = config["embedding_models"]["text_embedding_3_large"]
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_DEPLOYMENT = config["gpt_models"]["model_gpt4o"]

# --- Helpers ---
def parse_llm_json(raw_text: str) -> str:
    pattern = r"```(?:json)?\\s*(.*?)```"
    match = re.search(pattern, raw_text, flags=re.DOTALL)
    raw_text = match.group(1).strip() if match else raw_text.strip()
    if raw_text.startswith("json"):
        raw_text = raw_text[len("json"):].strip()
    try:
        parsed = json.loads(raw_text)
    except json.JSONDecodeError:
        fixed = raw_text.replace("'", '"')
        parsed = json.loads(fixed)
    return json.dumps(parsed)

def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_numeric(val: str) -> str:
    if not isinstance(val, str):
        return val
    match = re.search(r"[0-9]+\.?[0-9]*", val)
    return match.group(0) if match else ""

def enrich_value_with_units(value: str, evidence: str) -> str:
    if not value or not evidence:
        return value
    try:
        float_value = float(value)
    except:
        return value
    pattern = re.compile(rf"([<>]?\s*{re.escape(value)}\s*(?:mg/dl|mg/l)?)", re.IGNORECASE)
    match = pattern.search(evidence)
    return match.group(1).strip() if match else value

def batchify(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# --- Load dataset ---
csv_path = "MEDICAL_DATAS.csv"
df = pd.read_csv(csv_path)

# --- Create documents ---
all_documents = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row["text"]):
        continue
    text = row["text"]
    source = row["title"]
    word_count = len(text.split())
    if word_count < 500:
        chunk_size = 1000
        chunk_overlap = 0
    elif word_count < 2000:
        chunk_size = 1500
        chunk_overlap = 250
    else:
        chunk_size = 3000
        chunk_overlap = 500
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    all_documents.extend([Document(page_content=chunk, metadata={"source": source}) for chunk in chunks])

# --- Embedding & FAISS index ---
embedding_model = AzureOpenAIEmbeddings(
    deployment=EMBEDDING_DEPLOYMENT,
    model=EMBEDDING_MODEL,
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    chunk_size=1000
)

grouped_batches = list(batchify(all_documents, 10))
sub_indexes = []
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(FAISS.from_documents, batch, embedding_model): batch for batch in grouped_batches}
    for future in tqdm(as_completed(futures), total=len(futures)):
        try:
            sub_indexes.append(future.result())
        except Exception as e:
            print(f"Batch failed: {e}")

main_index = sub_indexes[0]
for sub_index in sub_indexes[1:]:
    main_index.merge_from(sub_index)
main_index.save_local("faiss_index")

# --- Search relevant chunks ---
vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)
query = "Extract the patient's kappa free light chain (mg/dL), lambda free light chain (mg/dL), and kappa/lambda ratio, along with the lab date and evidence."
results = vectorstore.similarity_search(query, k=1000)

filtered_chunks = []
for doc in results:
    source_title = doc.metadata.get("source", "Unknown")
    content = doc.page_content
    if "kappa" in content.lower() and "lambda" in content.lower():
        filtered_chunks.append({"title": source_title, "content": content})

# --- LLM setup ---
llm = AzureChatOpenAI(
    deployment_name=GPT_DEPLOYMENT,
    model_name="gpt-4o",
    openai_api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_version=AZURE_OPENAI_API_VERSION,
    temperature=0
)

# --- Extraction ---
final_results = []
for i, batch in enumerate(batchify(filtered_chunks, 10)):
    json_context = [{"note_id": j + 1, "title": doc["title"], "content": doc["content"]} for j, doc in enumerate(batch)]
    titles = [doc["title"] for doc in batch]

    full_prompt = f"""
You are a clinical data extraction assistant. For the given document, extract if at least **kappa and lambda** are found with correct units (mg/dL or mg/L).  
If `kappa_lambda_ratio` is missing in the sentence, set it to null.

Example:
{{
  "kappa_flc": "1.91 mg/dL",
  "lambda_flc": "<0.15 mg/dL",
  "kappa_lambda_ratio": null,
  "date_of_lab": "...",
  "evidence_sentences": ["..."]
}}

Respond only in strict JSON format:
[
  {{
    "kappa_flc": "...",
    "lambda_flc": "...",
    "kappa_lambda_ratio": "...",
    "date_of_lab": "...",
    "evidence_sentences": ["...", "..."]
  }}
]

--- Context:
{json.dumps(json_context, indent=2)}
"""

    try:
        print(f"\n🧠 Running batch {i+1}...")
        response = llm.invoke(full_prompt)
        cleaned = parse_llm_json(response.content)
        batch_result = json.loads(cleaned)

        for item, title in zip(batch_result, titles):
            evidence_text = " ".join(item.get("evidence_sentences", []))
            kappa = clean_numeric(item.get("kappa_flc", ""))
            lambda_ = clean_numeric(item.get("lambda_flc", ""))
            ratio = clean_numeric(item.get("kappa_lambda_ratio", ""))
            item["kappa_flc"] = enrich_value_with_units(kappa, evidence_text)
            item["lambda_flc"] = enrich_value_with_units(lambda_, evidence_text)
            item["kappa_lambda_ratio"] = enrich_value_with_units(ratio, evidence_text) if ratio else None
            item["source_document"] = title
            item["context"] = json.dumps(item, indent=2)
            final_results.append(item)

    except Exception as e:
        print(f"❌ Failed batch {i+1}: {e}")

# --- Save results ---
df = pd.DataFrame(final_results)
cols = ["source_document", "kappa_flc", "lambda_flc", "kappa_lambda_ratio", "date_of_lab", "evidence_sentences", "context"]

if not df.empty and all(col in df.columns for col in cols):
    df = df[cols]
    df.drop_duplicates(subset=["kappa_flc", "lambda_flc", "kappa_lambda_ratio"], keep='first', inplace=True)
    os.makedirs("output", exist_ok=True)
    df.to_excel("output/Output_with_units_enriched.xlsx", index=False)
    df.to_json("output/Output_with_units_enriched.json", orient="records", indent=2)
    print("\n✅ Output saved to 'output/' folder")
else:
    print("⚠️ No results to save. The final DataFrame is empty or missing expected columns.")
